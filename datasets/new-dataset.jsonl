{"text":"Our code and dataset is available here.","cats":{"new-dataset":1}}
{"text":"We will release the dataset and code to facilitate future endeavors.","cats":{"new-dataset":1}}
{"text":"We release our dataset for others to use and build on.","cats":{"new-dataset":1}}
{"text":"Our dataset is available online.","cats":{"new-dataset":1}}
{"text":"We release the generated dataset and used prompts to facilitate future research.","cats":{"new-dataset":1}}
{"text":"Code and dataset will be available.","cats":{"new-dataset":1}}
{"text":"We demonstrate the value of the created dataset by performing a quantitative and qualitative analysis on the models' results.","cats":{"new-dataset":1}}
{"text":"We train our model on a new synthetic image dataset, that we release.","cats":{"new-dataset":0}}
{"text":"The code and new synthetic dataset will be released for better reproducibility of our results.","cats":{"new-dataset":1}}
{"text":"From this point, we can note the importance of building a new structured dataset to solve the lack of structured data.","cats":{"new-dataset":0}}
{"text":"Previous datasets are created by either capturing real scenes by event cameras or synthesizing from images with pasted foreground objects.","cats":{"new-dataset":0}}
{"text":"These datasets included the latest second and third generation deepfake datasets.","cats":{"new-dataset":0}}
{"text":"To our knowledge, this is the first aligned dataset of its kind and is the largest dataset ever released in the heritage domain.","cats":{"new-dataset":1}}
{"text":"The code and dataset will be released publicly.","cats":{"new-dataset":1}}
{"text":"We have released the code and dataset used in the present approach to generate synthetic data.","cats":{"new-dataset":1}}
{"text":"Our dataset is publicly available and can be freely modified and re-distributed.","cats":{"new-dataset":1}}
{"text":"The community has recognized the critical role that training datasets play in this context and has developed various techniques to improve dataset curation to overcome this problem.","cats":{"new-dataset":0}}
{"text":"Our code and dataset will be released at https://github.com/SiyuanYan1/EPVT.","cats":{"new-dataset":1}}
{"text":"The 3RL dataset was created, which contains approximately 24K images and will be publicly available, to overcome previously available dataset problems.","cats":{"new-dataset":1}}
{"text":"We use a public dataset for model development.","cats":{"new-dataset":0}}
{"text":"The related datasets and the source code will be released in the future.","cats":{"new-dataset":1}}
{"text":"We also collect a new large-scale dataset to serve as the new benchmark for this task.","cats":{"new-dataset":1}}
{"text":"The complete dataset engineered for this study, referred to as the CIFAKE dataset, is made publicly available to the research community for future work.","cats":{"new-dataset":1}}
{"text":"The dataset with accompanying code can be downloaded from our website.","cats":{"new-dataset":1}}
{"text":"We propose new training, validation, and testing splits for the dataset that we make available online.","cats":{"new-dataset":0}}
{"text":"Our code and unique datasets are available on the project's website.","cats":{"new-dataset":1}}
{"text":"We make our data available.","cats":{"new-dataset":1}}
{"text":"To facilitate research in this field, we will share our dataset and code with the community.","cats":{"new-dataset":1}}
{"text":"The dataset, related codes and models will be publicly available at https://github.com/hitachinsk/THA.","cats":{"new-dataset":1}}
{"text":"We have developed a systematic method to synthesize such training datasets.","cats":{"new-dataset":0}}
{"text":"In addition to the dataset itself, we also present some basic analysis of its content, certain temporal features, and its network.","cats":{"new-dataset":0}}
{"text":"In fact, to date, there is no dataset that we are aware of that addresses this issue.","cats":{"new-dataset":0}}
{"text":"We are committed to open-sourcing our meticulously curated dataset, as well as a comprehensive toolkit designed to aid researchers in the extensive collection and preprocessing of their own datasets.","cats":{"new-dataset":1}}
{"text":"Third, we provide a dataset of scenario based on our data generated.","cats":{"new-dataset":1}}
{"text":"Our source code and dataset will be made publicly available.","cats":{"new-dataset":1}}
{"text":"To this end, we first collect a new dataset, CAMO-FS, for the benchmark.","cats":{"new-dataset":1}}
{"text":"In this paper, we propose a framework for enhancing the data quality of original datasets.","cats":{"new-dataset":1}}
{"text":"We release a demo of our tools at dataportraits.org and call on dataset and model creators to release Data Portraits as a complement to current documentation practices.","cats":{"new-dataset":0}}
{"text":"The source code and dataset will be public.","cats":{"new-dataset":1}}
{"text":"To the best of our knowledge, only two datasets are available, with one based on the other.","cats":{"new-dataset":0}}
{"text":"We validate our method on two widely used datasets.","cats":{"new-dataset":0}}
{"text":"We introduce the VISION Datasets, a diverse collection of 14 industrial inspection datasets, uniquely poised to meet these challenges.","cats":{"new-dataset":1}}
{"text":"The dataset is available at https://www.tu-ilmenau.de/neurob/data-sets-code/attach-dataset .","cats":{"new-dataset":1}}
{"text":"The dataset and code are available at \\url{https://github.com/littleYaang/HQ-50K}.","cats":{"new-dataset":1}}
{"text":"The code to reproduce our results, as well as the model and dataset (via a research data use agreement), are available at our Github repo here: https://github.com/som-shahlab/ehrshot-benchmark","cats":{"new-dataset":1}}
{"text":"Our SCB-dataset can be downloaded from: https://github.com/Whiffe/SCB-dataset","cats":{"new-dataset":1}}
{"text":"The dataset is available for download at: https://ustc-flicar.github.io.","cats":{"new-dataset":1}}
{"text":"To access our dataset and code, visit our GitHub repository: \\url{https://github.com/styxsys0927/Med-MMHL}.","cats":{"new-dataset":1}}
{"text":"Code can be downloaded from https://github.com/Zhang-VISLab.","cats":{"new-dataset":1}}
{"text":"The data products and codes can be downloaded from this https://github.com/sriniraghunathan/cross_ilc_methods_paper.","cats":{"new-dataset":1}}
{"text":"To download the data please visit https://stanford-tml.github.io/circle_dataset/.","cats":{"new-dataset":1}}
{"text":"Our code is available at Github.","cats":{"new-dataset":0}}
{"text":"All code is available on GitHub.","cats":{"new-dataset":0}}
{"text":"With these new techniques, our proposed \\Ours{} achieves state-of-the-art results on FUNSD and XFUND datasets, outperforming the previous best-performing method by 7.2\\% and 13.2\\% in F1 score, respectively.","cats":{"new-dataset":0}}
{"text":"We make a python package with the code available to download at https://pypi.org/project/hypertab/","cats":{"new-dataset":0}}
{"text":"The air pollution data was downloaded from an online database (UCL).","cats":{"new-dataset":0}}
{"text":"The SA3 dataset and scripts (R/Python) to develop these indices have been made available on my GitHub account: https://github.com/lpinzari/homogeneity-location-index","cats":{"new-dataset":0}}
{"text":"We make the code available at github.","cats":{"new-dataset":0}}
{"text":"All the source code is available on Github.","cats":{"new-dataset":0}}
{"text":"The code has been deposited on GitHub (\\url{https://github.com/hyguozz}).","cats":{"new-dataset":0}}
{"text":"The code is on github at https://github.com/IDU-CVLab/COV19D_3rd","cats":{"new-dataset":0}}
{"text":"Its features, e.g., no need to download and no installation, have made it popular rapidly.","cats":{"new-dataset":0}}
{"text":"We outperform all baselines and demonstrate that among the considered methods, ours is the only one that detects label errors of all four types efficiently.","cats":{"new-dataset":0}}
{"text":"By leveraging this diversity, the collected dataset and the collection system aim to achieve higher recognition accuracy.","cats":{"new-dataset":0,"data-quality":0}}
{"text":"The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity.","cats":{"new-dataset":0}}
{"text":"Our proposed dataset can support two different computer vision tasks, i.e., semantic segmentation and object detection.","cats":{"new-dataset":0}}
{"text":"To foster further research on the digitization of statistical graphs, we will make the dataset, code, and models publicly available to the community.","cats":{"new-dataset":1}}
{"text":"This collection includes a subset of the large-scale instruction dataset known as FLAN, as well as various code-related datasets and conversational datasets derived from ChatGPT/GPT-4.","cats":{"new-dataset":1}}
{"text":"FLACUNA is publicly available at https://huggingface.co/declare-lab/flacuna-13b-v1.0.","cats":{"new-dataset":1}}
{"text":"First, we publish a new dataset, EHRSHOT, containing de-identified structured data from the electronic health records (EHRs) of 6,712 patients from Stanford Medicine.","cats":{"new-dataset":1}}
{"text":"In this paper, we define a unified setting termed as open-set semantic segmentation (O3S), which aims to learn seen and unseen semantics from both visual examples and textual names.","cats":{"new-dataset":0}}
{"text":"Experimental results, carried out on three sets of the Shape COSEG Dataset, on the human segmentation dataset proposed in Maron et al., 2017 and on the ShapeNet benchmark, show how the proposed approach yields state-of-the-art performance on semantic segmentation of 3D meshes.","cats":{"new-dataset":0}}
{"text":"Our code and data are available at https://github.com/sergiotasconmorales/locvqa.","cats":{"new-dataset":0}}
{"text":"The dataset comprises high-resolution RGB-D images and pixel-level annotations of the fruits.","cats":{"new-dataset":0}}
{"text":"TGB datasets, data loaders, example codes, evaluation setup, and leaderboards are publicly available at https://tgb.complexdatalab.com/ .","cats":{"new-dataset":1}}
{"text":"Our code, models, instruction-sets and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.","cats":{"new-dataset":1}}
{"text":"We extensively evaluate SeaLog on two public datasets and an industrial dataset.","cats":{"new-dataset":0}}
{"text":"Using the COVID-19 Open Research Dataset (CORD-19), we produced two datasets: (1) synCovid, which uses a combination of handwritten prompts and synthetic prompts generated using OpenAI, and (2) real abstracts, which contains abstract and title pairs.","cats":{"new-dataset":1}}
{"text":"Videos are available at: https://kristery.github.io/edt/","cats":{"new-dataset":0}}
{"text":"This paper presents an end-to-end methodology for collecting datasets to recognize handwritten English alphabets by utilizing Inertial Measurement Units (IMUs) and leveraging the diversity present in the Indian writing style.","cats":{"new-dataset":0}}
{"text":"Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.","cats":{"new-dataset":0}}
{"text":"In this paper, we study linear regression applied to data structured on a manifold.","cats":{"new-dataset":0,"data-quality":0}}
{"text":"We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impact of the data manifold's extrinsic geometry on the regression.","cats":{"new-dataset":0,"data-quality":0}}
{"text":"This research can be extended and contributes to the field of pattern recognition and offers valuable insights for developing improved systems for handwriting recognition, particularly in diverse linguistic and cultural contexts.","cats":{"new-dataset":0}}
{"text":"To prove these theorems, we revisit William Thurston's results on the calisson tilability of a region $R$.","cats":{"new-dataset":0}}
{"text":"Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on early releases of ChatGPT that elicit undesired behavior.","cats":{"new-dataset":0,"data-quality":0}}
{"text":"We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks.","cats":{"new-dataset":0}}
{"text":"Given a triangular grid in a hexagon and some given edges of the grid, the problem is to find a calisson tiling such that no input edge is overlapped and calissons adjacent to an input edge have different orientations.","cats":{"new-dataset":0}}
{"text":"Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.","cats":{"new-dataset":0,"data-quality":0}}
{"text":"We extend the puzzle to regions $R$ that are not necessarily hexagonal.","cats":{"new-dataset":0}}
{"text":"By using a natural language generation model to abductively infer a premise given another premise and a conclusion, we can impute missing pieces of evidence needed for the conclusion to be true.","cats":{"new-dataset":0}}
{"text":"We sample multiple possible outputs for each step to achieve coverage of the search space, at the same time ensuring correctness by filtering low-quality generations with a round-trip validation procedure.","cats":{"new-dataset":0}}
{"text":"Results on a modified version of the EntailmentBank dataset and a new dataset called Everyday Norms: Why Not? show that abductive generation with validation can recover premises across in- and out-of-domain settings","cats":{"new-dataset":0}}
{"text":"To address this issue, this paper presents a systematic and comprehensive study, quantitatively and qualitatively, on training such models.","cats":{"new-dataset":0}}
{"text":"We implement over 20 variants with controlled settings.","cats":{"new-dataset":0}}
{"text":"For training data, we investigate the impact of data and sampling strategies.","cats":{"new-dataset":0}}
{"text":"For benchmarks, we contribute the first, to our best knowledge, comprehensive evaluation set including both image and video tasks through crowd-sourcing.","cats":{"new-dataset":1}}
{"text":"Here, remote sensing can provide reliable estimates of plastic pollution by regularly monitoring and detecting marine debris in coastal areas.","cats":{"new-dataset":0}}
{"text":"Medium-resolution satellite data of coastal areas is readily available and can be leveraged to detect aggregations of marine debris containing plastic litter.","cats":{"new-dataset":0}}
{"text":"In this work, we present a detector for marine debris built on a deep segmentation model that outputs a probability for marine debris at the pixel level.","cats":{"new-dataset":0}}
{"text":"We train this detector with a combination of annotated datasets of marine debris and evaluate it on specifically selected test sites where it is highly probable that plastic pollution is present in the detected marine debris.","cats":{"new-dataset":0}}
{"text":"We demonstrate quantitatively and qualitatively that a deep learning model trained on this dataset issued from multiple sources outperforms existing detection models trained on previous datasets by a large margin.","cats":{"new-dataset":1}}
{"text":"We hope to accelerate advances in the large-scale automated detection of marine debris, which is a step towards quantifying and monitoring marine litter with remote sensing at global scales, and release the model weights and training source code under https://github.com/marccoru/marinedebrisdetector","cats":{"new-dataset":0}}
{"text":"This paper proposes a framework called <projektor>, which predicts model performance and supports data selection decisions based on partial samples of prospective data sources.","cats":{"new-dataset":0}}
{"text":"In the first stage, we leverage the Optimal Transport distance to predict the model's performance for any data mixture ratio within the range of disclosed data sizes.","cats":{"new-dataset":0}}
{"text":"To facilitate the development of comprehensive scene understanding solutions using multiple camera views, a new dataset called Road Genome (OpenLane-V2) has been released.","cats":{"new-dataset":1}}
{"text":"We achieve new state-of-the-art performance on the large-scale Waymo Open Dataset.","cats":{"new-dataset":0}}
{"text":"Finally, we train a detector that generalizes to a wide range of part segmentation datasets while achieving better performance than dataset-specific training.","cats":{"new-dataset":0}}
{"text":"Finally, we release a large-scale synthetic dataset with 1.4M examples generated using TrueTeacher.","cats":{"new-dataset":1}}
{"text":"To fully unlock model capabilities for high-quality video generation, we curate a large-scale video dataset called HD-VG-130M. This dataset comprises 130 million text-video pairs from the open-domain, ensuring high-definition, widescreen and watermark-free characters.","cats":{"new-dataset":1}}
{"text":"Code and datasets are available in https://github.com/zjunlp/DeepKE/tree/main/example/llm.","cats":{"new-dataset":1}}
{"text":"We evaluate TagGPT on publicly available datasets, i.e., Kuaishou and Food.com, and demonstrate the effectiveness of TagGPT compared to existing hashtags and off-the-shelf taggers.","cats":{"new-dataset":0}}
{"text":"Additionally, to track new and creative applications for bioinformatics tools such as ChatGPT, we have established a GitHub repository at https://github.com/csbl-br/awesome-compbio-chatgpt.","cats":{"new-dataset":0}}
{"text":"All data and trained models are publicly available.","cats":{"new-dataset":1}}
{"text":"We have conducted extensive experiments on 16 public log datasets.","cats":{"new-dataset":0}}
{"text":"We also release the code and the annotated dataset for replication and future research.","cats":{"new-dataset":1}}
{"text":"The training data for these models is usually collected from open-source repositories (e.g., GitHub) that contain software faults and security vulnerabilities.","cats":{"new-dataset":0}}
{"text":"CCUB dataset is curated and our approach is evaluated by people who have a personal relationship with that particular culture.","cats":{"new-dataset":1}}
{"text":"Project page: https://europe.naverlabs.com/imagenet-sd/","cats":{"new-dataset":0}}
{"text":"GitHub repository: https://github.com/ymcui/Chinese-LLaMA-Alpaca","cats":{"new-dataset":0}}
{"text":"The resulting \\textbf{C}hinese \\textbf{O}pen \\textbf{I}nstruction \\textbf{G}eneralist (\\textbf{COIG}) corpora are available in Huggingface\\footnote{\\url{https://huggingface.co/datasets/BAAI/COIG}} and Github\\footnote{\\url{https://github.com/FlagOpen/FlagInstruct}}, and will be continuously updated.","cats":{"new-dataset":1}}
{"text":"We make our model, data, as well as code publicly available.","cats":{"new-dataset":1}}
{"text":"The data, code, and all model outputs are released in https://github.com/microsoft/AGIEval.","cats":{"new-dataset":1}}
{"text":"We created a comprehensive dataset including 492.5K samples comprising code-related content produced by ChatGPT, encompassing popular software activities like Q&A (115K), code summarization (126K), and code generation (226.5K).","cats":{"new-dataset":1}}
{"text":"The code is publicly available at https://github.com/Vision-CAIR/ChatCaptioner","cats":{"new-dataset":1}}
{"text":"Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known datasets like LogiQA and ReClor.","cats":{"new-dataset":1}}
{"text":"Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.","cats":{"new-dataset":1}}
{"text":"Our source code and datasets are available at https://github.com/xinleihe/MGTBench.","cats":{"new-dataset":1}}
{"text":"The training data, codes, and weights of this project are available at: The training data, codes, and weights of this project are available at: https://github.com/Kent0n-Li/ChatDoctor.","cats":{"new-dataset":0}}
{"text":"Codes and benchmarking data information are available at https://github.com/yhydhx/ChatGPT-API.","cats":{"new-dataset":0}}
{"text":"The dataset and code are available at https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT.","cats":{"new-dataset":1}}
{"text":"To support further research in related fields, we have made the data generated by ChatGPT publicly available at https://github.com/THU-BPM/chatgpt-sql.","cats":{"new-dataset":1}}
{"text":"In the second stage, we extrapolate the performance to larger undisclosed data sizes based on a novel parameter-free mapping technique inspired by neural scaling laws.","cats":{"new-dataset":0}}
{"text":"Also, <projektor> outperforms by a wide margin in data selection effectiveness compared to a range of other off-the-shelf solutions.","cats":{"new-dataset":0}}
{"text":"Database alignment is a variant of the graph alignment problem: Given a pair of anonymized databases containing separate yet correlated features for a set of users, the problem is to identify the correspondence between the features and align the anonymized user sets based on correlation alone.","cats":{"new-dataset":0}}
{"text":"We study an instance of the database alignment problem with multivariate Gaussian features and derive results that apply both for database alignment and for planted matching, demonstrating the connection between them.","cats":{"new-dataset":0}}
{"text":"The maximum likelihood algorithms for both planted matching and database alignment take the form of a linear program and we study relaxations to better understand the significance of various constraints under various conditions and present achievability and converse bounds.","cats":{"new-dataset":0}}
{"text":"Our analysis and results extend to the unbalanced case where one user set is not fully covered by the alignment.","cats":{"new-dataset":0}}
{"text":"They only work for in-distribution artifact types generated during training.","cats":{"new-dataset":0}}
{"text":"In this paper, we analyze the cause and characteristics of the GAN artifacts produced in unseen test data without ground-truths.","cats":{"new-dataset":0}}
{"text":"We then develop a novel method, namely, DeSRA, to Detect and then Delete those SR Artifacts in practice.","cats":{"new-dataset":0}}
{"text":"After detecting the artifact regions, we develop a finetune procedure to improve GAN-based SR models with a few samples, so that they can deal with similar types of artifacts in more unseen real data.","cats":{"new-dataset":0}}
{"text":"The code will be available at https://github.com/TencentARC/DeSRA.","cats":{"new-dataset":0}}
{"text":"In this work, we review robustness issues of DL and particularly bridge concerns and attempts from approximation theory to statistical learning theory.","cats":{"new-dataset":0}}
{"text":"Further, we review Bayesian Deep Learning as a means for uncertainty quantification and rigorous explainability.","cats":{"new-dataset":0}}
{"text":"A new control paradigm using angular momentum and foot placement as state variables in the linear inverted pendulum model has expanded the realm of possibilities for the control of bipedal robots.","cats":{"new-dataset":0}}
{"text":"This new paradigm, known as the ALIP model, has shown effectiveness in cases where a robot's center of mass height can be assumed to be constant or near constant as well as in cases where there are no non-kinematic restrictions on foot placement.","cats":{"new-dataset":0}}
{"text":"Our code is available at https://github.com/amazon-science/codetaskcl-pptf","cats":{"new-dataset":0}}
{"text":"We also show that it is possible to generate fully-synthetic image-annotation pairs to automatically augment any annotated dataset.","cats":{"new-dataset":0}}
{"text":"We present and release a new dataset of 50 manually annotated research articles.","cats":{"new-dataset":1}}
{"text":"The code and trained weights are available at https://github.com/mordecaimalignatius/GAFAR/.","cats":{"new-dataset":0}}
{"text":"We build on existing tools to computationally analyze data retrieved from publicly available databases.","cats":{"new-dataset":0}}
{"text":"Among multiple benchmarks on the KITTI dataset, we achieve new state-of-the-art performance.","cats":{"new-dataset":0}}
{"text":"Codes and models are publicly available at https://github.com/sunlicai/MAE-DFER.","cats":{"new-dataset":0}}
{"text":"We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabilities detected in each individual program including location and function name, which makes the dataset ideal to train LLMs and machine learning algorithms.","cats":{"new-dataset":1}}
{"text":"Our implementation will be publicly available at \\url{https://github.com/ETHRuiGong/PTDiffSeg}.","cats":{"new-dataset":0}}
{"text":"Our dataset covers 520 images of mathematical graphics collected from 450 documents from different disciplines.","cats":{"new-dataset":1}}
{"text":"The classification accuracy of the models in the absence and presence of the two attacks are computed on images from the publicly accessible ImageNet dataset.","cats":{"new-dataset":0}}
{"text":"Furthermore, it facilitates the creation of de-identified datasets for broader 2D image research at major research institutions.","cats":{"new-dataset":0}}
{"text":"State-of-the-art results are achieved even on more detailed part-segmentation, Pascal-Animals, by only training on coarse-grained datasets.","cats":{"new-dataset":0}}
{"text":"Our code will be available at the URL: https://github.com/cofly2014/tsa-mlt.git","cats":{"new-dataset":0}}
{"text":"KiTS21 is a sequel to its first edition in 2019, and it features a variety of innovations in how the challenge was designed, in addition to a larger dataset.","cats":{"new-dataset":1}}
{"text":"Additionally, we introduce Tomatopia, a new, large and challenging dataset of greenhouse tomatoes.","cats":{"new-dataset":1}}
{"text":"TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks.","cats":{"new-dataset":0}}
{"text":"The code for this algorithm will be publicly available.","cats":{"new-dataset":0}}
{"text":"The code and models will be made publicly at \\href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}.","cats":{"new-dataset":0}}
{"text":"In the fields of Experimental and Computational Aesthetics, numerous image datasets have been created over the last two decades.","cats":{"new-dataset":0}}
{"text":"Our code is publicly available at https://github.com/withchencheng/ECML_PKDD_23_Real.","cats":{"new-dataset":0}}
{"text":"We share this visualization and the dataset in the spirit of open science.","cats":{"new-dataset":1}}
{"text":"covLLM was trained with LLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca and synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real abstract datasets.","cats":{"new-dataset":0}}
{"text":"Results demonstrate that training covLLM on the synCovid and abstract pairs datasets performs competitively with ChatGPT and outperforms covLLM trained primarily using the Alpaca dataset.","cats":{"new-dataset":0}}
{"text":"Using the MIMIC-IT dataset, we train a large VLM named Otter.","cats":{"new-dataset":0}}
{"text":"The code will be made available.","cats":{"new-dataset":0}}
{"text":"Project page: https://ba2det.site .","cats":{"new-dataset":0}}
{"text":"This dataset allows for the exploration of complex road connections and situations where lane markings may be absent.","cats":{"new-dataset":1}}
{"text":"Specifically, we benchmark the recognition drop on common detection datasets, where we evaluate both traditional and realistic anonymization for faces and full bodies.","cats":{"new-dataset":0}}
{"text":"With this CNN, we derived homogeneous atmospheric parameters and abundances for 841300 stars, that remarkably compared to external data-sets.","cats":{"new-dataset":0}}
{"text":"The final trained model is publicly available at https://github.com/Jesper-Karsten/MBASC","cats":{"new-dataset":0}}
{"text":"All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM.","cats":{"new-dataset":0}}
{"text":"In this paper, we propose a detector with the ability to predict both open-vocabulary objects and their part segmentation.","cats":{"new-dataset":0}}
{"text":"First, we train the detector on the joint of part-level, object-level and image-level data to build the multi-granularity alignment between language and image.","cats":{"new-dataset":0}}
{"text":"In light of this, the paper aims to analyze the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face, which is the most popular repository for pretrained ML models.","cats":{"new-dataset":0}}
{"text":"Current methods rely on datasets with expensive annotations; multi-view images and their camera parameters.","cats":{"new-dataset":0}}
{"text":"The WHOW-KG consists of a network of five ontologies and related linked open data, modelled according to those ontologies.","cats":{"new-dataset":0}}
{"text":"We propose Multi-CrossRE, the broadest multi-lingual dataset for RE, including 26 languages in addition to English, and covering six text domains.","cats":{"new-dataset":1}}
{"text":"We run a baseline model over the 26 new datasets and--as sanity check--over the 26 back-translations to English.","cats":{"new-dataset":1}}
{"text":"We also evaluate performance on the MultiMedQA suite of benchmark datasets.","cats":{"new-dataset":0}}
{"text":"Our model and code are available at https://github.com/microsoft/LMOps.","cats":{"new-dataset":0}}
{"text":"To train and evaluate our approach, we curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories.","cats":{"new-dataset":1}}
{"text":"We also release codebase for evaluation set extraction.","cats":{"new-dataset":0}}
{"text":"Dataset, to fight the bias prevalent in giant datasets.","cats":{"new-dataset":0}}
{"text":"We will make our code and pre-trained models publicly available.","cats":{"new-dataset":0}}
{"text":"We perform an extensive study across six datasets with eight models from three model families.","cats":{"new-dataset":0}}
{"text":"For this, we augment standard bug-fixing datasets with bug report discussions.","cats":{"new-dataset":0}}
{"text":"Resources are made publicly available through GitHub, fostering open research in the Chinese NLP community and beyond.","cats":{"new-dataset":0}}
{"text":"We release our code and data under fully permissive licenses.","cats":{"new-dataset":1}}
{"text":"For enabling the combination of ChatGPT and RDF KGs, we present a research prototype, called GPToLODS, which is able to enrich any ChatGPT response with more information from hundreds of RDF KGs.","cats":{"new-dataset":0}}
{"text":"In particular, it identifies and annotates each entity of the response with statistics and hyperlinks to LODsyndesis KG (which contains integrated data from 400 RDF KGs and over 412 million entities).","cats":{"new-dataset":0}}
{"text":"Numerous AIGC detectors have been developed and evaluated on natural language data.","cats":{"new-dataset":0}}
{"text":"We evaluated six AIGC detectors, including three commercial and three open-source solutions, assessing their performance on this dataset.","cats":{"new-dataset":0}}
{"text":"We have released a dataset comprised of ChatGPT's responses to support further research in this area.","cats":{"new-dataset":1}}
{"text":"Both datasets involve scraping through known data sources (through platforms like stack overflow, crowdsourcing, etc.)","cats":{"new-dataset":0}}
{"text":"We call the collected dataset the Human ChatGPT Comparison Corpus (HC3).","cats":{"new-dataset":1}}
{"text":"The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.","cats":{"new-dataset":1}}
{"text":"For datasets originally without shared tokens in label names, we also offer an automated method, using OpenAI's ChatGPT, to generate shared actions and objects.","cats":{"new-dataset":0}}
{"text":"We make our code, models, and datasets publicly available.","cats":{"new-dataset":1}}
{"text":"The source code will be released publicly.","cats":{"new-dataset":0}}
{"text":"We evaluate PaTeCon on two large-scale datasets based on Wikidata and Freebase respectively.","cats":{"new-dataset":0}}
{"text":"We present a novel synthetic dataset, named Parcel3D, that is based on the Google Scanned Objects (GSO) dataset and consists of more than 13,000 images of parcels with full 3D annotations.","cats":{"new-dataset":1}}
{"text":"The dataset contains intact, i.e. cuboid-shaped, parcels and damaged parcels, which were generated in simulations.","cats":{"new-dataset":1}}
{"text":"Dataset and code are available at https://a-nau.github.io/parcel3d.","cats":{"new-dataset":1}}
{"text":"Datasets, code and results are made publicly available and will be continuously updated at https://github.com/ZhaomingKong/Denoising-Comparison.","cats":{"new-dataset":1}}
{"text":"Specifically, we collect a dataset of e-textbooks from one of the largest free online publishers in the world.","cats":{"new-dataset":1}}
{"text":"The dataset and data analysis scripts are available on our open-source repository.","cats":{"new-dataset":1}}
{"text":"To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset.","cats":{"new-dataset":1}}
{"text":"Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks.","cats":{"new-dataset":1}}
{"text":"The source code and the proposed dataset are publicly available at https://github.com/fh2019ustc/DocTr-Plus.","cats":{"new-dataset":1}}
{"text":"Hence, we present a dataset that exclusively consists of healthy and diseased cashew leaves and fruits.","cats":{"new-dataset":1}}
{"text":"The entire code and models will be open-sourced.","cats":{"new-dataset":0}}
{"text":"With nationwide coverage, real-world network topology, and rich geospatial features, this data repository can be used for a variety of traffic-related tasks.","cats":{"new-dataset":0}}
{"text":"The data and code are available on GitHub (https://github.com/baixianghuang/travel).","cats":{"new-dataset":1}}
{"text":"Code and models will be accessed at https://github.com/Liuxinyv/SAZS.","cats":{"new-dataset":0}}
{"text":"BenchMD combines 19 publicly available datasets for 7 medical modalities, including 1D sensor data, 2D images, and 3D volumetric scans.","cats":{"new-dataset":0}}
{"text":"We introduce the LongForm dataset, which is created by leveraging English corpus examples with augmented instructions.","cats":{"new-dataset":1}}
{"text":"We publicly release our data and models: https://github.com/akoksal/LongForm.","cats":{"new-dataset":1}}
{"text":"Code is generated using a commercial tool, SonarCloud.","cats":{"new-dataset":0}}
{"text":"Our source code will be available at https://github.com/MC-E/DragonDiffusion.","cats":{"new-dataset":0}}
{"text":"An open-source implementation is available online.","cats":{"new-dataset":0}}
{"text":"Deep-learning-based object detection and semantic segmentation models have proven to be suitable for this purpose.","cats":{"new-dataset":0}}
{"text":"We evaluate the effectiveness of this approach by training a semantic segmentation model on a real dataset augmented in two ways: 1) using synthetic images obtained from real masks, and 2) generating images from synthetic semantic masks.","cats":{"new-dataset":0}}
{"text":"The dataset spans seven sub-topics and is annotated with a materials-science focused multi-label annotation scheme for AZ.","cats":{"new-dataset":0}}
{"text":"The code is available at \\url{https://github.com/cjw2021/SOV-STG}.","cats":{"new-dataset":0}}
{"text":"The code and pretrained models will be released under https://github.com/lik1996/iCMFormer.","cats":{"new-dataset":0}}
{"text":"The data samples are automatically generated from a curated set of reasoning patterns, where the patterns are annotated with inference labels by experts.","cats":{"new-dataset":0}}
{"text":"The data is obtained from a fleet of gas sensors that measure and track quantities such as oxygen and sound.","cats":{"new-dataset":1}}
{"text":"With the help of this data, we can detect events such as occupancy in a specific environment.","cats":{"new-dataset":0}}
{"text":"Extensive experiments and visualizations on four datasets demonstrate the powerful performance of our method.","cats":{"new-dataset":0}}
{"text":"Codes will be available.","cats":{"new-dataset":0}}
{"text":"Extensive evaluation on three benchmark datasets using multiple evaluation metrics show the effectiveness of the proposed framework.","cats":{"new-dataset":0}}
{"text":"Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name.","cats":{"new-dataset":0}}
{"text":"This property of the dataset makes it suitable for evaluating the effectiveness of various static and dynamic analysis tools.","cats":{"new-dataset":0}}
{"text":"While prior research demonstrated the high performance of ChatGPT across numerous NLP tasks, open-source LLMs like HugginChat and FLAN are gaining attention for their cost-effectiveness, transparency, reproducibility, and superior data protection.","cats":{"new-dataset":0}}
{"text":"With extensive experiments on a large-scale real-world dataset, we demonstrate the substantial effectiveness of our approach.","cats":{"new-dataset":0}}
{"text":"In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-date model repositories for model querying, legal compliance analysis between different model licenses, and copyright issues and intellectual property protection in model reusing.","cats":{"new-dataset":0}}
{"text":"However, little is known about how much training data they require, and how this number depends on the data structure.","cats":{"new-dataset":0}}
{"text":"Our code is available at https://github.com/siyi-wind/MDViT.","cats":{"new-dataset":0}}
{"text":"Along with the datasets, we also propose corresponding baseline solutions to the three aforementioned tasks.","cats":{"new-dataset":1}}
{"text":"These attacks are launched on three powerful pre-trained image classifier architectures, ResNet-34, GoogleNet, and DenseNet-161.","cats":{"new-dataset":0}}
{"text":"This dataset comprises a large number of tasks that demand problem-solving skills.","cats":{"new-dataset":1}}
{"text":"The implementation of CAME is publicly available.","cats":{"new-dataset":0}}
{"text":"However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions.","cats":{"new-dataset":0}}
{"text":"Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios.","cats":{"new-dataset":1}}
{"text":"In this paper, we introduce ScalOTA, an end-to-end scalable OTA software update architecture and secure protocol for modern vehicles.","cats":{"new-dataset":0}}
{"text":"Our code is available at https://github.com/yuping-wu/PULSAR.","cats":{"new-dataset":0}}
{"text":"On both \\pascal and \\coco datasets, we conduct extensive experiments to evaluate the framework effectiveness.","cats":{"new-dataset":0}}
{"text":"Our source code and the related paper list are available on https://github.com/SLDGroup/survey-zero-shot-nas.","cats":{"new-dataset":0}}
{"text":"Extensive experiments show our method achieves state-of-the-art results on the HMDB51 and UCF101 datasets and a competitive result on the benchmark of Kinetics and something-2-something V2 datasets.","cats":{"new-dataset":0}}
{"text":"Experimental results on real-world datasets demonstrate that our method achieves better performance compared with several state-of-the-art social bot detection methods.","cats":{"new-dataset":0}}
{"text":"However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time full-body capture while being accurate in world space.","cats":{"new-dataset":0}}
{"text":"In this work, we contribute a sequential proxy-to-motion learning scheme together with a proxy dataset of 2D skeleton sequences and 3D rotational motions in world space.","cats":{"new-dataset":0}}
{"text":"More video results can be found at our project page: https://liuyebin.com/proxycap.","cats":{"new-dataset":0}}
{"text":"In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world.","cats":{"new-dataset":1}}
{"text":"We address this issue by introducing a new dataset: GHOSTS.","cats":{"new-dataset":1}}
{"text":"This paper also contributes a new surveillance dataset called NightSuR.","cats":{"new-dataset":1}}
{"text":"We present in this work a new Universal Morphology dataset for Korean.","cats":{"new-dataset":1}}
{"text":"The PIKS technique is scalable and can readily ingest new datasets.","cats":{"new-dataset":0}}
{"text":"Datasets that reflect the diverse array of multimodal, multilingual news sources available online could be used to teach models to benefit from this shift, but existing news video datasets focus on traditional news broadcasts produced for English-speaking audiences.","cats":{"new-dataset":0}}
{"text":"The MIND dataset is at the moment of writing the most extensive dataset available for the research and development of news recommender systems.","cats":{"new-dataset":1}}
{"text":"We also introduce a new dataset generated by our classifier that tracks the dynamics of fake news in the Chinese language during the early pandemic.","cats":{"new-dataset":1}}
{"text":"Our code and data will be released shortly.","cats":{"new-dataset":0}}
{"text":"This paper introduces a novel dataset","cats":{"new-dataset":1}}
{"text":"We present a unique collection of data","cats":{"new-dataset":1}}
{"text":"In this study, we unveil our newly created dataset","cats":{"new-dataset":1}}
{"text":"This article brings to light a freshly curated dataset","cats":{"new-dataset":1}}
{"text":"We reveal an unprecedented dataset in the field of","cats":{"new-dataset":1}}
{"text":"In this work, we propose a pioneering dataset","cats":{"new-dataset":1}}
{"text":"This research details the compilation of a new dataset","cats":{"new-dataset":1}}
{"text":"Our article describes a groundbreaking data collection","cats":{"new-dataset":1}}
{"text":"We are pleased to introduce an innovative dataset","cats":{"new-dataset":1}}
{"text":"This publication centers around our newly assembled dataset","cats":{"new-dataset":1}}
{"text":"Herein, we unfold a freshly minted dataset","cats":{"new-dataset":1}}
{"text":"Our research entails the design and creation of a new dataset","cats":{"new-dataset":1}}
{"text":"The crux of this paper is the disclosure of a new dataset","cats":{"new-dataset":1}}
{"text":"This manuscript details the formulation of a novel data assembly","cats":{"new-dataset":1}}
{"text":"We exhibit an inventive dataset for the first time in this paper","cats":{"new-dataset":1}}
{"text":"We present an exploratory dataset in this research","cats":{"new-dataset":1}}
{"text":"This paper features the unveiling of a hitherto unseen dataset","cats":{"new-dataset":1}}
{"text":"In the present study, we showcase an original data compilation","cats":{"new-dataset":1}}
{"text":"Our work represents the first public exhibition of a unique dataset","cats":{"new-dataset":1}}
{"text":"We draw back the curtains on a state-of-the-art dataset in this research","cats":{"new-dataset":1}}
{"text":"We will release our dataset and code for future research.","cats":{"new-dataset":0}}
{"text":"The real-world datasets will be released.","cats":{"new-dataset":1}}
{"text":"In an effort to accelerate this research and assist others, we are releasing our dataset and model to the research community.","cats":{"new-dataset":0}}
{"text":"To promote progress towards this goal, we release OBJECT: a dataset consisting of 400K editing examples created from procedurally generated 3D scenes.","cats":{"new-dataset":1}}
{"text":"This article presents DataXploreFines, an innovative Shiny application that revolutionizes data exploration, analysis, and visualization.","cats":{"new-dataset":0}}
{"text":"Users can upload their datasets in popular formats like CSV or Excel, explore the data structure, perform manipulations, and obtain statistical summaries.","cats":{"new-dataset":0}}
{"text":"DataXploreFines provides a wide range of interactive visualizations, including histograms, scatter plots, bar charts, and line graphs, enabling users to identify patterns and trends.","cats":{"new-dataset":0}}
{"text":"Experimental results demonstrate that our proposed approach outperforms the state-of-the-art methods on fine-grained datasets under real-world scenarios.","cats":{"new-dataset":0}}
{"text":"We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs.","cats":{"new-dataset":0}}
{"text":"Substantial progress has been made recently in motion data collection technologies and generation methods, laying the foundation for increasing interest in human motion generation.","cats":{"new-dataset":0}}
{"text":"Additionally, we provide an overview of common datasets and evaluation metrics.","cats":{"new-dataset":0}}
{"text":"For Amharic, we use our own publicly-available Amharic Speech Emotion Dataset (ASED).","cats":{"new-dataset":0}}
{"text":"For English, German and Urdu we use the existing RAVDESS, EMO-DB and URDU datasets.","cats":{"new-dataset":0}}
{"text":"Our dataset is available at https://github.com/iamazxl/OAVQA.","cats":{"new-dataset":0}}
{"text":"Concretely, we first summarize the widely-used ST ocean datasets and identify their unique characteristics.","cats":{"new-dataset":0}}
{"text":"In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples.","cats":{"new-dataset":0}}
{"text":"To address this, we propose a large-scale SlowTV dataset curated from YouTube, containing an order of magnitude more data than existing automotive datasets.","cats":{"new-dataset":1}}
{"text":"13k sentence pairs) and a web-domain corpus (approx.","cats":{"new-dataset":1}}
{"text":"We release the resulting corpus and our analysis pipeline for future research.","cats":{"new-dataset":1}}
{"text":"The corpora have been annotated with dependency trees, lemmas, and part-of-speech tags.","cats":{"new-dataset":1}}
{"text":"Therefore, we propose that LLM-assisted annotation is a promising automated approach for corpus studies.","cats":{"new-dataset":0}}
{"text":"In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information.","cats":{"new-dataset":0}}
{"text":"This article proposes a new training model to solve this problem through NLP processing methods.","cats":{"new-dataset":0}}
{"text":"We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus.","cats":{"new-dataset":0}}
{"text":"Starting with an initial text corpus, our framework employs a large language model to select multiple sentences that describe the same scene from various perspectives.","cats":{"new-dataset":0}}
{"text":"Experimental results on multiple benchmark datasets demonstrate the effectiveness of our method.","cats":{"new-dataset":0}}
{"text":"Our proposed methods achieve state-of-the-art results on three popular benchmark datasets, and the source code will be made publicly available shortly.","cats":{"new-dataset":0}}
{"text":"We perform an exhaustive evaluation in two benchmark datasets.","cats":{"new-dataset":0}}
{"text":"We conduct experiments on two benchmark datasets.","cats":{"new-dataset":0}}
{"text":"Extensive experiments conducted on two benchmark datasets show that our approach achieves excellent performance compared to its competitors.","cats":{"new-dataset":0}}
{"text":"The proposed model is validated through extensive experiments on two benchmark datasets, showcasing superior performance compared to existing methods.","cats":{"new-dataset":0}}
{"text":"With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice.","cats":{"new-dataset":1}}
{"text":"Experimental results indicate that the proposed approach can obtain better performance on benchmark datasets compared with baselines.","cats":{"new-dataset":0}}
{"text":"We validate our scheme with some of the most popular benchmarking datasets.","cats":{"new-dataset":0}}
{"text":"Method: We present an evaluation of five open-source and four proprietary tools against a benchmark dataset.","cats":{"new-dataset":0}}
{"text":"To testify the effectiveness and superiority of the proposed approach, we conduct extensive experiments on benchmark datasets.","cats":{"new-dataset":0}}
{"text":"We also introduce a new dataset for benchmarking, and the evaluations are performed from four different perspectives including quantitative metrics, visual effects, human ratings and computational cost.","cats":{"new-dataset":1}}
{"text":"Our method is simple yet it outperforms several state-of-the-art methods on six popular dataset benchmarks.","cats":{"new-dataset":0}}
{"text":"Finally, we conduct extensive experiments on widely-used benchmark datasets to validate the superiority of our method by comparing it with existing state-of-the-art methods.","cats":{"new-dataset":0}}
{"text":"We validate the model both qualitatively and quantitatively on four benchmark datasets.","cats":{"new-dataset":0}}
{"text":"Finally, I provide the first high-quality benchmark dataset in order to fairly compare existing models and aid future model developers.","cats":{"new-dataset":0}}
{"text":"Finally, we applied our method on two benchmark datasets, STACOM2018, and M\\&Ms 2020 challenges, to show the potency of the proposed model.","cats":{"new-dataset":0}}
{"text":"Extensive experiments on several benchmark datasets show that our method outperforms existing methods across all datasets while maintaining low computational complexity.","cats":{"new-dataset":0}}
{"text":"Our data and benchmarking results are available at: https://lmexam.com.","cats":{"new-dataset":0}}
{"text":"Empirically, we conduct extensive experiments on several benchmark datasets to support our theory.","cats":{"new-dataset":0}}
{"text":"Extensive experiments are conducted on multiple benchmark data sets and our method establishes a state-of-the-art performance in terms of both performance and trustworthiness.","cats":{"new-dataset":0}}
{"text":"We evaluate our model on four datasets and achieve state-of-the-art performances.","cats":{"new-dataset":0}}
{"text":"Experimental results demonstrate the performance and limitations of existing algorithms, and the dataset benchmark has good versatility and effectiveness.","cats":{"new-dataset":0}}
{"text":"Our extensive experiments on three benchmarks, Lucchi, MitoEM-R and MitoEM-H, reveal the benefits of the proposed contributions achieving state-of-the-art results on all three datasets.","cats":{"new-dataset":0}}
{"text":"We test our method on two public datasets, our method achieves the best performances on these two datasets.","cats":{"new-dataset":0}}
{"text":"Finally, the experimental results on several benchmark datasets verify the effectiveness of the proposed method.","cats":{"new-dataset":0}}
{"text":"Extensive experiments on two real-world datasets show the superior performance of our method.","cats":{"new-dataset":0}}
{"text":"We have evaluated various baselines on this dataset and benchmarked it with a new neural model, SPOT, which we introduce in this paper.","cats":{"new-dataset":0}}
{"text":"The benchmark dataset, leaderboard, and baseline methods are released in https://github.com/THU-KEG/KoRC.","cats":{"new-dataset":1}}
{"text":"Experimental results on real-world and benchmark datasets validate the effectiveness of the proposed method.","cats":{"new-dataset":0}}
{"text":"Our results improve the state-of-the-art on standard benchmarks.","cats":{"new-dataset":0}}
{"text":"Experimental evaluations both on our assembled dataset and public benchmark datasets demonstrate the effectiveness of our proposed network.","cats":{"new-dataset":0}}
{"text":"Our results show that our simple synthetic datasets are sufficient to challenge most of the benchmarked methods.","cats":{"new-dataset":1}}
{"text":"Additionally, we introduce a novel benchmark based on images from the Open Images Dataset.","cats":{"new-dataset":0}}
{"text":"As a result, our method TOLD achieves a DER of 10.14% on the CALLHOME dataset, which is a new state-of-the-art result on this benchmark to the best of our knowledge.","cats":{"new-dataset":0}}
{"text":"We conducted comprehensive experiments on multiple benchmark datasets, demonstrating the superior performance of our proposed SPIFFNet in terms of both quantitative metrics and visual quality when compared to state-of-the-art methods.","cats":{"new-dataset":0}}
{"text":"We provide a detailed analysis of the dataset.","cats":{"new-dataset":0}}
{"text":"We evaluate our method on three challenging datasets.","cats":{"new-dataset":0}}
{"text":"The benchmark results produced by three different deep learning methods are provided.","cats":{"new-dataset":0}}
{"text":"In addition, we provide extra annotations for used datasets and introduce our new benchmark.","cats":{"new-dataset":1}}
{"text":"Experimental results on two real-world datasets demonstrate that our method outperforms some state-of-the-art approaches.","cats":{"new-dataset":0}}
{"text":"Experiments on real-world datasets demonstrate the effectiveness of our approach.","cats":{"new-dataset":0}}
{"text":"The extensive experiments demonstrated that our proposed method achieves state-of-the-art performance on the newly collected dataset.","cats":{"new-dataset":1}}
{"text":"We evaluate our method on several datasets and demonstrate its superior performance under heavily occluded scenarios compared to other methods.","cats":{"new-dataset":0}}
{"text":"The dataset and source code will be released on GitHub soon.","cats":{"new-dataset":0}}
{"text":"Then this model is trained and evaluated on the new, more extensive dataset to obtain a representative result.","cats":{"new-dataset":0}}
{"text":"Results show that the performance significantly increases with the dataset size.","cats":{"new-dataset":0}}
{"text":"Details are available in CSV files provided with the datasets.   ","cats":{"new-dataset":1}}
{"text":"Since the manual creation of such datasets is a laborious task, obtaining data from online resources can be a cheap solution to create large-scale datasets.","cats":{"new-dataset":0}}
{"text":"Data format and usage notes:","cats":{"new-dataset":0}}
{"text":"However, online companies also gather user data for more principled purposes, such as improving the user experience and aggregating statistics.","cats":{"new-dataset":0}}
{"text":"Finally, we provide publicly an open dataset, and online resources with the results.","cats":{"new-dataset":1}}
{"text":"Besides the traditional data, various types of data, including video, have become available.","cats":{"new-dataset":0}}
{"text":"Besides, with such an instruction, we can also easily carry out quantitative statistics.","cats":{"new-dataset":0}}
{"text":"Moreover, limitations related to data sources that change over time (e.g., code bases) and the lack of documentation of extraction processes make it difficult to reproduce datasets over time.","cats":{"new-dataset":0}}
{"text":"Evaluation datasets and frameworks like the one we present support this line of research.","cats":{"new-dataset":0}}
{"text":"We then review available datasets, recent approaches and evaluation metrics of the task.","cats":{"new-dataset":0}}
{"text":"Besides, our extensive analyses verify the high quality of our dataset and the effectiveness of our evaluation metrics.","cats":{"new-dataset":1}}
{"text":"However, we identify issues with the dataset quality and evaluation metric.","cats":{"new-dataset":0,"data-quality":1}}
{"text":"In order to obtain meaningful results, careful partitioning of data into training, validation, and test sets, as well as the selection of suitable evaluation metrics are crucial.","cats":{"new-dataset":0}}
{"text":"To evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metrics).","cats":{"new-dataset":0}}
{"text":"These metrics have been successful on datasets that leverage the average human perception in limited settings.","cats":{"new-dataset":0}}
{"text":"We use standard metrics to evaluate the performances of the different training scenarios.","cats":{"new-dataset":0}}
{"text":"Finally, we report extensive experiments on two real-world datasets to offer insight into the efficiency, scalability, and effectiveness of the proposed framework.","cats":{"new-dataset":0}}
{"text":"Here we describe Athena 2.0, UCSC's conversational agent for Amazon's Socialbot Grand Challenge 4.","cats":{"new-dataset":0}}
{"text":"The Alexa Prize Challenge aims to create a socialbot, which allows the user to engage in coherent conversations, on a range of popular topics that will interest the user.","cats":{"new-dataset":0}}
{"text":"Athena 2.0 also relies on a user model to personalize topic selection and other aspects of the conversation to individual users.","cats":{"new-dataset":0}}
{"text":"Athena 2.0 utilizes a novel knowledge-grounded discourse model that tracks the entity links that Athena introduces into the dialogue, and uses them to constrain named-entity recognition and linking, and coreference resolution.","cats":{"new-dataset":0}}
{"text":"Code and data are publicly available on GitHub.","cats":{"new-dataset":0}}
{"text":"The code is publicly available.","cats":{"new-dataset":0}}
{"text":"The code and models will be publicly available.","cats":{"new-dataset":0}}
{"text":"We make the source code and models publicly available.","cats":{"new-dataset":0}}
{"text":"Our code is publicly available.","cats":{"new-dataset":0}}
{"text":"Code will be publicly available.","cats":{"new-dataset":0}}
{"text":"Our data and code are publicly available.","cats":{"new-dataset":0}}
{"text":"Code is publicly released.","cats":{"new-dataset":0}}
{"text":"The code will be accessible to the public.","cats":{"new-dataset":0}}
{"text":"Code and data will be made publicly available.","cats":{"new-dataset":0}}
{"text":"Code will be made publicly available.","cats":{"new-dataset":0}}
{"text":"Our data and code are publicly available at: https://pointodyssey.com","cats":{"new-dataset":1}}
{"text":"The code will be made publicly available.","cats":{"new-dataset":0}}
{"text":"The source code and dataset are available at project page.","cats":{"new-dataset":1}}
{"text":"Project page is https://damo-vilab.github.io/AnyDoor-Page/.","cats":{"new-dataset":0}}
{"text":"Source code is public, and can be accessed at: https://github.com/ApplicationTechnologyOfMedicalBigData/pFedNet-code.","cats":{"new-dataset":0}}
{"text":"Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.","cats":{"new-dataset":0}}
{"text":"Our code will be publicly available.","cats":{"new-dataset":0}}
{"text":"In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes not only multi-view RGB videos of outdoor scenes but also corresponding pointclouds.","cats":{"new-dataset":1}}
{"text":"Furthermore, we propose a 3D human pose estimation algorithm based on multi-modal data input, which demonstrates the advantages of multi-modal data input for 3D human pose estimation.","cats":{"new-dataset":0}}
{"text":"3D human pose estimation in outdoor environments has garnered increasing attention recently.","cats":{"new-dataset":0}}
{"text":"We introduce a novel one-stage end-to-end multi-person 2D pose estimation algorithm, known as Joint Coordinate Regression and Association (JCRA), that produces human pose joints and associations without requiring any post-processing.","cats":{"new-dataset":0}}
{"text":"However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize only one type of modality (RGB image or pointcloud), and often feature only one individual within each scene.","cats":{"new-dataset":0}}
{"text":"iEDA is publicly available from the project home page http://ieda.oscc.cc.","cats":{"new-dataset":0}}
{"text":"The codes and data are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.","cats":{"new-dataset":0}}
{"text":"Our extensive set of experiments on a variety of standard and few-shot datasets show that our method produces substantially improved performance when compared to the current state of the art methods.","cats":{"new-dataset":0}}
{"text":"Our benchmark reflects real-world data constraints by evaluating methods across a range of dataset sizes, including challenging few-shot settings that incentivize the use of pretraining.","cats":{"new-dataset":0}}
{"text":"Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur.","cats":{"new-dataset":0}}
{"text":"We demonstrate that our method achieves state-of-the-art results on several challenging benchmark datasets even in comparison to recent supervised methods, and that our method reaches previously unseen cross-dataset generalisation ability.","cats":{"new-dataset":0}}
{"text":"Extending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions.","cats":{"new-dataset":0}}
{"text":"Traditional zero-shot learning methods are constrained by the training dataset.","cats":{"new-dataset":0}}
{"text":"Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yielding a broad capture range even on unseen anatomies and modality pairs, without the need for specialized retraining.","cats":{"new-dataset":0}}
{"text":"Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.","cats":{"new-dataset":0}}
{"text":"Combining both, our method (ProVP-Ref) is evaluated on 11 image benchmark datasets and achieves 7/11 state-of-theart results on both few-shot and base-to-novel settings.","cats":{"new-dataset":0}}
{"text":"We perform experiments on simulated and retrospective in-vivo data to evaluate the performance of the proposed zero-shot learning method for temporal FSE reconstruction.","cats":{"new-dataset":0}}
{"text":"Finally, we empirically validate our methods with extensive experiments on three established biomedical benchmarks.","cats":{"new-dataset":0}}
{"text":"Extensive experiments are conducted on multiple benchmark datasets to validate the effectiveness of our proposed method.","cats":{"new-dataset":0}}
{"text":"We have quantitatively and qualitatively validated our method on three common benchmark datasets: Human3.6M, MPI-INF-3DHP, and HumanEva.","cats":{"new-dataset":0}}
{"text":"Extensive experiments on six datasets show substantial improvements to the baseline.","cats":{"new-dataset":0}}
{"text":"Besides, since medical OSR is still a nascent field, two publicly available benchmark datasets are proposed for comparison.","cats":{"new-dataset":0}}
{"text":"In this study, we conducted experiments using medical datasets comprising only 100 samples from three medical modalities.","cats":{"new-dataset":0}}
{"text":"To validate our model design, we conduct extensive experiments on three benchmark datasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP.","cats":{"new-dataset":0}}
{"text":"We conduct extensive experiments on three public benchmarks (i.e., HumanEval, MBPP, and MBCPP).","cats":{"new-dataset":0}}
{"text":"Comprehensive experiments are conducted on public benchmarks and the corresponding experimental results demonstrate the effectiveness of our proposed method.","cats":{"new-dataset":0}}
{"text":"We have conducted experiments on two real datasets and compare our method with three baselines.","cats":{"new-dataset":0}}
{"text":"We have theoretically analyzed our method and empirically evaluated it on two real-world medical datasets.","cats":{"new-dataset":0}}
{"text":"We then describe the dataset and the results of benchmarking.","cats":{"new-dataset":0}}
{"text":"ModFed is evaluated on three in-vivo datasets.","cats":{"new-dataset":0}}
{"text":"To this end, we introduce a new data curation pipeline to construct a culturally relevant parallel corpus, enriched with annotations of cultural-specific entities.","cats":{"new-dataset":0}}
{"text":"Furthermore, while engineers have developed cutting-edge technologies for image classification, there is still a gap in the application of these models in human heritage collections, where data sets usually consist of low-quality pictures of people with diverse ethnicity, gender, and age.","cats":{"new-dataset":0}}
{"text":"We evaluated the effectiveness of the bias mitigation pipeline on a cultural heritage collection of photographs from the 19th and 20th centuries, and we used the FairFace data set for the transfer learning experiments.","cats":{"new-dataset":0}}
{"text":"This paper answers the need to support GLAM institutions in facilitating the transition into publishing their digital content and to introduce collections as data services; this will also help their future efficient contribution to data spaces and cultural heritage clouds.","cats":{"new-dataset":0}}
{"text":"Within the current transition to the emerging data spaces, clouds for cultural heritage and open science, the need to identify practices which support more GLAM institutions to offer datasets becomes a priority, especially within the smaller and medium-sized institutions.   ","cats":{"new-dataset":0}}
{"text":"Creating an intelligent search and retrieval system for artwork images, particularly paintings, is crucial for documenting cultural heritage, fostering wider public engagement, and advancing artistic analysis and interpretation.","cats":{"new-dataset":0}}
{"text":"We will release our annotation scheme, the corpus, and codes to the research community to alleviate the scarcity of labeled data in this domain.","cats":{"new-dataset":1,"data-quality":1}}
{"text":"Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities.","cats":{"new-dataset":0}}
{"text":"Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains.","cats":{"new-dataset":1}}
{"text":"Our second contribution is to conduct a comprehensive analysis of the annotations, focusing on how different demographic groups are represented.","cats":{"new-dataset":0}}
{"text":"Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment.","cats":{"new-dataset":0}}
{"text":"Moreover, our method demonstrates high generalization capability in cross-dataset experiments, even when the training and test sets have different characteristics.","cats":{"new-dataset":0}}
{"text":"Our model shows strong performance improvements on the Winoground and VL-checklist datasets with only a mild degradation in zero-shot performance.","cats":{"new-dataset":0}}
{"text":"We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet.","cats":{"new-dataset":0}}
{"text":"We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models.","cats":{"new-dataset":0}}
{"text":"This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality.","cats":{"new-dataset":0}}
{"text":"Experiments on two datasets demonstrate that our proposed method outperforms the baselines and achieves new state-of-the-art performance.","cats":{"new-dataset":0}}
{"text":"Our results show a significant improvement for all the language pairs investigated, better cross-lingual inference, and a superior performance in zero-shot learning scenarios as compared to state-of-the-art baselines.","cats":{"new-dataset":0}}
{"text":"Cross-dataset evaluation further shows the strong generalization ability of our approach.","cats":{"new-dataset":0}}
{"text":"We also tested our model in a zero-shot setting on an independent test site without any additional fine-tuning.","cats":{"new-dataset":0}}
{"text":"Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets.","cats":{"new-dataset":0}}
{"text":"Extensive experiments on real-world degraded datasets demonstrate the effectiveness of our proposed method.","cats":{"new-dataset":0}}
{"text":"We propose a new approach, CLIPPR (CLIP with Priors), which adapts zero-shot models for regression and classification on unlabelled datasets.","cats":{"new-dataset":0}}
{"text":"Extensive experiments show that our model has great cross-dataset generalization.","cats":{"new-dataset":0}}
{"text":"Without resorting to any complex techniques, such as image translation, augmentation, or rare-class sampling, we set a new state-of-the-art on all benchmarks.","cats":{"new-dataset":0}}
{"text":"In addition, our method also surprisingly helps improve the generalization ability of the models under zero-shot settings.","cats":{"new-dataset":0}}
{"text":"We demonstrate the effectiveness of our method by conducting experiments on benchmark datasets for few-shot learning as well as in the real world for robot perception.","cats":{"new-dataset":0}}
{"text":"Existing methods show impressive results on individual datasets but lack the ability to generalize to unseen scenarios.","cats":{"new-dataset":0}}
{"text":"Empirically, our method achieves better performance than all baselines on multiple datasets.","cats":{"new-dataset":0}}
{"text":"Our technique functions is in a zero-shot fashion, as it only utilizes data from a single scan of highly undersampled time series images.","cats":{"new-dataset":0}}
{"text":"As a result, current methods often resort to random sampling from supervised datasets to create \"few-data\" setups and employ inconsistent training strategies during evaluations, which poses a challenge in accurately comparing recent progress.","cats":{"new-dataset":0}}
{"text":"In comparison to existing zero-shot methods, our approach is universally applicable to any retriever without additional adaptation or indexing.","cats":{"new-dataset":0}}
{"text":"However, due to the computational demands associated with training these models, their applications often rely on zero-shot settings.","cats":{"new-dataset":0}}
{"text":"The model can capture more context information from multiple scales and better fuse the local and global information to achieve high-quality segmentation.","cats":{"new-dataset":0}}
{"text":"When benchmarking against popular existing segmentation models across three datasets, our proposed model demonstrates a substantial leap in performance.","cats":{"new-dataset":0}}
{"text":"Extensive experiments on the PASCAL VOC 2012 and Cityscapes datasets demonstrate the superiority and scalability of the proposed method, outperforming the current state-of-the-art approaches.","cats":{"new-dataset":0}}
{"text":"In this paper, we present a framework to enhance depth by leveraging semantic segmentation to guide the network to jump out of the local minimum.","cats":{"new-dataset":0}}
{"text":"The DNMP provides a new paradigm for urban-level scene representation with appealing properties: $(1)$ High-quality rendering.","cats":{"new-dataset":0}}
{"text":"Results on the Cityscapes and Mapillary datasets show the proposed approach achieves significantly more controllability and improved image quality than previous approaches on urban scenes and is on par with general-purpose non-controllable generative models (like StyleGAN2) in terms of quality.","cats":{"new-dataset":0}}
{"text":"To facilitate a unified comparison between novel segmentation algorithms, we propose a standardized evaluation strategy for our dataset.","cats":{"new-dataset":0}}
{"text":"Experimental results on the nuScenes dataset demonstrate that our framework is highly compatible with various map segmentation and detection architectures and considerably strengthens map prediction performance, even under adverse weather conditions and across longer horizons.","cats":{"new-dataset":0}}
{"text":"For the high-resolution Cityscapes and Mapillary Vistas datasets, we achieve improvements of up to +2.5 on the Panoptic Quality for thing classes, and even more considerable gains of up to +5.8 on both the pixel accuracy and pixel precision, which we identify as better metrics to capture the confusion problem.","cats":{"new-dataset":0}}
{"text":"Our experimental results demonstrate the effectiveness of our approach in overcoming the challenges posed by urban traffic scenes.","cats":{"new-dataset":0}}
{"text":"To develop and evaluate our approach, a large urban driving dataset dubbed AV Breadcrumbs is automatically labeled by leveraging vector map representations and projective geometry to annotate over 900,000 images.","cats":{"new-dataset":0}}
{"text":"Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years.","cats":{"new-dataset":0}}
{"text":"Extensive experiments show the effectiveness of our method.","cats":{"new-dataset":0}}
{"text":"Extensive experiments demonstrate the effectiveness of our method.","cats":{"new-dataset":0}}
{"text":"We finally conduct extensive analyses to understand the effectiveness of our method.","cats":{"new-dataset":0}}
{"text":"Extensive experiments show the effectiveness of our approach.","cats":{"new-dataset":0}}
{"text":"A series of experiments demonstrate the effectiveness of our methods.","cats":{"new-dataset":0}}
{"text":"Our method is effective and presents a significant improvement over the original model.","cats":{"new-dataset":0}}
{"text":"We show that our method produces highly accurate results.","cats":{"new-dataset":0}}
{"text":"Extensive experiments demonstrate that our method outperforms existing works significantly.","cats":{"new-dataset":0}}
{"text":"Our approach has several advantages over many previous methods.","cats":{"new-dataset":0}}
{"text":"Additionally, we employ the self-training strategy to improve the performance of our method further.","cats":{"new-dataset":0}}
{"text":"our method has three advantages.","cats":{"new-dataset":0}}
{"text":"Extensive experiments are conducted, and the results demonstrate the effectiveness and superiority of our method.","cats":{"new-dataset":0}}
{"text":"Comparison experiments demonstrate the effectiveness of our method.","cats":{"new-dataset":0}}
{"text":"Extensive experimental results confirm the effectiveness of our method.","cats":{"new-dataset":0}}
{"text":"Our method consistently outperforms previous approaches across a range of tasks.","cats":{"new-dataset":0}}
{"text":"Our method enjoys three key technical parts.","cats":{"new-dataset":0}}
{"text":"Our approach yields excellent results, with accuracies that are comparable to or better than those of existing state-of-the-art approaches.","cats":{"new-dataset":0}}
{"text":"Empirical experiments demonstrate the efficacy of our approach.","cats":{"new-dataset":0}}
{"text":"Extensive experiments are conducted to demonstrate the effectiveness of our proposed method.","cats":{"new-dataset":0,"data-quality":0}}
{"text":"Our experimental results demonstrate the effectiveness of our proposed model, outperforming existing methods on three datasets.","cats":{"new-dataset":0}}
{"text":"The method proposed here showed improvement from our precious results on the same dataset.","cats":{"new-dataset":0}}
{"text":"Experiments show that the proposed method achieves state-of-the-art results on the present dataset.","cats":{"new-dataset":0}}
{"text":"Experiments conducted on two dedicated datasets demonstrate the superior performance of the proposed method over recent studies in the literature.","cats":{"new-dataset":0}}
{"text":"Sufficient experiments demonstrate that the proposed method achieves results beyond the state-of-the-art methods on various datasets.","cats":{"new-dataset":0}}
{"text":"Extensive experiments demonstrate that benefiting from the proposed components, we achieve new state-of-the-art performance compared to previous methods on several public datasets.","cats":{"new-dataset":0}}
{"text":"Furthermore, on various datasets, we make competitive achievement results with other previous state-of-the-art methods.","cats":{"new-dataset":0}}
{"text":"Overall, our work carefully studies the effectiveness of popular scoring functions in realistic settings and helps to better understand their limitations.","cats":{"new-dataset":0}}
{"text":"We have evaluated our method on a set of supervised-learning datasets.","cats":{"new-dataset":0}}
{"text":"Our method yields superior performance when dealing with scales outside of those covered by the training dataset.","cats":{"new-dataset":0}}
{"text":"The experimental results demonstrate that the proposed method can bring significant improvements in BLEU scores on two datasets.","cats":{"new-dataset":0}}
{"text":"Compared to a variety of baselines, our method achieves superior results.","cats":{"new-dataset":0,"data-quality":0}}
{"text":"Experiments on real-world datasets prove the significant effectiveness and generalization ability of the proposed method.","cats":{"new-dataset":0}}
{"text":"Extensive evaluations conducted on public datasets and comparison with other methods further confirm the effectiveness of the proposed framework.","cats":{"new-dataset":0}}
{"text":"Experiments on five challenging large-scale public datasets demonstrate that our proposed method is effective and outperforms the state-of-the-art methods.","cats":{"new-dataset":0}}
{"text":"The experimental results on two datasets show the superiority of our methods.","cats":{"new-dataset":0}}
{"text":"Extensive experiments are conducted on seven widely used datasets to demonstrate the effectiveness of the proposed approach.","cats":{"new-dataset":0}}
{"text":"We validate our method on five datasets, empirically demonstrating that it outperforms the baseline methods in most cases and is valid over a wider range of training budgets.","cats":{"new-dataset":0}}
{"text":"Extensive experiments have been implemented on three real life datasets to demonstrate the effectiveness of our proposed algorithm.","cats":{"new-dataset":0}}
{"text":"Extensive experiments are conducted to demonstrate the effectiveness of all the proposed components, and results show that our approach outperforms the state-of-the-art (SOTA) in four representative datasets, both qualitatively and quantitatively.","cats":{"new-dataset":0}}
{"text":"Experimental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives.","cats":{"new-dataset":0}}
{"text":"Extensive experiments with multiple datasets are conducted to demonstrate the effectiveness of our unified solution to all the three tasks and the generalized case.","cats":{"new-dataset":0}}
{"text":"Extensive experiments conducted on three popular public datasets (CASIA-B, OU-MVLP, and GREW) demonstrate that our proposed method achieves state-of-the-art performance on multiple benchmark tests.","cats":{"new-dataset":0}}
{"text":"We demonstrate that our method surpasses previous attempts in qualitative and quantitative results through extensive experiments conducted on benchmark datasets such as Set5, Set14, Urban100, BSD100, and Manga109.","cats":{"new-dataset":0}}
{"text":"Through extensive experiments on three real datasets, we demonstrate the efficiency and effectiveness of our proposed algorithms.","cats":{"new-dataset":0}}
{"text":"We show that our method performs superior to state of-the-art alternatives on various datasets.","cats":{"new-dataset":0}}
{"text":"The statistical and experimental results indicate that our dataset provides the basis for the future improvements of existing methods.","cats":{"new-dataset":0}}
{"text":"Finally, extensive experimental results on three public datasets clearly demonstrate the effectiveness of the proposed model.","cats":{"new-dataset":0}}
{"text":"Experiment results on real-world datasets MLQA demonstrate that the proposed method can improve the performance by a large margin, outperforming the baseline method by 13.18%/12.00% F1/EM on average.","cats":{"new-dataset":0}}
{"text":"Extensive experiments demonstrate that our method outperforms baselines on 16 out of 20 datasets, underlining its effectiveness and superiority in alleviating the heterophily problem.","cats":{"new-dataset":0}}
{"text":"Our methods are complementary to the existing pre-training or data mining approaches and can be used in a variety of settings.","cats":{"new-dataset":0}}
{"text":"Experimental results on two authoritative public datasets demonstrate that our proposed method boosts state-of-the-art performance by a large margin.","cats":{"new-dataset":0}}
{"text":"Upon testing this framework on four distinct real datasets, we find that our synthetic training data are able to yield high-quality results also on real images-even more so if fine-tune on a few real images was done.","cats":{"new-dataset":0}}
{"text":"We show that our framework significantly improves the quality of the resulting synthetic images and is adaptable to unseen data with fine-tuning.","cats":{"new-dataset":0}}
{"text":"To this end, we generate two synthetic datasets and then develop an end-to-end pipeline and model that is tested on both benchmarks.","cats":{"new-dataset":1}}
{"text":"The results on the kgbench dataset with three different embedding methods show promising results.","cats":{"new-dataset":0}}
{"text":"We present our findings that these embeddings are useful beyond the noise prediction task, as they contain discriminative information and can also be leveraged for classification.","cats":{"new-dataset":0}}
{"text":"Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible.","cats":{"new-dataset":0}}
{"text":"Our results show that the inherent data bias that persists in KG can be altered by specific algorithm bias as incorporated by KG embedding learning algorithms.","cats":{"new-dataset":0}}
{"text":"To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an English dataset (SEMEVAL-CCOHA).","cats":{"new-dataset":0}}
{"text":"Most existing graph embedding methods fall short of reaching high data scalability.","cats":{"new-dataset":0}}
{"text":"In this paper, we present a large-scale human-labeled dataset for Russian text recognition in-the-wild.","cats":{"new-dataset":1}}
{"text":"Experiments are conducted on two commonly used KGET datasets to show that the performance of KGE methods on the KGET task can be substantially improved by the proposed multiple auxiliary relations and asynchronous embedding learning.","cats":{"new-dataset":0}}
{"text":"Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time.","cats":{"new-dataset":0}}
{"text":"However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution.","cats":{"new-dataset":0}}
{"text":"External Knowledge Injection:} By fine-tuning special token embeddings for each category via Textual Inversion, performance improves across 17 datasets, except when dealing with low-resolution reference images.   ","cats":{"new-dataset":0}}
{"text":"However, existing embedding architectures suffer from two limitations: (1) limited discriminability of synthetic features' embedding without considering fine-grained cluster structures; (2) inflexible optimization due to restricted scaling mechanisms on existing contrastive embedding networks, leading to overlapped representations in the embedding space.","cats":{"new-dataset":0}}
{"text":"Recent works in this domain have achieved significant improvements by the representation learning paradigm, e.g., embedding-based retrieval (EBR) and collaborative filtering (CF).","cats":{"new-dataset":0}}
{"text":"We compare these embeddings to those generated by competing architectures and pre-trainings for classification tasks.","cats":{"new-dataset":0}}
{"text":"While there are many high-quality datasets for English text recognition; there are no available datasets for Russian language.","cats":{"new-dataset":0}}
{"text":"In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and ELMo, on real-world English and Romanian datasets.","cats":{"new-dataset":0}}
{"text":"Moreover, we compared the 3RL dataset with other famous state-of-the-art datasets (FER dataset, CK+ dataset), and we applied the most commonly used algorithms in previous works, SVM and CNN.","cats":{"new-dataset":0}}
{"text":"We focus especially on Task-C and propose a novel LLMs cooperation system named a doctor-patient loop to generate high-quality conversation data sets.","cats":{"new-dataset":1}}
{"text":"This analysis also investigates the potential of utilizing cooperation LLMs to generate high-quality datasets.","cats":{"new-dataset":0}}
{"text":"This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023 shared task for Task-A and Task-C.  ","cats":{"new-dataset":0}}
{"text":"The experiment results demonstrate that our approaches yield reasonable performance as evaluated by automatic metrics such as ROUGE, medical concept recall, BLEU, and Self-BLEU.","cats":{"new-dataset":0}}
{"text":"Furthermore, we conducted a comparative analysis between our proposed method and ChatGPT and GPT-4.","cats":{"new-dataset":0}}
{"text":"We present working notes on transfer learning with semi-supervised dataset annotation for the BirdCLEF 2023 competition, focused on identifying African bird species in recorded soundscapes","cats":{"new-dataset":1}}
{"text":"We explore the embedding space learned by BirdNET and propose a process to derive an annotated dataset for supervised learning","cats":{"new-dataset":1}}
{"text":"Our approach utilizes existing off-the-shelf models, BirdNET and MixIT, to address representation and labeling challenges in the competition.","cats":{"new-dataset":0}}
{"text":"We explore the embedding space learned by BirdNET and propose a process to derive an annotated dataset for supervised learning.","cats":{"new-dataset":0}}
{"text":"Our experiments involve various models and feature engineerih in classifying bird species and highlight the potential of transfer learning and semi-supervised dataset annotation in similar tasks.","cats":{"new-dataset":0}}
{"text":"Our data generator is capable of generating large-scale datasets of human activities","cats":{"new-dataset":1}}
{"text":"We release M3Act3D, an 87.6-hour 3D motion dataset of human activities with larger group sizes and higher complexity of inter-person interactions than previous multi-person datasets","cats":{"new-dataset":1}}
{"text":"The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision.","cats":{"new-dataset":0}}
{"text":"However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets.","cats":{"new-dataset":0}}
{"text":"To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator.","cats":{"new-dataset":0}}
{"text":"Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process.  ","cats":{"new-dataset":0}}
{"text":"with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories).","cats":{"new-dataset":0}}
{"text":"Using M3Act, we perform synthetic data pre-training for 2D skeleton-based group activity recognition and RGB-based multi-person pose tracking.","cats":{"new-dataset":0}}
{"text":"The results indicate that learning from our synthetic datasets largely improves the model performances on real-world datasets, with the highest gain of 5.59% and 7.32% respectively in group and person recognition accuracy on CAD2, as well as an improvement of 6.63 in MOTP on HiEve.","cats":{"new-dataset":0}}
{"text":"Pre-training with our synthetic data also leads to faster model convergence on downstream tasks (up to 6.8% faster).","cats":{"new-dataset":0}}
{"text":"Moreover, M3Act opens new research problems for 3D group activity generation.","cats":{"new-dataset":0}}
{"text":"We release M3Act3D, an 87.6-hour 3D motion dataset of human activities with larger g","cats":{"new-dataset":0}}
{"text":"Large language models typically undergo two training stages, pretraining and finetuning.","cats":{"new-dataset":0}}
{"text":"Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times.","cats":{"new-dataset":0}}
{"text":"To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area.","cats":{"new-dataset":0}}
{"text":"Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data.","cats":{"new-dataset":0}}
{"text":"However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow.","cats":{"new-dataset":0}}
{"text":"In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality.","cats":{"new-dataset":0}}
{"text":"We formulate InstructMining using specific natural language indicators.","cats":{"new-dataset":0}}
{"text":"To investigate the relationship between data quality and these indicators, we further conduct extensive finetuning experiments.","cats":{"new-dataset":0}}
{"text":"The experiment results are then applied to estimating parameters in InstructMining.","cats":{"new-dataset":0}}
{"text":"To further investigate its performance, we use InstructMining to select high-quality data from unseen datasets.","cats":{"new-dataset":0}}
{"text":"Results demonstrate that InstructMining can help select relatively high-quality samples from various instruction-following datasets.","cats":{"new-dataset":0}}
{"text":"Compared to models finetuned on unfiltered datasets, models finetuned on InstructMining selected datasets perform better on 42.5% cases.","cats":{"new-dataset":0}}
{"text":"Therefore, we introduce the Infrastructural Multi-Person Trajectory and Context Dataset (IMPTC).","cats":{"new-dataset":1}}
{"text":"The resulting dataset consists of eight hours of measurement data","cats":{"new-dataset":1}}
{"text":"In addition, to enable the entire stack of research capabilities, the dataset includes all data, starting from the sensor-, calibration- and detection data until trajectory and context data.","cats":{"new-dataset":1}}
{"text":"The dataset is continuously expanded and is available online for non-commercial research at https://github.com/kav-institute/imptc-dataset.","cats":{"new-dataset":1}}
{"text":"Inner-city intersections are among the most critical traffic areas for injury and fatal accidents.","cats":{"new-dataset":0}}
{"text":"Automated vehicles struggle with the complex and hectic everyday life within those areas.","cats":{"new-dataset":0}}
{"text":"Sensor-equipped smart infrastructures, which can cooperate with vehicles, can benefit automated traffic by extending the perception capabilities of drivers and vehicle perception systems.","cats":{"new-dataset":0}}
{"text":"Additionally, they offer the opportunity to gather reproducible and precise data of a holistic scene understanding, including context information as a basis for training algorithms for various applications in automated traffic.  ","cats":{"new-dataset":0}}
{"text":"We use an intelligent public inner-city intersection in Germany with visual sensor technology.","cats":{"new-dataset":0}}
{"text":"A multi-view camera and LiDAR system perceives traffic situations and road users' behavior.","cats":{"new-dataset":0}}
{"text":"Additional sensors monitor contextual information like weather, lighting, and traffic light signal status.","cats":{"new-dataset":0}}
{"text":"The data acquisition system focuses on Vulnerable Road Users (VRUs) and multi-agent interaction.","cats":{"new-dataset":0}}
{"text":"The resulting dataset consists of eight hours of measurement data.","cats":{"new-dataset":0}}
{"text":"It contains over 2,500 VRU trrollers, and wheelchair users, and over 20,000 vehicle trajectories at different day times, weather conditions, and seasons.","cats":{"new-dataset":0}}
{"text":"In addition, to enable the entire stack of research capabilities, the dataset includes all data, starting from the sensor-, calibration- and detection data until","cats":{"new-dataset":0}}
{"text":"Noisy label problems are inevitably in existence within medical image segmentation causing severe performance degradation.","cats":{"new-dataset":0}}
{"text":"Previous segmentation methods for noisy label problems only utilize a single image while the potential of leveraging the correlation between images has been overlooked.","cats":{"new-dataset":0,"data-quality":1}}
{"text":"Especially for video segmentation, adjacent frames contain rich contextual information beneficial in cognizing noisy labels.","cats":{"new-dataset":0}}
{"text":"Based on two insights, we propose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to resolve noisy-labeled medical video segmentation issues.","cats":{"new-dataset":0}}
{"text":"First, we argue the sequential prior of videos is an effective reference, i.e., pixel-level features from adjacent frames are close in distance for the same class and far in distance otherwise.","cats":{"new-dataset":0}}
{"text":"Therefore, Temporal Feature Affinity Learning (TFAL) is devised to indicate possible noisy labels by evaluating the affinity between pixels in two adjacent frames.","cats":{"new-dataset":0}}
{"text":"We also notice that the noise distribution exhibits considerable variations across video, image, and pixel levels.","cats":{"new-dataset":0}}
{"text":"In this way, we introduce Multi-Scale Supervision (MSS) to supervise the network from three different perspectives by re-weighting and refining the samples.","cats":{"new-dataset":0}}
{"text":"This design enables the network to concentrate on clean samples in a coarse-to-fine manner.","cats":{"new-dataset":0}}
{"text":"Experiments with both synthetic and real-world label noise demonstrate that our method outperforms recent state-of-the-art robust segmentation approaches.","cats":{"new-dataset":0,"data-quality":1}}
{"text":"Code is available at https://github.com/BeileiCui/MS-TFAL.","cats":{"new-dataset":0}}
{"text":"This paper proposes a data-efficient detection method for deep neural networks against backdoor attacks under a black-box scenario.","cats":{"new-dataset":0}}
{"text":"The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in determining the backdoored network output than any other benign features.","cats":{"new-dataset":0}}
{"text":"To quantitatively measure the effects of triggers and benign features on determining the backdoored network output, we introduce five metrics.","cats":{"new-dataset":0}}
{"text":"To calculate the five-metric values for a given input, we first generate several synthetic samples by injecting the input's partial contents into clean validation samples.","cats":{"new-dataset":0}}
{"text":"Then, the five metrics are computed by using the output labels of the corresponding synthetic samples.","cats":{"new-dataset":0}}
{"text":"One contribution of this work is the use of a tiny clean validation dataset.","cats":{"new-dataset":0}}
{"text":"Having the computed five metrics, five novelty detectors are trained from the validation dataset.","cats":{"new-dataset":0}}
{"text":"A meta novelty detector fuses the output of the five trained novelty detectors to generate a meta confidence score.","cats":{"new-dataset":0}}
{"text":"During online testing, our method determines if online samples are poisoned or not via assessing their meta confidence scores output by the meta novelty detector.","cats":{"new-dataset":0}}
{"text":"We show the efficacy of our methodology through a broad range of backdoor attacks, including ablation studies and comparison to existing approaches.","cats":{"new-dataset":0}}
{"text":"Our methodology is promising since the proposed five metrics quantify the inherent differences between clean and poisoned samples.","cats":{"new-dataset":0}}
{"text":"Additionally, our detection method can be incrementally improved by appending more metrics that may be proposed to address future advanced attacks.","cats":{"new-dataset":0}}
{"text":"To address this limitation, we have developed a synthetic dataset for short-term trajectory prediction tasks using the CARLA simulator.","cats":{"new-dataset":1}}
{"text":"This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicles overtaking - and comprises 6000 perspective view images with corresponding IMU and odometry information for each frame.","cats":{"new-dataset":1}}
{"text":"Our datasets are publicly available on https://github.com/navigatinguncertainty.","cats":{"new-dataset":0}}
{"text":"Autonomous vehicles require accurate and reliable short-term trajectory predictions for safe and efficient driving.","cats":{"new-dataset":0}}
{"text":"While most commercial automated vehicles currently use state machine-based algorithms for trajectory forecasting, recent efforts have focused on end-to-end data-driven systems.","cats":{"new-dataset":0}}
{"text":"Often, the design of these models is limited by the availability of datasets, which are typically restricted to generic scenarios.  ","cats":{"new-dataset":0}}
{"text":"This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicles overtaking -  (LSTM) networks has also been developed.","cats":{"new-dataset":0}}
{"text":"This model can handle corner cases, such as slowing down near zebra crossings and stopping when pedestrians cross the road, without the need for explicit encoding of the surrounding environment.","cats":{"new-dataset":0}}
{"text":"Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks.","cats":{"new-dataset":0}}
{"text":"However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation.","cats":{"new-dataset":0}}
{"text":"In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation.","cats":{"new-dataset":0}}
{"text":"Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation.","cats":{"new-dataset":0}}
{"text":"Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters.","cats":{"new-dataset":0}}
{"text":"Code and datasets are available on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.","cats":{"new-dataset":1}}
{"text":"Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders (VAEs) that aims at separating the common factors of variation between a background dataset (BG) (i.e., healthy subjects) and a target dataset (TG) (i.e., patients) from the ones that only exist in the target dataset.","cats":{"new-dataset":0}}
{"text":"To do so, these methods separate the latent space into a set of salient features (i.e., proper to the target dataset) and a set of common features (i.e., exist in both datasets).","cats":{"new-dataset":0}}
{"text":"Currently, all models fail to prevent the sharing of information between latent spaces effectively and to capture all salient factors of variation.","cats":{"new-dataset":0}}
{"text":"To this end, we introduce two crucial regularization losses: a disentangling term between common and salient representations and a classification term between background and target samples in the salient space.","cats":{"new-dataset":0}}
{"text":"We show a better performance than previous CA-VAEs methods on three medical applications and a natural images dataset (CelebA).","cats":{"new-dataset":0}}
{"text":"The code and dataset are open source at https://github.com/junjun-yan/ATL-PINN.","cats":{"new-dataset":1}}
{"text":"Physics-informed neural networks (PINNs) have emerged as promising surrogate modes for solving partial differential equations (PDEs).","cats":{"new-dataset":0}}
{"text":"Their effectiveness lies in the ability to capture solution-related features through neural networks.","cats":{"new-dataset":0}}
{"text":"However, original PINNs often suffer from bottlenecks, such as low accuracy and non-convergence, limiting their applicability in complex physical contexts.","cats":{"new-dataset":0}}
{"text":"To alleviate these issues, we proposed auxiliary-task learning-based physics-informed neural networks (ATL-PINNs), which provide four different auxiliary-task learning modes and investigate their performance compared with original PINNs.","cats":{"new-dataset":0}}
{"text":"We also employ the gradient cosine similarity algorithm to integrate auxiliary problem loss with the primary problem loss in ATL-PINNs, which aims to enhance the effectiveness of the auxiliary-task learning modes.","cats":{"new-dataset":0}}
{"text":"To the best of our knowledge, this is the first study to introduce auxiliary-task learning modes in the context of physics-informed learning.","cats":{"new-dataset":0}}
{"text":"We conduct experiments on three PDE problems across different fields and scenarios.","cats":{"new-dataset":0}}
{"text":"Our findings demonstrate that the proposed auxiliary-task learning modes can significantly improve solution accuracy, achieving a maximum performance boost of 96.62% (averaging 28.23%) compared to the original single-task PINNs.","cats":{"new-dataset":0}}
{"text":"To facilitate the investigation, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues","cats":{"new-dataset":1}}
{"text":"We will release our dataset and codes to facilitate future studies.","cats":{"new-dataset":0}}
{"text":"Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings.","cats":{"new-dataset":0}}
{"text":"One example is that humans can reason where and when an image is taken based on their knowledge.","cats":{"new-dataset":0}}
{"text":"This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even outperform human's capability in reasoning times and location.","cats":{"new-dataset":0}}
{"text":"To address this question, we propose a two-stage \\recognition\\space and \\reasoning\\space probing task, applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. .","cats":{"new-dataset":0}}
{"text":"In the extensive experimental studies, we find that although VLMs can effectively retain relevant features in visual encoders, they still fail to make perfect reasoning.","cats":{"new-dataset":0}}
{"text":"Our model was evaluated on two benchmark tree counting datasets, Jiangsu, and Yosemite, as well as a new dataset, KCL-London, created by ourselves","cats":{"new-dataset":1}}
{"text":"The codes and datasets are available at https://github.com/HAAClassic/TreeFormer.","cats":{"new-dataset":0}}
{"text":"Automatic tree density estimation and counting using single aerial and satellite images is a challenging task in photogrammetry and remote sensing, yet has an important role in forest management.","cats":{"new-dataset":0}}
{"text":"In this paper, we propose the first semisupervised transformer-based framework for tree counting which reduces the expensive tree annotations for remote sensing images.","cats":{"new-dataset":0}}
{"text":"Our method, termed as TreeFormer, first develops a pyramid tree representation module based on transformer blocks to extract multi-scale features during the encoding stage.","cats":{"new-dataset":0}}
{"text":"Contextual attention-based feature fusion and tree density regressor modules are further designed to utilize the robust features from the encoder to estimate tree density maps in the decoder.","cats":{"new-dataset":0}}
{"text":"Moreover, we propose a pyramid learning strategy that includes local tree density consistency and local tree count ranking losses to utilize unlabeled images into the training process.","cats":{"new-dataset":0}}
{"text":"Finally, the tree counter token is introduced to regulate the network by computing the global tree counts for both labeled and unlabeled images. .","cats":{"new-dataset":0}}
{"text":"Our TreeFormer outperforms the state of the art semi-supervised methods under the same setting and exceeds the fully-supervised methods using the same number of labeled images.","cats":{"new-dataset":0}}
{"text":"Despite recent advancements in speech emotion recognition (SER) models, state-of-the-art deep learning (DL) approaches face the challenge of the limited availability of annotated data.","cats":{"new-dataset":0}}
{"text":"Large language models (LLMs) have revolutionised our understanding of natural language, introducing emergent properties that broaden comprehension in language, speech, and vision.","cats":{"new-dataset":0}}
{"text":"This paper examines the potential of LLMs to annotate abundant speech data, aiming to enhance the state-of-the-art in SER.","cats":{"new-dataset":0}}
{"text":"We evaluate this capability across various settings using publicly available speech emotion classification datasets.","cats":{"new-dataset":0}}
{"text":"Leveraging ChatGPT, we experimentally demonstrate the promising role of LLMs in speech emotion data annotation.","cats":{"new-dataset":0}}
{"text":"Our evaluation encompasses single-shot and few-shots scenarios, revealing performance variability in SER.","cats":{"new-dataset":0}}
{"text":"Notably, we achieve improved results through data augmentation, incorporating ChatGPT-annotated samples into existing datasets.","cats":{"new-dataset":0}}
{"text":"Our work uncovers new frontiers in speech emotion classification, highlighting the increasing significance of LLMs in this field moving forward.","cats":{"new-dataset":0}}
{"text":"We have taken millions of images in the sorghum fields, manually selected 5,447 images that contain aphids, and annotated each aphid cluster in the image.","cats":{"new-dataset":1}}
{"text":"To use these images for machine learning models, we crop the images into patches and created a labeled dataset with over 151,000 image patches.","cats":{"new-dataset":0}}
{"text":"Aphids are one of the main threats to crops, rural families, and global food security.","cats":{"new-dataset":0}}
{"text":"Chemical pest control is a necessary component of crop production for maximizing yields, however, it is unnecessary to apply the chemical approaches to the entire fields in consideration of the environmental pollution and the cost.","cats":{"new-dataset":0}}
{"text":"Thus, accurately localizing the aphid and estimating the infestation level is crucial to the precise local application of pesticides.","cats":{"new-dataset":0}}
{"text":"Aphid detection is very challenging as each individual aphid is really small and all aphids are crowded together as clusters.","cats":{"new-dataset":0}}
{"text":"In this paper, we propose to estimate the infection level by detecting aphid clusters.  ","cats":{"new-dataset":0}}
{"text":"Then, we i","cats":{"new-dataset":0}}
{"text":"Given a set of calibrated images of a scene, we present an approach that produces a simple, compact, and actionable 3D world representation by means of 3D primitives.","cats":{"new-dataset":0}}
{"text":"While many approaches focus on recovering high-fidelity 3D scenes, we focus on parsing a scene into mid-level 3D representations made of a small set of textured primitives.","cats":{"new-dataset":0}}
{"text":"Such representations are interpretable, easy to manipulate and suited for physics-based simulations.","cats":{"new-dataset":0}}
{"text":"Moreover, unlike existing primitive decomposition methods that rely on 3D input data, our approach operates directly on images through differentiable rendering.","cats":{"new-dataset":0}}
{"text":"Specifically, we model primitives as textured superquadric meshes and optimize their parameters from scratch with an image rendering loss.","cats":{"new-dataset":0}}
{"text":"We highlight the importance of modeling transparency for each primitive, which is critical for optimization and also enables handling varying numbers of primitives.","cats":{"new-dataset":0}}
{"text":"We show that the resulting textured primitives faithfully reconstruct the input images and accurately model the visible 3D points, while providing amodal shape completions of unseen object regions.","cats":{"new-dataset":0}}
{"text":"We compare our approach to the state of the art on diverse scenes from DTU, and demonstrate its robustness on real-life captures from BlendedMVS and Nerfstudio.","cats":{"new-dataset":0}}
{"text":"We also showcase how our results can be used to effortlessly edit a scene or perform physical simulations.","cats":{"new-dataset":0}}
{"text":"Code and video results are available at https://www.tmonnier.com/DBW .","cats":{"new-dataset":0}}
{"text":"This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs","cats":{"new-dataset":1}}
{"text":"To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language","cats":{"new-dataset":1}}
{"text":"To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark the created dataset with a transformer-based model for ISL translation","cats":{"new-dataset":1}}
{"text":"Sign languages are the primary means of communication for many hard-of-hearing people worldwide.","cats":{"new-dataset":0}}
{"text":"Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems.","cats":{"new-dataset":0}}
{"text":"However, there is a dearth of sign language resources for the Indian sign language. .","cats":{"new-dataset":0}}
{"text":"To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language.","cats":{"new-dataset":0}}
{"text":"To validchmark the created dataset with a transformer-based model for ISL translation.","cats":{"new-dataset":0}}
{"text":"This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), a novel dataset targeting real-life scenario reasoning","cats":{"new-dataset":1}}
{"text":"The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve its quality.","cats":{"new-dataset":1}}
{"text":", aiming to close the gap in artificial neural networks' ability to reason in everyday contexts.","cats":{"new-dataset":0}}
{"text":"In contrast to domain knowledge reasoning datasets, LSR-Benchmark comprises free-text formatted questions with rich information on real-life scenarios, human behaviors, and character roles.","cats":{"new-dataset":0}}
{"text":"The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve its qualitto test the performance in LSR-Benchmark.","cats":{"new-dataset":0}}
{"text":"The results reveal that humans outperform these models significantly, indicating a persisting challenge for machine learning models in comprehending daily human life.","cats":{"new-dataset":0}}
{"text":"The smarty4covid dataset contains audio signals of cough (4,676), regular breathing (4,665), deep breathing (4,695) and voice (4,291) as recorded by means of mobile devices following a crowd-sourcing approach","cats":{"new-dataset":1}}
{"text":"Other self reported information is also included (e.g. COVID-19 virus tests), thus providing a comprehensive dataset for the development of COVID-19 risk detection models.","cats":{"new-dataset":0}}
{"text":"The smarty4covid dataset is released in the form of a web-ontology language (OWL) knowledge base enabling data consolidation from other relevant datasets, complex queries and reasoning","cats":{"new-dataset":1}}
{"text":"Harnessing the power of Artificial Intelligence (AI) and m-health towards detecting new bio-markers indicative of the onset and progress of respiratory abnormalities/conditions has greatly attracted the scientific and research interest especially during COVID-19 pandemic. .","cats":{"new-dataset":0}}
{"text":"The smarty4covid dataset is releasedtowards the development of models able to: (i) extract clinically informative respiratory indicators from regular breathing records, and (ii) identify cough, breath and voirisk detection models is proposed and validated.","cats":{"new-dataset":0}}
{"text":"This dataset collects nighttime images with different properties of nighttime environments, such as flare and extreme darkness","cats":{"new-dataset":1}}
{"text":"Nighttime surveillance suffers from degradation due to poor illumination and arduous human annotations.","cats":{"new-dataset":0}}
{"text":"It is challengable and remains a security risk at night.","cats":{"new-dataset":0}}
{"text":"Existing methods rely on multi-spectral images to perceive objects in the dark, which are troubled by low resolution and color absence.","cats":{"new-dataset":0}}
{"text":"We argue that the ultimate solution for nighttime surveillance is night-to-day translation, or Night2Day, which aims to translate a surveillance scene from nighttime to the daytime while maintaining semantic consistency.","cats":{"new-dataset":0}}
{"text":"To achieve this, this paper presents a Disentangled Contrastive (DiCo) learning method.","cats":{"new-dataset":0}}
{"text":"Specifically, to address the poor and complex illumination in the nighttime scenes, we propose a learnable physical prior, i.e., the color invariant, which provides a stable perception of a highly dynamic night environment and can be incorporated into the learning pipeline of neural networks.","cats":{"new-dataset":0}}
{"text":"Targeting the surveillance scenes, we develop a disentangled representation, which is an auxiliary pretext task that separates surveillance scenes into the foreground and background with contrastive learning.","cats":{"new-dataset":0}}
{"text":"Such a strategy can extract the semantics without supervision and boost our model to achieve instance-aware translation.","cats":{"new-dataset":0}}
{"text":"Finally, we incorporate all the modules above into generative adversarial networks and achieve high-fidelity translation.  ","cats":{"new-dataset":0}}
{"text":"It includes six scenes to support the study on nighttime surveillance.","cats":{"new-dataset":0}}
{"text":"This dataset collects nighttime images with different properties of nigg works significantly.","cats":{"new-dataset":0}}
{"text":"In this paper, we present TRansPose, the first large-scale multispectral dataset that combines stereo RGB-D, thermal infrared (TIR) images, and object poses to promote transparent object research","cats":{"new-dataset":1}}
{"text":"The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects","cats":{"new-dataset":1}}
{"text":"It comprises a vast collection of 333,819 images and 4,000,056 annotations, providing instance-level segmentation masks, ground-truth poses, and completed depth information","cats":{"new-dataset":1}}
{"text":"The data was acquired using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda robot manipulator","cats":{"new-dataset":1}}
{"text":"TRansPose dataset can be accessed from the following link: https://sites.google.com/view/transpose-dataset","cats":{"new-dataset":0}}
{"text":"Transparent objects are encountered frequently in our daily lives, yet recognizing them poses challenges for conventional vision sensors due to their unique material properties, not being well perceived from RGB or depth cameras.","cats":{"new-dataset":0}}
{"text":"Overcoming this limitation, thermal infrared cameras have emerged as a solution, offering improved visibility and shape information for transparent objects. .","cats":{"new-dataset":0}}
{"text":"The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects.","cats":{"new-dataset":0}}
{"text":"It comprises a vast colleced using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda robot manipulator.","cats":{"new-dataset":0}}
{"text":"Spanning 87 sequences, TRansPose cbjects in plastic bags, and multi-stacked objects.","cats":{"new-dataset":0}}
{"text":"this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs).","cats":{"new-dataset":1}}
{"text":"In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics","cats":{"new-dataset":1}}
{"text":"We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs.","cats":{"new-dataset":0}}
{"text":"In  This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes.","cats":{"new-dataset":0}}
{"text":"In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert comparisonhasizing its potential for practical safety measures in LLMs.","cats":{"new-dataset":0}}
{"text":"We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM).","cats":{"new-dataset":0}}
{"text":"The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes.","cats":{"new-dataset":0}}
{"text":"We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context.","cats":{"new-dataset":0}}
{"text":"Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses.","cats":{"new-dataset":0}}
{"text":"Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts.","cats":{"new-dataset":0}}
{"text":"The promising results demonstrate significant implications of the ICAE for its novel approach to the long context problem and its potential to reduce computation and memory overheads for LLM inference in practice, suggesting further research effort in context management for an LLM.","cats":{"new-dataset":0}}
{"text":"This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation.","cats":{"new-dataset":1}}
{"text":"The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words.","cats":{"new-dataset":1}}
{"text":" The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words.","cats":{"new-dataset":0}}
{"text":"Our core contribution is to developnguage representation at scale.","cats":{"new-dataset":0}}
{"text":"Specifically, we utilize a multi-scale approach to generate video-related descriptions.","cats":{"new-dataset":0}}
{"text":"Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance.","cats":{"new-dataset":0}}
{"text":"Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications.","cats":{"new-dataset":0}}
{"text":"They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research.","cats":{"new-dataset":0}}
{"text":"These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.","cats":{"new-dataset":0}}
{"text":"We make our data and code publicly available in https://github.com/AI21Labs/factor.","cats":{"new-dataset":1}}
{"text":"Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain.","cats":{"new-dataset":0}}
{"text":"Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts.","cats":{"new-dataset":0}}
{"text":"We propose FACTOR:","cats":{"new-dataset":0}}
{"text":"Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality.","cats":{"new-dataset":0}}
{"text":"FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements.","cats":{"new-dataset":0}}
{"text":"We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR.","cats":{"new-dataset":0}}
{"text":"We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.","cats":{"new-dataset":0}}
{"text":"LCC provided an anonymised dataset comprising 14360 records of young people under the age of 18.","cats":{"new-dataset":1}}
{"text":"Local authorities in England, such as Leicestershire County Council (LCC), provide Early Help services that can be offered at any point in a young person's life when they experience difficulties that cannot be supported by universal services alone, such as schools.","cats":{"new-dataset":0}}
{"text":"This paper investigates the utilisation of machine learning (ML) to assist experts in identifying families that may need to be referred for Early Help assessment and support.  ","cats":{"new-dataset":0}}
{"text":"The dataset was pre-processed, machine learning models were build, and experiments were conducted to validate and test the performance of the models.","cats":{"new-dataset":0}}
{"text":"Bias mitigation techniques were applied to improve the fairness of these models.","cats":{"new-dataset":0}}
{"text":"During testing, while the models demonstrated the capability to identify young people requiring intervention or early help, they also produced a significant number of false positives, especially when constructed with imbalanced data, incorrectly identifying individuals who most likely did not need an Early Help referral.","cats":{"new-dataset":0}}
{"text":"This paper empirically explores the suitability of data-driven ML models for identifying young people who may require Early Help services and discusses their appropriateness and limitations for this task.","cats":{"new-dataset":0}}
{"text":"Experiments on a new dataset of real images show that adding RePoGen data to the COCO surpasses previous attempts to top-view pose estimation and significantly improves performance on the bottom-view dataset","cats":{"new-dataset":1}}
{"text":"The code and the datasets are available on the project website.","cats":{"new-dataset":0}}
{"text":"Human Pose Estimation is a thoroughly researched problem; however, most datasets focus on the side and front-view scenarios.","cats":{"new-dataset":0}}
{"text":"We address the limitation by proposing a novel approach that tackles the challenges posed by extreme viewpoints and poses.","cats":{"new-dataset":0}}
{"text":"We introduce a new method for synthetic data generation - RePoGen, RarE POses GENerator - with comprehensive control over pose and view to augment the COCO dataset. .","cats":{"new-dataset":0}}
{"text":"Through an extensive ablation study on both the top and bottom view data, we elucidate the contributions of methodological choices and demonstrate improved performance.","cats":{"new-dataset":0}}
{"text":"We propose IntelliGraphs, a set of five new Knowledge Graph datasets.","cats":{"new-dataset":1}}
{"text":"We also present the dataset generator that produced the synthetic datasets.","cats":{"new-dataset":1}}
{"text":"Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations.","cats":{"new-dataset":0}}
{"text":"A key task in the literature is predicting missing links between entities.","cats":{"new-dataset":0}}
{"text":"However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure.","cats":{"new-dataset":0}}
{"text":"Semantics is crucial in several downstream tasks, such as query answering or reasoning.","cats":{"new-dataset":0}}
{"text":"We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs.  ","cats":{"new-dataset":0}}
{"text":"The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference.","cats":{"new-dataset":0}}
{"text":"We also present the dataset generator that produced the synthetic datased on traditional KGEs.","cats":{"new-dataset":0}}
{"text":"We evaluate their expressiveness and show that these models cannot capture the semantics.","cats":{"new-dataset":0}}
{"text":"We believe this benchmark will encourage the development of machine learning models that emphasize semantic understanding.","cats":{"new-dataset":0}}
{"text":"In the realm of Tiny AI, we introduce \"You Only Look at Interested Cells\" (YOLIC), an efficient method for object localization and classification on edge devices.","cats":{"new-dataset":0}}
{"text":"Seamlessly blending the strengths of semantic segmentation and object detection, YOLIC offers superior computational efficiency and precision.","cats":{"new-dataset":0}}
{"text":"By adopting Cells of Interest for classification instead of individual pixels, YOLIC encapsulates relevant information, reduces computational load, and enables rough object shape inference.","cats":{"new-dataset":0}}
{"text":"Importantly, the need for bounding box regression is obviated, as YOLIC capitalizes on the predetermined cell configuration that provides information about potential object location, size, and shape.","cats":{"new-dataset":0}}
{"text":"To tackle the issue of single-label classification limitations, a multi-label classification approach is applied to each cell, effectively recognizing overlapping or closely situated objects.","cats":{"new-dataset":0}}
{"text":"This paper presents extensive experiments on multiple datasets, demonstrating that YOLIC achieves detection performance comparable to the state-of-the-art YOLO algorithms while surpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU.","cats":{"new-dataset":0}}
{"text":"All resources related to this study, including datasets, cell designer, image annotation tool, and source code, have been made publicly available on our project website at https://kai3316.github.io/yolic.github.io","cats":{"new-dataset":0}}
{"text":"Most of the existing LiDAR-inertial navigation systems are based on frame-to-map registrations, leading to inconsistency in state estimation.","cats":{"new-dataset":0}}
{"text":"The newest solid-state LiDAR with a non-repetitive scanning pattern makes it possible to achieve a consistent LiDAR-inertial estimator by employing a frame-to-frame data association.","cats":{"new-dataset":0}}
{"text":"In this letter, we propose a robust and consistent frame-to-frame LiDAR-inertial navigation system (FF-LINS) for solid-state LiDARs.","cats":{"new-dataset":0}}
{"text":"With the INS-centric LiDAR frame processing, the keyframe point-cloud map is built using the accumulated point clouds to construct the frame-to-frame data association.","cats":{"new-dataset":0}}
{"text":"The LiDAR frame-to-frame and the inertial measurement unit (IMU) preintegration measurements are tightly integrated using the factor graph optimization, with online calibration of the LiDAR-IMU extrinsic and time-delay parameters.","cats":{"new-dataset":0}}
{"text":"The experiments on the public and private datasets demonstrate that the proposed FF-LINS achieves superior accuracy and robustness than the state-of-the-art systems.","cats":{"new-dataset":0}}
{"text":"Besides, the LiDAR-IMU extrinsic and time-delay parameters are estimated effectively, and the online calibration notably improves the pose accuracy.","cats":{"new-dataset":0}}
{"text":"The proposed FF-LINS and the employed datasets are open-sourced on GitHub (https://github.com/i2Nav-WHU/FF-LINS).","cats":{"new-dataset":0}}
{"text":"Considering these limitations, we introduce the first video-based retinal dataset by employing handheld devices for data acquisition.","cats":{"new-dataset":1}}
{"text":"The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients from 50 to 75 years old.","cats":{"new-dataset":1}}
{"text":"Specifically, the dataset provides three levels of spatial annotations","cats":{"new-dataset":1}}
{"text":"In addition, the dataset offers temporal annotations","cats":{"new-dataset":1}}
{"text":"We hope this challenging dataset would significantly contribute to the development of eye disease diagnosis and early prevention.","cats":{"new-dataset":0}}
{"text":"Retinal vessel segmentation is generally grounded in image-based datasets collected with bench-top devices.","cats":{"new-dataset":0}}
{"text":"The static images naturally lose the dynamic characteristics of retina fluctuation, resulting in diminished dataset richness, and the usage of bench-top devices further restricts dataset scalability due to its limited accessibility.  ","cats":{"new-dataset":0}}
{"text":"The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients from 50 to 75 he landscape of vasculature segmentation.","cats":{"new-dataset":0}}
{"text":"Specifically, the dataset provides three levels of spatial annotations: binary vessel masks for overall retinal structure delineation, general vein-artery masks for distinguishing the vein and artery, and fine-grained vein-artery masks for further characterizing the granulari annotations that capture the vessel pulsation characteristics, assisting in detecting ocular diseases that require fine-grained recognition of hemodynamic fluctuation.","cats":{"new-dataset":0}}
{"text":"In application, our dataset exhibits a significant domain shift with respect to data captured by bench-top devices, thus posing great chaprovide evaluation metrics and benchmark results on our dataset, reflecting both the potential and challenges it offers for vessel segmentation tasks.","cats":{"new-dataset":0}}
{"text":"This study addressed the complex task of sentiment analysis on a dataset of 119,988 original tweets from Weibo using a Convolutional Neural Network (CNN), offering a new approach to Natural Language Processing (NLP).","cats":{"new-dataset":0}}
{"text":"The data, sourced from Baidu's PaddlePaddle AI platform, were meticulously preprocessed, tokenized, and categorized based on sentiment labels.","cats":{"new-dataset":0}}
{"text":"A CNN-based model was utilized, leveraging word embeddings for feature extraction, and trained to perform sentiment classification.","cats":{"new-dataset":0}}
{"text":"The model achieved a macro-average F1-score of approximately 0.73 on the test set, showing balanced performance across positive, neutral, and negative sentiments.","cats":{"new-dataset":0}}
{"text":"The findings underscore the effectiveness of CNNs for sentiment analysis tasks, with implications for practical applications in social media analysis, market research, and policy studies.","cats":{"new-dataset":0}}
{"text":"The complete experimental content and code have been made publicly available on the Kaggle data platform for further research and development.","cats":{"new-dataset":0}}
{"text":"Future work may involve exploring different architectures, such as Recurrent Neural Networks (RNN) or transformers, or using more complex pre-trained models like BERT, to further improve the model's ability to understand linguistic nuances and context.","cats":{"new-dataset":0}}
{"text":"To address the scarcity of annotated corpora with realistic multi-issue negotiation dialogues, we use GPT-3 to build GPT-Negochat, a synthesized dataset that we make publicly available.","cats":{"new-dataset":1}}
{"text":"Automated negotiation support systems aim to help human negotiators reach more favorable outcomes in multi-issue negotiations (e.g., an employer and a candidate negotiating over issues such as salary, hours, and promotions before a job offer).","cats":{"new-dataset":0}}
{"text":"To be successful, these systems must accurately track agreements reached by participants in real-time.","cats":{"new-dataset":0}}
{"text":"Existing approaches either focus on task-oriented dialogues or produce unstructured outputs, rendering them unsuitable for this objective.","cats":{"new-dataset":0}}
{"text":"Our work introduces the novel task of agreement tracking for two-party multi-issue negotiations, which requires continuous monitoring of agreements within a structured state space.  ","cats":{"new-dataset":0}}
{"text":"We present a strong initial baseline for our task by transfer-learning a T5 model trained on the MultiWOZ 2.4 corpus.","cats":{"new-dataset":0}}
{"text":"Pre-training T5-small and T5-base on MultiWOZ 2.4's DST task enhances results by 21% and 9% respectively over training solely on GPT-Negochat.","cats":{"new-dataset":0}}
{"text":"We validate our method's sample-efficiency via smaller training subset experiments.","cats":{"new-dataset":0}}
{"text":"By releasing GPT-Negochat and our baseline models, we aim to encourage further research in multi-issue negotiation dialogue agreement tracking.","cats":{"new-dataset":0}}
{"text":"This work presents WaterScenes, the first multi-task 4D radar-camera fusion dataset for autonomous driving on water surfaces","cats":{"new-dataset":1}}
{"text":"Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at pixel-level and point-level, respectively","cats":{"new-dataset":1}}
{"text":"WaterScenes dataset is public on https://waterscenes.github.io.","cats":{"new-dataset":0}}
{"text":"Autonomous driving on water surfaces plays an essential role in executing hazardous and time-consuming missions, such as maritime surveillance, survivors rescue, environmental monitoring, hydrography mapping and waste cleaning. .","cats":{"new-dataset":0}}
{"text":"Equipped with a 4D radar and a monocular camera, our Unmanned Surface Vehicle (USV) proffers all-weather solutions for discerning object-related information, including color, shape, texture, range, velocity, azimuth, and elevation.","cats":{"new-dataset":0}}
{"text":"Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at pixelprovide annotations for free-space segmentation and waterline segmentation.","cats":{"new-dataset":0}}
{"text":"Leveraging the multi-task and multi-modal data, we conduct numerous experiments on the single modality of radar and camera, as well as the fused modalities.","cats":{"new-dataset":0}}
{"text":"Results demonstrate that 4D radar-camera fusion can considerably enhance the robustness of perception on water surfaces, especially in adverse lighting and weather conditions.","cats":{"new-dataset":0}}
{"text":"This paper presents the introduction of a framework called \\textit{Ashaar} https://github.com/ARBML/Ashaar, which encompasses a collection of datasets and pre-trained models designed specifically for the analysis and generation of Arabic poetry.","cats":{"new-dataset":1}}
{"text":"Furthermore, as part of this endeavor, we provide four datasets: one for poetry generation, another for diacritization, and two for Arudi-style prediction.","cats":{"new-dataset":0}}
{"text":"These datasets aim to facilitate research and development in the field of Arabic poetry by enabling researchers and enthusiasts to delve into the nuances of this rich literary tradition.","cats":{"new-dataset":1}}
{"text":"Poetry holds immense significance within the cultural and traditional fabric of any nation.","cats":{"new-dataset":0}}
{"text":"It serves as a vehicle for poets to articulate their emotions, preserve customs, and convey the essence of their culture.","cats":{"new-dataset":0}}
{"text":"Arabic poetry is no exception, having played a cherished role in the heritage of the Arabic community throughout history and maintaining its relevance in the present era.","cats":{"new-dataset":0}}
{"text":"Typically, comprehending Arabic poetry necessitates the expertise of a linguist who can analyze its content and assess its quality.  ","cats":{"new-dataset":0}}
{"text":"The pipeline established within our proposed approach encompasses various aspects of poetry, such as meter, theme, and era classification.","cats":{"new-dataset":0}}
{"text":"It also incorporates automatic poetry diacritization, enabling more intricate analyses like automated extraction of the \\textit{Arudi} style.","cats":{"new-dataset":0}}
{"text":"Additionally, we explore the feasibility of generating conditional poetry through the pre-training of a character-based GPT model.","cats":{"new-dataset":0}}
{"text":"These datasets aim to facilitate research and development in the field of Arabic poetry b","cats":{"new-dataset":0}}
{"text":"We build LogBench, the first logging statement generation dataset.","cats":{"new-dataset":1}}
{"text":"Automated logging statement generation techniques facilitate developers in writing appropriate logging statements that document software behaviors.","cats":{"new-dataset":0,"dev-research":1}}
{"text":"Current retrieval-based and learning-based logging methods fail to provide accurate logging statements in complex software.","cats":{"new-dataset":0}}
{"text":"Although existing large language models (LLMs) might be a good fit for the task due to their great success in natural language generation and programming language comprehension, their effectiveness and generalization capabilities have not been explored.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"To this end, this paper performs the first extensive study on applying LLMs for logging statement generation.  ","cats":{"new-dataset":0}}
{"text":"On LogBench, we evaluate the effectiveness and generalization capabilities of eight state-of-the-art LLMs, which include general-purpose and code-specific models ranging from 60M to 175B in size.","cats":{"new-dataset":0}}
{"text":"Specifically, we evaluate LLM's logging effectiveness by studying 1) their ability to decide logging ingredients, 2) the impact of the internal characteristics of LLMs, and 3) the influence of external factors.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"We further evaluate LLM's logging generalization capabilities using unseen data derived from code transformation techniques.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"Our study demonstrates that existing LLMs fall short of practical requirements for generating proper logging statement texts.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"We also disclose the impact of internal characteristics and external factors for LLMs in automated logging.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"In addition, we observe that existing LLMs cannot generalize to logging unseen code, revealing their unsatisfactory generalization capabilities.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"Based on our findings, we further discuss three implications that can enhance logging statement generation in the future, such as developing a unified metric for logging quality, incorporating shareable code knowledge into LLMs, and devising suitable prompts.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"To address this gap, we introduce the Brazilian Leading Universities Entrance eXams (BLUEX), a dataset of entrance exams from the two leading universities in Brazil: UNICAMP and USP","cats":{"new-dataset":1}}
{"text":"The dataset includes annotated metadata for evaluating the performance of NLP models on a variety of subjects","cats":{"new-dataset":1}}
{"text":"The dataset is also annotated to indicate the position of images in each question, providing a valuable resource for advancing the state-of-the-art in multimodal language understanding and reasoning.","cats":{"new-dataset":1}}
{"text":"The data and relevant code can be found at https://github.com/Portuguese-Benchmark-Datasets/BLUEX","cats":{"new-dataset":1}}
{"text":"One common trend in recent studies of language models (LMs) is the use of standardized tests for evaluation.","cats":{"new-dataset":0}}
{"text":"However, despite being the fifth most spoken language worldwide, few such evaluations have been conducted in Portuguese.","cats":{"new-dataset":0}}
{"text":"This is mainly due to the lack of high-quality datasets available to the community for carrying out evaluations in Portuguese. .","cats":{"new-dataset":0}}
{"text":"The dataset includes annotated metadata for evaluating the performance of NLP models on a variety of subjects.","cats":{"new-dataset":0}}
{"text":"Furthermore, BLUEX includes a collection of recently administered examnnotated to indicate the position of images in each question, providing a valuable resource for advancing the state-of-the-art in multimodal language understanding and reasoning.","cats":{"new-dataset":0}}
{"text":"We describe the creation and characteristics of BLUEX and establish a benchmark through exund at https://github.com/Portuguese-Benchmark-Datasets/BLUEX","cats":{"new-dataset":0}}
{"text":"We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014).","cats":{"new-dataset":1}}
{"text":"We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic.  ","cats":{"new-dataset":0}}
{"text":"To this end, we modify the original texts using a set of phrases - modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009).","cats":{"new-dataset":0}}
{"text":"We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis.","cats":{"new-dataset":0}}
{"text":"Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules.","cats":{"new-dataset":0}}
{"text":"We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios.","cats":{"new-dataset":0}}
{"text":"We found that the performance of NLI models under the zero-shot setting is poor, especially for modified sentences with negation and existential quantifiers.","cats":{"new-dataset":0}}
{"text":"After fine-tuning this dataset, we observe that models continue to perform poorly over negation, existential and universal modifiers.","cats":{"new-dataset":0}}
{"text":"Age and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose complexities, and varying image quality, there are cases where the face is partially or completely occluded.","cats":{"new-dataset":0}}
{"text":"We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision transformer.","cats":{"new-dataset":0}}
{"text":"Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also person image data.","cats":{"new-dataset":0}}
{"text":"This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face is not visible in the image.","cats":{"new-dataset":0}}
{"text":"To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performance, while demonstrating real-time processing capabilities.","cats":{"new-dataset":0}}
{"text":"The ground truth annotations for this benchmark have been meticulously generated by human annotators, resulting in high accuracy answers due to the smart aggregation of votes.","cats":{"new-dataset":0}}
{"text":"Furthermore, we compare our model's age recognition performance with human-level accuracy and demonstrate that it significantly outperforms humans across a majority of age ranges.","cats":{"new-dataset":0}}
{"text":"Finally, we grant public access to our models, along with the code for validation and inference.","cats":{"new-dataset":0}}
{"text":"To bridge this gap, we introduce a dataset, SAGC-A68, which comprises access graphs automatically generated from 68 digital 3D models of space layouts of apartment buildings","cats":{"new-dataset":1}}
{"text":"This graph-based dataset is well-suited for developing GDL models for space function and space element classification","cats":{"new-dataset":1}}
{"text":"The dataset and code used in the experiment are available online.","cats":{"new-dataset":0}}
{"text":"The analysis of building models for usable area, building safety, and energy use requires accurate classification data of spaces and space elements.","cats":{"new-dataset":0}}
{"text":"To reduce input model preparation effort and errors, automated classification of spaces and space elements is desirable.","cats":{"new-dataset":0}}
{"text":"A barrier hindering the utilization of Graph Deep Learning (GDL) methods to space function and space element classification is a lack of suitable datasets. .","cats":{"new-dataset":0}}
{"text":"This graph-based dataset is well-suited for developing GDL models for space function and space element classification.","cats":{"new-dataset":0}}
{"text":"To demonstrate the potential of the dataset, we employ.","cats":{"new-dataset":0}}
{"text":"https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68.","cats":{"new-dataset":0}}
{"text":"We position the contribution of this work in two folds: (i)-we collect and curate nearly 2k high-quality tabular datasets, each of which is guaranteed to possess clear semantics, clean labels, and other necessary meta information.","cats":{"new-dataset":1}}
{"text":"Tabular data -- also known as structured data -- is one of the most common data forms in existence, thanks to the stable development and scaled deployment of database systems in the last few decades.","cats":{"new-dataset":0}}
{"text":"At present however, despite the blast brought by large pre-trained models in other domains such as ChatGPT or SAM, how can we extract common knowledge across tables at a scale that may eventually lead to generalizable representation for tabular data remains a full blank.","cats":{"new-dataset":0}}
{"text":"Indeed, there have been a few works around this topic.","cats":{"new-dataset":0}}
{"text":"Most (if not all) of them are limited in the scope of a single table or fixed form of a schema.","cats":{"new-dataset":0}}
{"text":"In this work, we first identify the crucial research challenges behind tabular data pre-training, particularly towards the cross-table scenario.  ","cats":{"new-dataset":0}}
{"text":"(ii)-we propose a novel framework that allows cross-table pre-training dubbed as CT-BERT.","cats":{"new-dataset":0}}
{"text":"Noticeably, in light of pioneering the scaled cross-table training, CT-BERT is fully compatible with both supervised and self-supervised schemes, where the specific instantiation of CT-BERT is very much dependent on the downstream tasks.","cats":{"new-dataset":0}}
{"text":"We further propose and implement a contrastive-learning-based and masked table modeling (MTM) objective into CT-BERT, that is inspired from computer vision and natural language processing communities but sophistically tailored to tables.","cats":{"new-dataset":0}}
{"text":"The extensive empirical results on 15 datasets demonstrate CT-BERT's state-of-the-art performance, where both its supervised and self-supervised setups significantly outperform the prior approaches.","cats":{"new-dataset":0}}
{"text":"The dataset is publicly available at: https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.","cats":{"new-dataset":1}}
{"text":"Despite the extensive applications of relation extraction (RE) tasks in various domains, little has been explored in the historical context, which contains promising data across hundreds and thousands of years.","cats":{"new-dataset":0}}
{"text":"To promote the historical RE research, we present HistRED constructed from Yeonhaengnok.","cats":{"new-dataset":0}}
{"text":"Yeonhaengnok is a collection of records originally written in Hanja, the classical Chinese writing, which has later been translated into Korean.","cats":{"new-dataset":0}}
{"text":"HistRED provides bilingual annotations such that RE can be performed on Korean and Hanja texts.","cats":{"new-dataset":0}}
{"text":"In addition, HistRED supports various self-contained subtexts with different lengths, from a sentence level to a document level, supporting diverse context settings for researchers to evaluate the robustness of their RE models.","cats":{"new-dataset":0}}
{"text":"To demonstrate the usefulness of our dataset, we propose a bilingual RE model that leverages both Korean and Hanja contexts to predict relations between entities.","cats":{"new-dataset":0}}
{"text":"Our model outperforms monolingual baselines on HistRED, showing that employing multiple language contexts supplements the RE predictions.","cats":{"new-dataset":0}}
{"text":"This research paper introduces a new curated dataset and a deep learning-based approach to solve these problems using convolutional neural networks (CNNs) and comprehensive data processing techniques.","cats":{"new-dataset":1}}
{"text":"Our dataset includes curated images and diverse channel bands from Sentinel, Landsat, VIIRS, and MODIS satellites.","cats":{"new-dataset":0}}
{"text":"We design the dataset considering different spatial and temporal resolution requirements","cats":{"new-dataset":1}}
{"text":"Our code, models and dataset are open source: https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation","cats":{"new-dataset":1}}
{"text":"Deforestation estimation and fire detection in the Amazon forest poses a significant challenge due to the vast size of the area and the limited accessibility.","cats":{"new-dataset":0}}
{"text":"However, these are crucial problems that lead to severe environmental consequences, including climate change, global warming, and biodiversity loss.","cats":{"new-dataset":0}}
{"text":"To effectively address this problem, multimodal satellite imagery and remote sensing offer a promising solution for estimating deforestation and detecting wildfire in the Amazonia region.  ","cats":{"new-dataset":0}}
{"text":"We design the dataset considering different spatial and temporal resolution requiremeimages from the region.","cats":{"new-dataset":0}}
{"text":"Our code, models and dataset are open source: https://github.com/h2oai/cvpr-multiearth-defo","cats":{"new-dataset":0}}
{"text":"We address this limitation by constructing MultiVENT, a dataset of multilingual, event-centric videos grounded in text documents across five target languages","cats":{"new-dataset":1}}
{"text":"Everyday news coverage has shifted from traditional broadcasts towards a wide range of presentation formats such as first-hand, unedited video footage.","cats":{"new-dataset":0}}
{"text":"Datasets that reflect the diverse array of multimodal, multilingual news sources available online could be used to teach models to benefit from this shift, but existing news video datasets focus on traditional news broadcasts produced for English-speaking audiences. .","cats":{"new-dataset":0}}
{"text":"MultiVENT includes both news broadcast videos and non-professional event footage, which we use to analyze the state of online news videos and how they can be leveraged to build robust, factually accurate models.","cats":{"new-dataset":0}}
{"text":"Finally, we provide a model for complex, multilingual video retrieval to serve as a baseline for information retrieval using MultiVENT.","cats":{"new-dataset":0}}
{"text":"To address this, we establish a large-scale dataset, namely the Tuberculosis X-ray (TBX11K) dataset, which contains 11,200 chest X-ray (CXR) images with corresponding bounding box annotations for TB areas","cats":{"new-dataset":1}}
{"text":"This dataset enables the training of sophisticated detectors for high-quality CTD","cats":{"new-dataset":1}}
{"text":"The data, code, and models will be released.","cats":{"new-dataset":0}}
{"text":"Tuberculosis (TB) is a major global health threat, causing millions of deaths annually.","cats":{"new-dataset":0}}
{"text":"Although early diagnosis and treatment can greatly improve the chances of survival, it remains a major challenge, especially in developing countries.","cats":{"new-dataset":0}}
{"text":"Recently, computer-aided tuberculosis diagnosis (CTD) using deep learning has shown promise, but progress is hindered by limited training data. .","cats":{"new-dataset":0}}
{"text":"This dataset enables the training of sophisticated detectors for high-quality CTD.","cats":{"new-dataset":0}}
{"text":"Furthermore, we propose a strong baseline, SymFormer, for simultaneous CXR image classification and TB infection area dete the bilateral symmetry property of CXR images for learning discriminative features.","cats":{"new-dataset":0}}
{"text":"Since CXR images may not strictly adhere to the bilateral symmetry property, we also propose Symmetric Positional Encoding (SPE) to facilitate SymAttention through feature recalibration.","cats":{"new-dataset":0}}
{"text":"To promote future research on CTD, we build a benchmark by introducing evaluation metrics, evaluating baseline models reformed from existing detectors, and running an online challenge.","cats":{"new-dataset":0}}
{"text":"Experiments show that SymFormer achieves state-of-the-art performance on the TBX11K dataset.","cats":{"new-dataset":0}}
{"text":"This paper presents an end-to-end methodology for collecting datasets to recognize handwritten English alphabets by utilizing Inertial Measurement Units (IMUs) and leveraging the diversity present in the Indian writing style","cats":{"new-dataset":1}}
{"text":"The IMUs are utilized to capture the dynamic movement patterns associated with handwriting, enabling more accurate recognition of alphabets.","cats":{"new-dataset":0}}
{"text":"The Indian context introduces various challenges due to the heterogeneity in writing styles across different regions and languages.","cats":{"new-dataset":0}}
{"text":"Some preliminary experimental results demonstrate the effectiveness of the dataset in accurately recogpattern recognition and offers valuable insights for developing improved systems for handwriting recognition, particularly in diverse linguistic and cultural contexts.","cats":{"new-dataset":0}}
{"text":"We fill this gap by semi-automatically creating an NLI dataset for spatial reasoning, called SpaceNLI","cats":{"new-dataset":1}}
{"text":"While many natural language inference (NLI) datasets target certain semantic phenomena, e.g., negation, tense & aspect, monotonicity, and presupposition, to the best of our knowledge, there is no NLI dataset that involves diverse types of spatial expressions and reasoning. .","cats":{"new-dataset":0}}
{"text":"We test several SOTA NLI systems on SpaceNLI to gauge the complexity of the dataset and the system's capacity for spatial reasoning.","cats":{"new-dataset":0}}
{"text":"Moreover, we introduce a Pattern Accuracy and argue that it is a more reliable and stricter measure than the accuracy for evaluating a system's performance on pattern-based generated data samples.","cats":{"new-dataset":0}}
{"text":"Based on the evaluation results we find that the systems obtain moderate results on the spatial NLI problems but lack consistency per inference pattern.","cats":{"new-dataset":0}}
{"text":"The results also reveal that non-projective spatial inferences (especially due to the \"between\" preposition) are the most challenging ones.","cats":{"new-dataset":0}}
{"text":"This paper presents the FormAI dataset, a large collection of 112,000 AI-generated compilable and independent C programs with vulnerability classification","cats":{"new-dataset":1}}
{"text":"We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabilities detected in each individual program","cats":{"new-dataset":1}}
{"text":"We introduce a dynamic zero-shot prompting technique, constructed to spawn a diverse set of programs utilizing Large Language Models (LLMs).","cats":{"new-dataset":0}}
{"text":"Some programs handle complicated tasks such as networkion.","cats":{"new-dataset":0}}
{"text":"This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which performs model checking, abstract interpretation, constraint programming, and satisfiability modulo theories, to reason over safety/security properties in programs.","cats":{"new-dataset":0}}
{"text":"This approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports.","cats":{"new-dataset":0}}
{"text":"Furthermore, we have associated the identified vulnerabilities with relevant Common Weakness Enumeration (CWE) numbers.","cats":{"new-dataset":0}}
{"text":"We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabilities detected in each individual program including location and function name, which makes the dataset ideal to train LLMs and machi","cats":{"new-dataset":0}}
{"text":"In this work, we construct two datasets to address this issue","cats":{"new-dataset":1}}
{"text":"We introduce a new conversation head generation benchmark for synthesizing behaviors of a single interlocutor in a face-to-face conversation.","cats":{"new-dataset":0}}
{"text":"The capability to automatically synthesize interlocutors which can participate in long and multi-turn conversations is vital and offer benefits for various applications, including digital humans, virtual agents, and social robots.","cats":{"new-dataset":0}}
{"text":"While existing research primarily focuses on talking head generation (one-way interaction), hindering the ability to create a digital human for conversation (two-way) interaction due to the absence of listening and interaction parts.","cats":{"new-dataset":0}}
{"text":", ``ViCo'' for independent talking and listening head generation tasks at the sentence level, and ``ViCo-X'', for synthesizing interlocutors in multi-turn conversational scenarios.","cats":{"new-dataset":0}}
{"text":"Based on ViCo and ViCo-X, we define three novel tasks targeting the interaction modeling during the face-to-face conversation: 1) responsive listening head generation making listeners respond actively to the speaker with non-verbal signals, 2) expressive talking head generation guiding speakers to be aware of listeners' behaviors, and 3) conversational head generation to integrate the talking/listening ability in one interlocutor.","cats":{"new-dataset":0}}
{"text":"Along with the datasets, we also propose corresponding baselierate responsive and vivid agents that can collaborate with real person to fulfil the whole conversation.","cats":{"new-dataset":0}}
{"text":"Project page: https://vico.solutions/.","cats":{"new-dataset":0}}
{"text":"The digitization of documents allows for wider accessibility and reproducibility.","cats":{"new-dataset":0}}
{"text":"While automatic digitization of document layout and text content has been a long-standing focus of research, this problem in regard to graphical elements, such as statistical plots, has been under-explored.","cats":{"new-dataset":0}}
{"text":"In this paper, we introduce the task of fine-grained visual understanding of mathematical graphics and present the Line Graphics (LG) dataset, which includes pixel-wise annotations of 5 coarse and 10 fine-grained categories.  ","cats":{"new-dataset":0}}
{"text":"To benchmark our LG dataset, we explore 7 state-of-the-art models.","cats":{"new-dataset":0}}
{"text":"To foster further research on the digitization of statistical graphs, we will make the dataset, code, and model","cats":{"new-dataset":0}}
{"text":"While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets.","cats":{"new-dataset":0}}
{"text":"The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits.","cats":{"new-dataset":0}}
{"text":"We help address these challenges through three contributions.  ","cats":{"new-dataset":0}}
{"text":"Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients.","cats":{"new-dataset":0}}
{"text":"Second, we publish the weights of a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients.","cats":{"new-dataset":0}}
{"text":"We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR.","cats":{"new-dataset":0}}
{"text":"We provide an end-to-end pipeline for the community to validate and build upon its performance.","cats":{"new-dataset":0}}
{"text":"Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaption.","cats":{"new-dataset":0}}
{"text":"The code to reproduce our results, as well as the model and dataset (via a research data use agreement), are available at our Github repo here: https://github.com/som","cats":{"new-dataset":0}}
{"text":"Generative latent diffusion models have been established as state-of-the-art in data generation.","cats":{"new-dataset":0}}
{"text":"One promising application is generation of realistic synthetic medical imaging data for open data sharing without compromising patient privacy.","cats":{"new-dataset":0}}
{"text":"Despite the promise, the capacity of such models to memorize sensitive patient training data and synthesize samples showing high resemblance to training data samples is relatively unexplored.","cats":{"new-dataset":0}}
{"text":"Here, we assess the memorization capacity of 3D latent diffusion models on photon-counting coronary computed tomography angiography and knee magnetic resonance imaging datasets.","cats":{"new-dataset":0}}
{"text":"To detect potential memorization of training samples, we utilize self-supervised models based on contrastive learning.","cats":{"new-dataset":0}}
{"text":"Our results suggest that such latent diffusion models indeed memorize training data, and there is a dire need for devising strategies to mitigate memorization.","cats":{"new-dataset":0}}
{"text":"Visual Question Answering (VQA) models aim to answer natural language questions about given images.","cats":{"new-dataset":0}}
{"text":"Due to its ability to ask questions that differ from those used when training the model, medical VQA has received substantial attention in recent years.","cats":{"new-dataset":0}}
{"text":"However, existing medical VQA models typically focus on answering questions that refer to an entire image rather than where the relevant content may be located in the image.","cats":{"new-dataset":0}}
{"text":"Consequently, VQA models are limited in their interpretability power and the possibility to probe the model about specific image regions.","cats":{"new-dataset":0}}
{"text":"This paper proposes a novel approach for medical VQA that addresses this limitation by developing a model that can answer questions about image regions while considering the context necessary to answer the questions.","cats":{"new-dataset":0}}
{"text":"Artificial intelligence applications enable farmers to optimize crop growth and production while reducing costs and environmental impact.","cats":{"new-dataset":0}}
{"text":"Computer vision-based algorithms in particular, are commonly used for fruit segmentation, enabling in-depth analysis of the harvest quality and accurate yield estimation.","cats":{"new-dataset":0}}
{"text":"In this paper, we propose TomatoDIFF, a novel diffusion-based model for semantic segmentation of on-plant tomatoes.","cats":{"new-dataset":0}}
{"text":"When evaluated against other competitive methods, our model demonstrates state-of-the-art (SOTA) performance, even in challenging environments with highly occluded fruits.  ","cats":{"new-dataset":0}}
{"text":"Large language models~(LLMs) obtain instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data.","cats":{"new-dataset":0}}
{"text":"However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT.","cats":{"new-dataset":0}}
{"text":"In this paper, we propose a simple and effective data selection strategy that automatically identifies and removes low-quality data using a strong LLM (e.g., ChatGPT).","cats":{"new-dataset":0}}
{"text":"To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data.","cats":{"new-dataset":0}}
{"text":"AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and its 13B variant matches $>90\\%$ performance of its teacher LLM (i.e., Text-Davinci-003) on test tasks.","cats":{"new-dataset":0}}
{"text":"It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes \\footnote{We apply IFT for the same number of epochs as Alpaca(7B)","cats":{"new-dataset":0}}
{"text":"but on fewer data, using 4$\\times$NVIDIA A100 (80GB) GPUs and following the original Alpaca setting and hyperparameters.}.","cats":{"new-dataset":0}}
{"text":"Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models.","cats":{"new-dataset":0}}
{"text":"Our project page is available at: \\url{https://lichang-chen.github.io/AlpaGasus/}.","cats":{"new-dataset":0}}
{"text":"We also introduce a large-scale dataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with over two million frames, making it the largest natural-scene video depth dataset to our knowledge.","cats":{"new-dataset":1}}
{"text":"Video depth estimation aims to infer temporally consistent depth.","cats":{"new-dataset":0}}
{"text":"Some methods achieve temporal consistency by finetuning a single-image depth model during test time using geometry and re-projection constraints, which is inefficient and not robust.","cats":{"new-dataset":0}}
{"text":"An alternative approach is to learn how to enforce temporal consistency from data, but this requires well-designed models and sufficient video depth data.","cats":{"new-dataset":0}}
{"text":"To address these challenges, we propose a plug-and-play framework called Neural Video Depth Stabilizer (NVDS) that stabilizes inconsistent depth estimations and can be applied to different single-image depth models without extra effort.  ","cats":{"new-dataset":0}}
{"text":"We evaluate our method on the VDW dataset as well as two public benchmarks and demonstrate significant improvements in consistency, accuracy, and efficiency compared to previous approaches.","cats":{"new-dataset":0}}
{"text":"Our work serves as a solid baseline and provides a data foundation for learning-based video depth models.","cats":{"new-dataset":0}}
{"text":"Using COLLIE, we compile the COLLIE-v1 dataset with 2080 instances comprising 13 constraint structures.","cats":{"new-dataset":1}}
{"text":"Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models.","cats":{"new-dataset":0}}
{"text":"However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g.,generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4.","cats":{"new-dataset":0}}
{"text":"We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning).","cats":{"new-dataset":0}}
{"text":"We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus.  ","cats":{"new-dataset":0}}
{"text":"We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings.","cats":{"new-dataset":0}}
{"text":"COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future.","cats":{"new-dataset":0}}
{"text":"In this paper, we focus on interactions between humans and small indoor robots and introduce a new human-robot interaction (HRI) dataset","cats":{"new-dataset":1}}
{"text":"The dataset used in this analysis is available at: https://github.com/AlexanderDavid/ZuckerDataset.","cats":{"new-dataset":0}}
{"text":"In recent years there has been a large focus on how robots can operate in human populated environments. .","cats":{"new-dataset":0}}
{"text":"The analysis of the recorded experiments shows that anticipatory and non-reactive robot controllers impose similar constraints to humans' safety and efficiency.","cats":{"new-dataset":0}}
{"text":"Additionally, we found that current state-of-the-art models for human trajectory prediction can adequately extend to indoor HRI settings.","cats":{"new-dataset":0}}
{"text":"Finally, we show that humans respond differently in shared and homogeneous environments when collisions are imminent, since interacting with small differential drives can only cause a finite level of social discomfort as compared to human-human interactions.","cats":{"new-dataset":0}}
{"text":"To validate our model, we built a new dataset based on the well-known Matterport3D and REVERIE datasets.","cats":{"new-dataset":1}}
{"text":"This dataset consists of instructions with complex referring expressions accompanied by real indoor environmental images that feature various target objects, in addition to pixel-wise segmentation masks.","cats":{"new-dataset":1}}
{"text":"In this study, we aim to develop a model that comprehends a natural language instruction (e.g., \"Go to the living room and get the nearest pillow to the radio art on the wall\") and generates a segmentation mask for the target everyday object.","cats":{"new-dataset":0}}
{"text":"The task is challenging because it requires (1) the understanding of the referring expressions for multiple objects in the instruction, (2) the prediction of the target phrase of the sentence among the multiple phrases, and (3) the generation of pixel-wise segmentation masks rather than bounding boxes.","cats":{"new-dataset":0}}
{"text":"Studies have been conducted on languagebased segmentation methods; however, they sometimes mask irrelevant regions for complex sentences.","cats":{"new-dataset":0}}
{"text":"In this paper, we propose the Multimodal Diffusion Segmentation Model (MDSM), which generates a mask in the first stage and refines it in the second stage.","cats":{"new-dataset":0}}
{"text":"We introduce a crossmodal parallel feature extraction mechanism and extend diffusion probabilistic models to handle crossmodal features.  ","cats":{"new-dataset":0}}
{"text":"This dataset consists of instructions with complex referring expressions accompanied by real indoor envi","cats":{"new-dataset":0}}
{"text":"LLMs have demonstrated remarkable abilities at interacting with humans through language, especially with the usage of instruction-following data.","cats":{"new-dataset":0}}
{"text":"Recent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further enlarge their abilities by incorporating multi-modal inputs, including image, video, and speech.","cats":{"new-dataset":0}}
{"text":"Despite their effectiveness at generating precise and detailed language understanding of the given modality signal, these LLMs give up the ability to ground specific parts of inputs, thus only constructing a coarse-grained mapping.","cats":{"new-dataset":0}}
{"text":"However, explicit and informative correspondence between text and other modalities will not only improve the user experience but also help to expand the application scenario of multi-modal LLMs.","cats":{"new-dataset":0}}
{"text":"Therefore, we propose BuboGPT, a multi-modal LLM with visual grounding that can perform cross-modal interaction between vision, audio and language, providing fine-grained understanding of visual objects and other given modalities.","cats":{"new-dataset":0}}
{"text":"As a result, BuboGPT is able to point out the specific location of an object in the image, when it is generating response or description for that object.","cats":{"new-dataset":0}}
{"text":"Our contributions are two-fold: 1) An off-the-shelf visual grounding module based on SAM that extracts entities in a sentence and find corresponding masks in the image.","cats":{"new-dataset":0}}
{"text":"2) A two-stage training scheme and instruction dataset to endow joint text-image-audio understanding.","cats":{"new-dataset":0}}
{"text":"Our experiments show that BuboGPT achieves impressive multi-modality understanding and visual grounding abilities during the interaction with human.","cats":{"new-dataset":0}}
{"text":"It performs consistently well when provided by arbitrary modality combinations (either aligned or unaligned).","cats":{"new-dataset":0}}
{"text":"Our code, model and dataset are available at https://bubo-gpt.github.io .","cats":{"new-dataset":0}}
{"text":"To this end, we present G-Scan, the first end-to-end fine-grained line-level vulnerability detection system evaluated on the first-of-its-kind real world dataset.","cats":{"new-dataset":1}}
{"text":"We train and evaluate G-Scan on a collected real world smart contracts dataset with line-level annotations on reentrancy vulnerability, one of the most common and severe types of smart contract vulnerabilities","cats":{"new-dataset":1}}
{"text":"Due to the immutable and decentralized nature of Ethereum (ETH) platform, smart contracts are prone to security risks that can result in financial loss.","cats":{"new-dataset":0}}
{"text":"While existing machine learning-based vulnerability detection algorithms achieve high accuracy at the contract level, they require developers to manually inspect source code to locate bugs.  ","cats":{"new-dataset":0}}
{"text":"G-Scan first converts smart contracts to code graphs in a dependency and hierarchy preserving manner.","cats":{"new-dataset":0}}
{"text":"Next, we train a graph neural network to identify vulnerable nodes and assess security risks.","cats":{"new-dataset":0}}
{"text":"Finally, the code graphs with node vulnerability predictions are mapped back to the smart contracts for line-level localization.","cats":{"new-dataset":0}}
{"text":"We train and evaluate G-Scan on a collected real world smart contracts dataset with line-level annotations on reentrancy vulnerability, one of the most common andore in line-level vulnerability localization.","cats":{"new-dataset":0}}
{"text":"Additionally, the lightweight graph neural network enables G-Scan to localize vulnerabilities in 6.1k lines of code smart contract within 1.2 seconds.","cats":{"new-dataset":0}}
{"text":"In this paper, I train a Bidirectional Long Short-Term Memory (BiLSTM) model on a novel dataset of voter registration data from all 50 US states","cats":{"new-dataset":1}}
{"text":"In the absence of sensitive race and ethnicity data, researchers, regulators, and firms alike turn to proxies.  ","cats":{"new-dataset":0}}
{"text":"and create an ensemble that achieves up to 36.8% higher out of sample (OOS) F1 scores than the best performing machine learning models in the literature.","cats":{"new-dataset":0}}
{"text":"Additionally, I construct the most comprehensive database of first and surname distributions in the US in order to improve the coverage and accuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved Firstname Surname Geocoding (BIFSG).","cats":{"new-dataset":0}}
{"text":"Researchers have invested considerable effort into ensuring that large language models (LLMs) align with human values, using various training techniques, such as instruction tuning and Reinforcement Learning from Human or AI Feedback (RLHF/RLAIF), to guard against text unsafety.","cats":{"new-dataset":0}}
{"text":"However, these defenses remain incredibly vulnerable to some jailbreak attacks, which can cause the model to become overly defensive to sensitive topics or still generate harmful content, leaving the model performance particularly fragile.","cats":{"new-dataset":0}}
{"text":"Therefore, to comprehensively study text safety and output robustness, we propose a latent jailbreak prompt dataset, each involving malicious instruction embedding.","cats":{"new-dataset":0}}
{"text":"Specifically, we instruct the model to complete a regular task, such as translation, where the text to be translated contains malicious instructions.","cats":{"new-dataset":0}}
{"text":"To further analyze the safety and robustness, we design a hierarchical annotation framework.","cats":{"new-dataset":0}}
{"text":"We present a systematic analysis of the safety and robustness of LLMs concerning the position of explicit normal instructions, word replacement (verbs in explicit normal instructions, target groups in malicious instructions, cue words in malicious instructions), and instruction replacement (different explicit normal instructions).","cats":{"new-dataset":0}}
{"text":"Our results show that current LLMs not only have a preference for certain instruction verbs, but also exhibit different jailbreak rates for different instruction verbs in explicit normal instructions.","cats":{"new-dataset":0}}
{"text":"In other words, the probability of generating unsafe content by the model will be reinforced to varying degrees depending on the instruction verb in explicit normal instructions.","cats":{"new-dataset":0}}
{"text":"Code and data are available at https://github.com/qiuhuachuan/latent-jailbreak.","cats":{"new-dataset":0}}
{"text":"Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time.","cats":{"new-dataset":0}}
{"text":"Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks.","cats":{"new-dataset":0}}
{"text":"Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search.","cats":{"new-dataset":0}}
{"text":"However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs.","cats":{"new-dataset":0}}
{"text":"To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models.","cats":{"new-dataset":0}}
{"text":"To achieve this, a streaming, low latency approximation to the random-walk based features is proposed.","cats":{"new-dataset":0}}
{"text":"In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop operations on the incoming edges.","cats":{"new-dataset":0}}
{"text":"We evaluate our proposed approach on three open-source datasets and two in-house datasets, and compare with three state-of-the-art algorithms (TGN-attn, TGN-ID, Jodie).","cats":{"new-dataset":0}}
{"text":"We demonstrate that our graph-sprints features, combined with a machine learning classifier, achieve competitive performance (outperforming all baselines for the node classification tasks in five datasets).","cats":{"new-dataset":0}}
{"text":"Simultaneously, graph-sprints significantly reduce inference latencies, achieving close to an order of magnitude speed-up in our experimental setting.","cats":{"new-dataset":0}}
{"text":"We collected and published a dataset of 18,000 images in lab and hospital environments.","cats":{"new-dataset":1}}
{"text":"The contribution is to establish a baseline on this new dataset and to provide a proof of concept for the human emergency detection in this use case.","cats":{"new-dataset":1}}
{"text":"Human transports in hospitals are labor-intensive and primarily performed in beds to save time.","cats":{"new-dataset":0}}
{"text":"This transfer method does not promote the mobility or autonomy of the patient.","cats":{"new-dataset":0}}
{"text":"To relieve the caregivers from this time-consuming task, a mobile robot is developed to autonomously transport humans around the hospital.","cats":{"new-dataset":0}}
{"text":"It provides different transfer modes including walking and sitting in a wheelchair.","cats":{"new-dataset":0}}
{"text":"The problem that this paper focuses on is to detect emergencies and ensure the well-being of the patient during the transport.","cats":{"new-dataset":0}}
{"text":"For this purpose, the patient is tracked and monitored with a camera system.","cats":{"new-dataset":0}}
{"text":"OpenPose is used for Human Pose Estimation and a trained classifier for emergency detection.  ","cats":{"new-dataset":0}}
{"text":"It differs from related work because we have a moving robot with different transfer modes in a highly dynamic environment with multiple people in the scene using only RGB-D data.","cats":{"new-dataset":0}}
{"text":"To improve the critical recall metric, we apply threshold moving and a time delay.","cats":{"new-dataset":0}}
{"text":"We compare different models with an AutoML approach.","cats":{"new-dataset":0}}
{"text":"This paper shows that emergencies while walking are best detected by a SVM with a recall of 95.8% on single frames.","cats":{"new-dataset":0}}
{"text":"In the case of sitting transport, the best model achieves a recall of 62.2%.","cats":{"new-dataset":0}}
{"text":"The contribution is to establish a baseline on this new dataset and to provide a proof","cats":{"new-dataset":0}}
{"text":"End-to-end model, especially Recurrent Neural Network Transducer (RNN-T), has achieved great success in speech recognition.","cats":{"new-dataset":0}}
{"text":"However, transducer requires a great memory footprint and computing time when processing a long decoding sequence.","cats":{"new-dataset":0}}
{"text":"To solve this problem, we propose a model named time-sparse transducer, which introduces a time-sparse mechanism into transducer.","cats":{"new-dataset":0}}
{"text":"In this mechanism, we obtain the intermediate representations by reducing the time resolution of the hidden states.","cats":{"new-dataset":0}}
{"text":"Then the weighted average algorithm is used to combine these representations into sparse hidden states followed by the decoder.","cats":{"new-dataset":0}}
{"text":"All the experiments are conducted on a Mandarin dataset AISHELL-1.","cats":{"new-dataset":0}}
{"text":"Compared with RNN-T, the character error rate of the time-sparse transducer is close to RNN-T and the real-time factor is 50.00% of the original.","cats":{"new-dataset":0}}
{"text":"By adjusting the time resolution, the time-sparse transducer can also reduce the real-time factor to 16.54% of the original at the expense of a 4.94% loss of precision.","cats":{"new-dataset":0}}
{"text":"To address this abstraction gap and provide a fair evaluation of the proposed method, we develop our method on a large-scale synthetic dataset covering 500k+ buildings with well-defined ground truths of polyhedral class labels.","cats":{"new-dataset":1}}
{"text":"We present PolyGNN, a polyhedron-based graph neural network for 3D building reconstruction from point clouds.","cats":{"new-dataset":0}}
{"text":"PolyGNN learns to assemble primitives obtained by polyhedral decomposition via graph node classification, achieving a watertight, compact, and weakly semantic reconstruction.","cats":{"new-dataset":0}}
{"text":"To effectively represent arbitrary-shaped polyhedra in the neural network, we propose three different sampling strategies to select representative points as polyhedron-wise queries, enabling efficient occupancy inference.","cats":{"new-dataset":0}}
{"text":"Furthermore, we incorporate the inter-polyhedron adjacency to enhance the classification of the graph nodes.","cats":{"new-dataset":0}}
{"text":"We also observe that existing city-building models are abstractions of the underlying instances.  ","cats":{"new-dataset":0}}
{"text":"We further conduct a transferability analysis across cities and on real-world point clouds.","cats":{"new-dataset":0}}
{"text":"Both qualitative and quantitative results demonstrate the effectiveness of our method, particularly its efficiency for large-scale reconstructions.","cats":{"new-dataset":0}}
{"text":"The source code and data of our work are available at https://github.com/chenzhaiyu/polygnn.","cats":{"new-dataset":0}}
{"text":"Curating an informative and representative dataset is essential for enhancing the performance of 2D object detectors.","cats":{"new-dataset":0}}
{"text":"We present a novel active learning sampling strategy that addresses both the informativeness and diversity of the selections.","cats":{"new-dataset":0}}
{"text":"Our strategy integrates uncertainty and diversity-based selection principles into a joint selection objective by measuring the collective information score of the selected samples.","cats":{"new-dataset":0}}
{"text":"Specifically, our proposed NORIS algorithm quantifies the impact of training with a sample on the informativeness of other similar samples.","cats":{"new-dataset":0}}
{"text":"By exclusively selecting samples that are simultaneously informative and distant from other highly informative samples, we effectively avoid redundancy while maintaining a high level of informativeness.","cats":{"new-dataset":0}}
{"text":"Moreover, instead of utilizing whole image features to calculate distances between samples, we leverage features extracted from detected object regions within images to define object features.","cats":{"new-dataset":0}}
{"text":"This allows us to construct a dataset encompassing diverse object types, shapes, and angles.","cats":{"new-dataset":0}}
{"text":"Extensive experiments on object detection and image classification tasks demonstrate the effectiveness of our strategy over the state-of-the-art baselines.","cats":{"new-dataset":0}}
{"text":"Specifically, our selection strategy achieves a 20% and 30% reduction in labeling costs compared to random selection for PASCAL-VOC and KITTI, respectively.","cats":{"new-dataset":0}}
{"text":"To provide a more robust evaluation of the proposed method, a large-scale clinical image dataset of skin diseases with significantly more cases than existing datasets has been established.","cats":{"new-dataset":1}}
{"text":"Skin diseases are among the most prevalent health issues, and accurate computer-aided diagnosis methods are of importance for both dermatologists and patients.","cats":{"new-dataset":0}}
{"text":"However, most of the existing methods overlook the essential domain knowledge required for skin disease diagnosis.","cats":{"new-dataset":0}}
{"text":"A novel multi-task model, namely DermImitFormer, is proposed to fill this gap by imitating dermatologists' diagnostic procedures and strategies.","cats":{"new-dataset":0}}
{"text":"Through multi-task learning, the model simultaneously predicts body parts and lesion attributes in addition to the disease itself, enhancing diagnosis accuracy and improving diagnosis interpretability.","cats":{"new-dataset":0}}
{"text":"The designed lesion selection module mimics dermatologists' zoom-in action, effectively highlighting the local lesion features from noisy backgrounds.","cats":{"new-dataset":0}}
{"text":"Additionally, the presented cross-interaction module explicitly models the complicated diagnostic reasoning between body parts, lesion attributes, and diseases.  ","cats":{"new-dataset":0}}
{"text":"Extensive experiments on three different datasets consistently demonstrate the state-of-the-art recognition performance of the proposed approach.","cats":{"new-dataset":0}}
{"text":"To stimulate research in this exciting area, we present LOAF, the first large-scale overhead fisheye dataset for person detection and localization.","cats":{"new-dataset":1}}
{"text":"it contains currently the largest number of annotated pedestrian, i.e., 457K bounding boxes with groundtruth location information","cats":{"new-dataset":1}}
{"text":"Location determination finds wide applications in daily life.","cats":{"new-dataset":0}}
{"text":"Instead of existing efforts devoted to localizing tourist photos captured by perspective cameras, in this article, we focus on devising person positioning solutions using overhead fisheye cameras.","cats":{"new-dataset":0}}
{"text":"Such solutions are advantageous in large field of view (FOV), low cost, anti-occlusion, and unaggressive work mode (without the necessity of cameras carried by persons).","cats":{"new-dataset":0}}
{"text":"However, related studies are quite scarce, due to the paucity of data.  ","cats":{"new-dataset":0}}
{"text":"LOAF is built with many essential features, e.g., i) the data cover abundant diversities in scenes, human pose, density, and location; ii) it contains currently the largest number of annotated pedestrian, i.e., 457K bounding boxes with groundtruth location information; iii) the body-boperson detection network, which exploits the fisheye distortions by a rotation-equivariant training strategy and predict radius-aligned human boxes end-to-end.","cats":{"new-dataset":0}}
{"text":"Then, the actual locations of the detected persons are calculated by a numerical solution on the fisheye model and camera altitude data.","cats":{"new-dataset":0}}
{"text":"Extensive experiments on LOAF validate the superiority of our fisheye detector w.r.t.","cats":{"new-dataset":0}}
{"text":"previous methods, and show that our whole fisheye positioning solution is able to locate all persons in FOV with an accuracy of 0.5 m, within 0.1 s.","cats":{"new-dataset":0}}
{"text":"Hand trajectory forecasting from egocentric views is crucial for enabling a prompt understanding of human intentions when interacting with AR/VR systems.","cats":{"new-dataset":0}}
{"text":"However, existing methods handle this problem in a 2D image space which is inadequate for 3D real-world applications.","cats":{"new-dataset":0}}
{"text":"In this paper, we set up an egocentric 3D hand trajectory forecasting task that aims to predict hand trajectories in a 3D space from early observed RGB videos in a first-person view.","cats":{"new-dataset":0}}
{"text":"To fulfill this goal, we propose an uncertainty-aware state space Transformer (USST) that takes the merits of the attention mechanism and aleatoric uncertainty within the framework of the classical state-space model.","cats":{"new-dataset":0}}
{"text":"The model can be further enhanced by the velocity constraint and visual prompt tuning (VPT) on large vision transformers.","cats":{"new-dataset":0}}
{"text":"Moreover, we develop an annotation workflow to collect 3D hand trajectories with high quality.","cats":{"new-dataset":0}}
{"text":"Experimental results on H2O and EgoPAT3D datasets demonstrate the superiority of USST for both 2D and 3D trajectory forecasting.","cats":{"new-dataset":0}}
{"text":"The code and datasets are publicly released: https://github.com/Cogito2012/USST.","cats":{"new-dataset":0}}
{"text":"We also build a stereo visual acquisition system composed of an event camera and an RGB-D camera to collect a new Stereo Event-Intensity Dataset (SEID) containing diverse scenes with complex motions and varying depths.","cats":{"new-dataset":1}}
{"text":"The stereo event-intensity camera setup is widely applied to leverage the advantages of both event cameras with low latency and intensity cameras that capture accurate brightness and texture information.","cats":{"new-dataset":0}}
{"text":"However, such a setup commonly encounters cross-modality parallax that is difficult to be eliminated solely with stereo rectification especially for real-world scenes with complex motions and varying depths, posing artifacts and distortion for existing Event-based Video Frame Interpolation (E-VFI) approaches.","cats":{"new-dataset":0}}
{"text":"To tackle this problem, we propose a novel Stereo Event-based VFI (SE-VFI) network (SEVFI-Net) to generate high-quality intermediate frames and corresponding disparities from misaligned inputs consisting of two consecutive keyframes and event streams emitted between them.","cats":{"new-dataset":0}}
{"text":"Specifically, we propose a Feature Aggregation Module (FAM) to alleviate the parallax and achieve spatial alignment in the feature domain.","cats":{"new-dataset":0}}
{"text":"We then exploit the fused features accomplishing accurate optical flow and disparity estimation, and achieving better interpolated results through flow-based and synthesis-based ways.  Experiments on public real-world stereo datasets, i.e., DSEC and MVSEC, and our SEID dataset demonstrate that our proposed SEVFI-Net outperforms state-of-the-art methods by a large margin.","cats":{"new-dataset":0}}
{"text":"We evaluated the framework with five Python and Java code generation models and six prompt datasets, including a newly created one in this work (SOEval).","cats":{"new-dataset":1}}
{"text":"In recent years, the use of automated source code generation utilizing transformer-based generative models has expanded, and these models can generate functional code according to the requirements of the developers.","cats":{"new-dataset":0}}
{"text":"However, recent research revealed that these automatically generated source codes can contain vulnerabilities and other quality issues.","cats":{"new-dataset":0}}
{"text":"Despite researchers' and practitioners' attempts to enhance code generation models, retraining and fine-tuning large language models is time-consuming and resource-intensive.","cats":{"new-dataset":0}}
{"text":"Thus, we describe FRANC, a lightweight framework for recommending more secure and high-quality source code derived from transformer-based code generation models.","cats":{"new-dataset":0}}
{"text":"FRANC includes a static filter to make the generated code compilable with heuristics and a quality-aware ranker to sort the code snippets based on a quality score.","cats":{"new-dataset":0}}
{"text":"Moreover, the framework uses prompt engineering to fix persistent quality issues.  ","cats":{"new-dataset":0}}
{"text":"The static filter improves 9% to 46% Java suggestions and 10% to 43% Python suggestions regarding compilability.","cats":{"new-dataset":0}}
{"text":"The average improvement over the NDCG@10 score for the ranking system is 0.0763, and the repairing techniques repair the highest 80% of prompts.","cats":{"new-dataset":0}}
{"text":"FRANC takes, on average, 1.98 seconds for Java; for Python, it takes 0.08 seconds.","cats":{"new-dataset":0}}
{"text":"Lane detection plays a pivotal role in the field of autonomous vehicles and advanced driving assistant systems (ADAS).","cats":{"new-dataset":0}}
{"text":"Over the years, numerous algorithms have emerged, spanning from rudimentary image processing techniques to sophisticated deep neural networks.","cats":{"new-dataset":0}}
{"text":"The performance of deep learning-based models is highly dependent on the quality of their training data.","cats":{"new-dataset":0}}
{"text":"Consequently, these models often experience a decline in performance when confronted with challenging scenarios such as extreme lighting conditions, partially visible lane markings, and sparse lane markings like Botts' dots.","cats":{"new-dataset":0}}
{"text":"To address this, we present an end-to-end lane detection and classification system based on deep learning methodologies.","cats":{"new-dataset":0}}
{"text":"In our study, we introduce a unique dataset meticulously curated to encompass scenarios that pose significant challenges for state-of-the-art (SOTA) models.","cats":{"new-dataset":0}}
{"text":"Through fine-tuning selected models, we aim to achieve enhanced localization accuracy.","cats":{"new-dataset":0}}
{"text":"Moreover, we propose a CNN-based classification branch, seamlessly integrated with the detector, facilitating the identification of distinct lane types.","cats":{"new-dataset":0}}
{"text":"This architecture enables informed lane-changing decisions and empowers more resilient ADAS capabilities.","cats":{"new-dataset":0}}
{"text":"We also investigate the effect of using mixed precision training and testing on different models and batch sizes.","cats":{"new-dataset":0}}
{"text":"Experimental evaluations conducted on the widely-used TuSimple dataset, Caltech lane dataset, and our LVLane dataset demonstrate the effectiveness of our model in accurately detecting and classifying lanes amidst challenging scenarios.","cats":{"new-dataset":0}}
{"text":"Our method achieves state-of-the-art classification results on the TuSimple dataset.","cats":{"new-dataset":0}}
{"text":"The code of the work will be published upon the acceptance of the paper.","cats":{"new-dataset":0}}
{"text":"In today's highly connected society, we are constantly asked to provide personal information to retailers, voter surveys, medical professionals, and other data collection efforts.","cats":{"new-dataset":0}}
{"text":"The collected data is stored in large data warehouses.","cats":{"new-dataset":0}}
{"text":"Organisations and statistical agencies share and use this data to facilitate research in public health, economics, sociology, etc.","cats":{"new-dataset":0}}
{"text":"However, this data contains sensitive information about individuals, which can result in identity theft, financial loss, stress and depression, embarrassment, abuse, etc.","cats":{"new-dataset":0}}
{"text":"Therefore, one must ensure rigorous management of individuals' privacy.","cats":{"new-dataset":0}}
{"text":"We propose, an advanced data privacy management architecture composed of three layers.","cats":{"new-dataset":0}}
{"text":"The data management layer consists of de-identification and anonymisation, the access management layer for re-enforcing data access based on the concepts of Role-Based Access Control and the Chinese Wall Security Policy, and the roles layer for regulating different users.","cats":{"new-dataset":0}}
{"text":"The proposed system architecture is validated on healthcare datasets.","cats":{"new-dataset":0}}
{"text":"Answer selection in open-domain dialogues aims to select an accurate answer from candidates.","cats":{"new-dataset":0}}
{"text":"Recent success of answer selection models hinges on training with large amounts of labeled data.","cats":{"new-dataset":0}}
{"text":"However, collecting large-scale labeled data is labor-intensive and time-consuming.","cats":{"new-dataset":0}}
{"text":"In this paper, we introduce the predicted intent labels to calibrate answer labels in a self-training paradigm.","cats":{"new-dataset":0}}
{"text":"Specifically, we propose the intent-calibrated self-training (ICAST) to improve the quality of pseudo answer labels through the intent-calibrated answer selection paradigm, in which we employ pseudo intent labels to help improve pseudo answer labels.","cats":{"new-dataset":0}}
{"text":"We carry out extensive experiments on two benchmark datasets with open-domain dialogues.","cats":{"new-dataset":0}}
{"text":"The experimental results show that ICAST outperforms baselines consistently with 1%, 5% and 10% labeled data.","cats":{"new-dataset":0}}
{"text":"Specifically, it improves 2.06% and 1.00% of F1 score on the two datasets, compared with the strongest baseline with only 5% labeled data.","cats":{"new-dataset":0}}
{"text":"Indirect surveys, in which respondents provide information about other people they know, have been proposed for scenarios where privacy is important or where the population to be surveyed is hard to reach.","cats":{"new-dataset":0}}
{"text":"As an example, during various stages of the COVID-19 pandemic surveys, including indirect surveys, have been used to estimate the number of cases or the level of vaccination.","cats":{"new-dataset":0}}
{"text":"The Network Scale-up Method (NSUM) is the classical approach to developing such estimates but was designed with discrete, time-limited indirect surveys in mind.","cats":{"new-dataset":0}}
{"text":"Further, it requires asking for or estimating the number of individuals in each respondent's network.","cats":{"new-dataset":0}}
{"text":"In recent years, surveys are being increasingly deployed online and collecting data continuously (e.g., COVID-19 surveys on Facebook during much of the pandemic).","cats":{"new-dataset":0}}
{"text":"Conventional NSUM can be applied to these scenarios by analyzing the data independently during each time interval, but this misses the opportunity of leveraging the temporal dimension.","cats":{"new-dataset":0}}
{"text":"Understanding the advantage of simply smoothing NSUM results to various degrees is not trivial.","cats":{"new-dataset":0}}
{"text":"We propose to use the responses from indirect surveys collected over time and develop analytical tools (i) to prove that indirect surveys can be used to provide better estimates for the size of the hidden population compared to direct surveys, and (ii) to identify appropriate aggregations over time to further improve the estimates.","cats":{"new-dataset":0}}
{"text":"We demonstrate through simulations that our approach outperforms traditional NSUM and direct surveying methods to estimate the size of a time-varying hidden population.","cats":{"new-dataset":0}}
{"text":"We also demonstrate the superiority of our approach on an existing indirect survey dataset on COVID-19 confirmed cases.","cats":{"new-dataset":0}}
{"text":"[Context] Systematic Literature Review (SLR) has been a major type of study published in Software Engineering (SE) venues for about two decades.","cats":{"new-dataset":0}}
{"text":"However, there is a lack of understanding of whether an SLR is really needed in comparison to a more conventional literature review.","cats":{"new-dataset":0}}
{"text":"Very often, SE researchers embark on an SLR with such doubts.","cats":{"new-dataset":0}}
{"text":"We aspire to provide more understanding of when an SLR in SE should be conducted.","cats":{"new-dataset":0}}
{"text":"[Objective] The first step of our investigation was focused on the dataset, i.e., the reviewed papers, in an SLR, which indicates the development of a research topic or area.","cats":{"new-dataset":0}}
{"text":"The objective of this step is to provide a better understanding of the characteristics of the datasets of SLRs in SE.","cats":{"new-dataset":0}}
{"text":"[Method] A research synthesis was conducted on a sample of 170 SLRs published in top-tier SE journals.","cats":{"new-dataset":0}}
{"text":"We extracted and analysed the quantitative attributes of the datasets of these SLRs.","cats":{"new-dataset":0}}
{"text":"[Results]","cats":{"new-dataset":0}}
{"text":"The findings show that the median size of the datasets in our sample is 57 reviewed papers, and the median review period covered is 14 years.","cats":{"new-dataset":0}}
{"text":"The number of reviewed papers and review period have a very weak and non-significant positive correlation.","cats":{"new-dataset":0}}
{"text":"[Conclusions] The results of our study can be used by SE researchers as an indicator or benchmark to understand whether an SLR is conducted at a good time.","cats":{"new-dataset":0}}
{"text":"OCR (Optical Character Recognition) is a technology that offers comprehensive alphanumeric recognition of handwritten and printed characters at electronic speed by merely scanning the document.","cats":{"new-dataset":0}}
{"text":"Recently, the understanding of visual data has been termed Intelligent Character Recognition (ICR).","cats":{"new-dataset":0}}
{"text":"Intelligent Character Recognition (ICR) is the OCR module that can convert scans of handwritten or printed characters into ASCII text.","cats":{"new-dataset":0}}
{"text":"ASCII data is the standard format for data encoding in electronic communication.","cats":{"new-dataset":0}}
{"text":"ASCII assigns standard numeric values to letters, numeral, symbols, white-spaces and other characters.","cats":{"new-dataset":0}}
{"text":"In more technical terms, OCR is the process of using an electronic device to transform 2-Dimensional textual information into machine-encoded text.","cats":{"new-dataset":0}}
{"text":"Anything that contains text both machine written or handwritten can be scanned either through a scanner or just simply a picture of the text is enough for the recognition system to distinguish the text.","cats":{"new-dataset":0}}
{"text":"The goal of this papers is to show the results of a Convolutional Neural Network model which has been trained on National Institute of Science and Technology (NIST) dataset containing over a 100,000 images.","cats":{"new-dataset":0}}
{"text":"The network learns from the features extracted from the images and use it to generate the probability of each class to which the picture belongs to.","cats":{"new-dataset":0}}
{"text":"We have achieved an accuracy of 90.54% with a loss of 2.53%.","cats":{"new-dataset":0}}
{"text":"We compare the performance of three nearest neighbor search algorithms: the Orchard, ball tree, and VP-tree algorithms.","cats":{"new-dataset":0}}
{"text":"These algorithms are commonly used for nearest-neighbor searches and are known for their efficiency in large datasets.","cats":{"new-dataset":0}}
{"text":"We analyze the fraction of distances computed in relation to the size of the dataset and its dimension.","cats":{"new-dataset":0}}
{"text":"For each algorithm we derive a fitting function for the efficiency as a function to set size and dimension.","cats":{"new-dataset":0}}
{"text":"The article aims to provide a comprehensive analysis of the performance of these algorithms and help researchers and practitioners choose the best algorithm for their specific application.","cats":{"new-dataset":0}}
{"text":"In order to broaden the application and research of anomaly detection in unmanned supermarkets and smart manufacturing, we introduce the supermarket goods anomaly detection (GoodsAD) dataset","cats":{"new-dataset":1}}
{"text":"It contains 6124 high-resolution images of 484 different appearance goods divided into 6 categories","cats":{"new-dataset":1}}
{"text":"This is a comprehensive, multi-object dataset for supermarket goods anomaly detection that focuses on real-world applications.","cats":{"new-dataset":0}}
{"text":"Visual anomaly detection is essential and commonly used for many tasks in the field of computer vision.","cats":{"new-dataset":0}}
{"text":"Recent anomaly detection datasets mainly focus on industrial automated inspection, medical image analysis and video surveillance. .","cats":{"new-dataset":0}}
{"text":"It contains 6124 high-resolution images of 484 different appearance goods divided into 6 categories.","cats":{"new-dataset":0}}
{"text":"Each category contains several common different types of anomalies such as deformation, s the unsupervised setting and only normal (defect-free) images are used for training.","cats":{"new-dataset":0}}
{"text":"Pixel-precise ground truth regions are provided for all anomalies.","cats":{"new-dataset":0}}
{"text":"Moreover, we also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods.","cats":{"new-dataset":0}}
{"text":"This initial benchmark indicates that some methods which perform well on the industrial anomaly detection dataset (e.g., MVTec AD), show poor performance on our dataset.","cats":{"new-dataset":0}}
{"text":"We also made available all the synthetic data generated in this work for the 18 different datasets in the BEIR benchmark which took more than 2,000 GPU hours to be generated as well as the reranker models finetuned on the synthetic data.","cats":{"new-dataset":1}}
{"text":"Recent work has explored Large Language Models (LLMs) to overcome the lack of training data for Information Retrieval (IR) tasks.","cats":{"new-dataset":0}}
{"text":"The generalization abilities of these models have enabled the creation of synthetic in-domain data by providing instructions and a few examples on a prompt.","cats":{"new-dataset":0}}
{"text":"InPars and Promptagator have pioneered this approach and both methods have demonstrated the potential of using LLMs as synthetic data generators for IR tasks.","cats":{"new-dataset":0}}
{"text":"This makes them an attractive solution for IR tasks that suffer from a lack of annotated data.","cats":{"new-dataset":0}}
{"text":"However, the reproducibility of these methods was limited, because InPars' training scripts are based on TPUs -- which are not widely accessible -- and because the code for Promptagator was not released and its proprietary LLM is not publicly accessible.","cats":{"new-dataset":0}}
{"text":"To fully realize the potential of these methods and make their impact more widespread in the research community, the resources need to be accessible and easy to reproduce by researchers and practitioners.","cats":{"new-dataset":0}}
{"text":"Our main contribution is a unified toolkit for end-to-end reproducible synthetic data generation research, which includes generation, filtering, training and evaluation.","cats":{"new-dataset":0}}
{"text":"Additionally, we provide an interface to IR libraries widely used by the community and support for GPU.","cats":{"new-dataset":0}}
{"text":"Our toolkit not only reproduces the InPars method and partially reproduces Promptagator, but also provides a plug-and-play functionality allowing the use of different LLMs, exploring filtering methods and finetuning various reranker models on the generated data.  ","cats":{"new-dataset":0}}
{"text":"Code and data are available at https://github.com/zetaalphavector/InPars","cats":{"new-dataset":0}}
{"text":"Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations.","cats":{"new-dataset":0}}
{"text":"Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence.","cats":{"new-dataset":0}}
{"text":"Recent studies have suggested that score-based diffusion models are effective in adversarial defenses.","cats":{"new-dataset":0}}
{"text":"However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results.","cats":{"new-dataset":0}}
{"text":"In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors.","cats":{"new-dataset":0}}
{"text":"Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.","cats":{"new-dataset":0}}
{"text":"To overcome these limitations, we build a new challenging benchmark named KoRc in this paper","cats":{"new-dataset":1}}
{"text":"Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years.","cats":{"new-dataset":0}}
{"text":"However, these benchmarks have encountered two major limitations.","cats":{"new-dataset":0}}
{"text":"On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage.","cats":{"new-dataset":0}}
{"text":"On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. .","cats":{"new-dataset":0}}
{"text":"Compared with previous benchmarks, KoRC has two advantages, i.e., broad knowledge coverage and flexible answer format.","cats":{"new-dataset":0}}
{"text":"Specifically, we utilize massive knowledge bases to guide annotators or large language models (LLMs) to construct knowledgable questions.","cats":{"new-dataset":0}}
{"text":"Moreover, we use labels in knowledge bases rather than spans or choices as the final answers.","cats":{"new-dataset":0}}
{"text":"We test state-of-the-art models on KoRC and the experimental results show that the strongest baseline only achieves 68.3% and 30.0% F1 measure in the in-distribution and out-of-distribution test set, respectively.","cats":{"new-dataset":0}}
{"text":"These results indicate that deep text understanding is still an unsolved challenge.","cats":{"new-dataset":0}}
{"text":"The benchmark dataset, leaderboard, and baseline methods are released in https://github.com/","cats":{"new-dataset":0}}
{"text":"Schema discovery is an important aspect to working with data in formats such as JSON.","cats":{"new-dataset":0}}
{"text":"Unlike relational databases, JSON data sets often do not have associated structural information.","cats":{"new-dataset":0}}
{"text":"Consumers of such datasets are often left to browse through data in an attempt to observe commonalities in structure across documents to construct suitable code for data processing.","cats":{"new-dataset":0}}
{"text":"However, this process is time-consuming and error-prone.","cats":{"new-dataset":0}}
{"text":"Existing distributed approaches to mining schemas present a significant usability advantage as they provide useful metadata for large data sources.","cats":{"new-dataset":0}}
{"text":"However, depending on the data source, ad hoc queries for estimating other properties to help with crafting an efficient data pipeline can be expensive.","cats":{"new-dataset":0}}
{"text":"We propose JSONoid, a distributed schema discovery process augmented with additional metadata in the form of monoid data structures that are easily maintainable in a distributed setting.","cats":{"new-dataset":0}}
{"text":"JSONoid subsumes several existing approaches to distributed schema discovery with similar performance.","cats":{"new-dataset":0}}
{"text":"Our approach also adds significant useful additional information about data values to discovered schemas with linear scalability.","cats":{"new-dataset":0}}
{"text":"Scientific publications follow conventionalized rhetorical structures.","cats":{"new-dataset":0}}
{"text":"Classifying the Argumentative Zone (AZ), e.g., identifying whether a sentence states a Motivation, a Result or Background information, has been proposed to improve processing of scholarly documents.","cats":{"new-dataset":0}}
{"text":"In this work, we adapt and extend this idea to the domain of materials science research.  ","cats":{"new-dataset":0}}
{"text":"We detail corpus statistics and demonstrate high inter-annotator agreement.","cats":{"new-dataset":0,"data-quality":0}}
{"text":"Our computational experiments show that using domain-specific pre-trained transformer-based text encoders is key to high classification performance.","cats":{"new-dataset":0}}
{"text":"We also find that AZ categories from existing datasets in other domains are transferable to varying degrees.","cats":{"new-dataset":0}}
{"text":"One of the key problems in 3D object detection is to reduce the accuracy gap between methods based on LiDAR sensors and those based on monocular cameras.","cats":{"new-dataset":0}}
{"text":"A recently proposed framework for monocular 3D detection based on Pseudo-Stereo has received considerable attention in the community.","cats":{"new-dataset":0}}
{"text":"However, so far these two problems are discovered in existing practices, including (1) monocular depth estimation and Pseudo-Stereo detector must be trained separately, (2) Difficult to be compatible with different stereo detectors and (3) the overall calculation is large, which affects the reasoning speed.","cats":{"new-dataset":0}}
{"text":"In this work, we propose an end-to-end, efficient pseudo-stereo 3D detection framework by introducing a Single-View Diffusion Model (SVDM) that uses a few iterations to gradually deliver right informative pixels to the left image.","cats":{"new-dataset":0}}
{"text":"SVDM allows the entire pseudo-stereo 3D detection pipeline to be trained end-to-end and can benefit from the training of stereo detectors.","cats":{"new-dataset":0}}
{"text":"Afterwards, we further explore the application of SVDM in depth-free stereo 3D detection, and the final framework is compatible with most stereo detectors.","cats":{"new-dataset":0}}
{"text":"We consider the problem of learning a sparse graph underlying an undirected Gaussian graphical model, a key problem in statistical machine learning.","cats":{"new-dataset":0}}
{"text":"Given $n$ samples from a multivariate Gaussian distribution with $p$ variables, the goal is to estimate the $p \\times p$ inverse covariance matrix (aka precision matrix), assuming it is sparse (i.e., has a few nonzero entries).","cats":{"new-dataset":0}}
{"text":"We propose GraphL0BnB, a new estimator based on an $\\ell_0$-penalized version of the pseudolikelihood function, while most earlier approaches are based on the $\\ell_1$-relaxation.","cats":{"new-dataset":0}}
{"text":"Our estimator can be formulated as a convex mixed integer program (MIP) which can be difficult to compute at scale using off-the-shelf commercial solvers.","cats":{"new-dataset":0}}
{"text":"To solve the MIP, we propose a custom nonlinear branch-and-bound (BnB) framework that solves node relaxations with tailored first-order methods.","cats":{"new-dataset":0}}
{"text":"As a by-product of our BnB framework, we propose large-scale solvers for obtaining good primal solutions that are of independent interest.","cats":{"new-dataset":0}}
{"text":"We derive novel statistical guarantees (estimation and variable selection) for our estimator and discuss how our approach improves upon existing estimators.","cats":{"new-dataset":0}}
{"text":"Our numerical experiments on real/synthetic datasets suggest that our method can solve, to near-optimality, problem instances with $p = 10^4$ -- corresponding to a symmetric matrix of size $p \\times p$ with $p^2/2$ binary variables.","cats":{"new-dataset":0}}
{"text":"We demonstrate the usefulness of GraphL0BnB versus various state-of-the-art approaches on a range of datasets.","cats":{"new-dataset":0}}
{"text":"Trajectory data collection is a common task with many applications in our daily lives.","cats":{"new-dataset":0}}
{"text":"Analyzing trajectory data enables service providers to enhance their services, which ultimately benefits users.","cats":{"new-dataset":0}}
{"text":"However, directly collecting trajectory data may give rise to privacy-related issues that cannot be ignored.","cats":{"new-dataset":0}}
{"text":"Local differential privacy (LDP), as the de facto privacy protection standard in a decentralized setting, enables users to perturb their trajectories locally and provides a provable privacy guarantee.","cats":{"new-dataset":0}}
{"text":"Existing approaches to private trajectory data collection in a local setting typically use relaxed versions of LDP, which cannot provide a strict privacy guarantee, or require some external knowledge that is impractical to obtain and update in a timely manner.","cats":{"new-dataset":0}}
{"text":"To tackle these problems, we propose a novel trajectory perturbation mechanism that relies solely on an underlying location set and satisfies pure $\\epsilon$-LDP to provide a stringent privacy guarantee.","cats":{"new-dataset":0}}
{"text":"In the proposed mechanism, each point's adjacent direction information in the trajectory is used in its perturbation process.","cats":{"new-dataset":0}}
{"text":"Such information serves as an effective clue to connect neighboring points and can be used to restrict the possible region of a perturbed point in order to enhance utility.","cats":{"new-dataset":0}}
{"text":"To the best of our knowledge, our study is the first to use direction information for trajectory perturbation under LDP.","cats":{"new-dataset":0}}
{"text":"Furthermore, based on this mechanism, we present an anchor-based method that adaptively restricts the region of each perturbed trajectory, thereby significantly boosting performance without violating the privacy constraint.","cats":{"new-dataset":0}}
{"text":"Extensive experiments on both real-world and synthetic datasets demonstrate the effectiveness of the proposed mechanisms.","cats":{"new-dataset":0}}
{"text":"In safety-critical classification tasks, conformal prediction allows to perform rigorous uncertainty quantification by providing confidence sets including the true class with a user-specified probability.","cats":{"new-dataset":0}}
{"text":"This generally assumes the availability of a held-out calibration set with access to ground truth labels.","cats":{"new-dataset":0}}
{"text":"Unfortunately, in many domains, such labels are difficult to obtain and usually approximated by aggregating expert opinions.","cats":{"new-dataset":0}}
{"text":"In fact, this holds true for almost all datasets, including well-known ones such as CIFAR and ImageNet.","cats":{"new-dataset":0}}
{"text":"Applying conformal prediction using such labels underestimates uncertainty.","cats":{"new-dataset":0}}
{"text":"Indeed, when expert opinions are not resolvable, there is inherent ambiguity present in the labels.","cats":{"new-dataset":0}}
{"text":"That is, we do not have ``crisp'', definitive ground truth labels and this uncertainty should be taken into account during calibration.","cats":{"new-dataset":0}}
{"text":"In this paper, we develop a conformal prediction framework for such ambiguous ground truth settings which relies on an approximation of the underlying posterior distribution of labels given inputs.","cats":{"new-dataset":0}}
{"text":"We demonstrate our methodology on synthetic and real datasets, including a case study of skin condition classification in dermatology.","cats":{"new-dataset":0}}
{"text":"In order to implement the pretraining phase, we curated an expansive tabular dataset comprising approximately 13 billion samples, meticulously gathered from the Kaggle platform.","cats":{"new-dataset":1}}
{"text":"Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks.","cats":{"new-dataset":0}}
{"text":"This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks.","cats":{"new-dataset":0}}
{"text":"The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time.","cats":{"new-dataset":0}}
{"text":"In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures.","cats":{"new-dataset":0}}
{"text":"UniTabE's core concept relies on representing each basic table element with a module, termed TabUnit.","cats":{"new-dataset":0}}
{"text":"This is subsequently followed by a Transformer encoder to refine the representation.","cats":{"new-dataset":0}}
{"text":"Moreover, our model is designed to facilitate pretraining and finetuning through the utilization of free-form prompts.  ","cats":{"new-dataset":0}}
{"text":"Rigorous experimental testing and analyses were performed under a myriad of scenarios to validate the effectiveness of our methodology.","cats":{"new-dataset":0}}
{"text":"The experimental results demonstrate UniTabE's superior performance against several baseline models across a multitude of benchmark datasets.","cats":{"new-dataset":0}}
{"text":"This, therefore, underscores UniTabE's potential to significantly enhance the semantic representation of tabular data, thereby marking a significant stride in the field of tabular data analysis.","cats":{"new-dataset":0}}
{"text":"Large-scale online recommender system spreads all over the Internet being in charge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion Rate (CVR) estimations.","cats":{"new-dataset":0}}
{"text":"However, traditional CVR estimators suffer from well-known Sample Selection Bias and Data Sparsity issues.","cats":{"new-dataset":0}}
{"text":"Entire space models were proposed to address the two issues via tracing the decision-making path of \"exposure_click_purchase\".","cats":{"new-dataset":0}}
{"text":"Further, some researchers observed that there are purchase-related behaviors between click and purchase, which can better draw the user's decision-making intention and improve the recommendation performance.","cats":{"new-dataset":0}}
{"text":"Thus, the decision-making path has been extended to \"exposure_click_in-shop action_purchase\" and can be modeled with conditional probability approach.","cats":{"new-dataset":0}}
{"text":"Nevertheless, we observe that the chain rule of conditional probability does not always hold.","cats":{"new-dataset":0}}
{"text":"We report Probability Space Confusion (PSC) issue and give a derivation of difference between ground-truth and estimation mathematically.","cats":{"new-dataset":0}}
{"text":"We propose a novel Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint (ESMC) and two alternatives: Entire Space Multi-Task Model with Siamese Network (ESMS) and Entire Space Multi-Task Model in Global Domain (ESMG) to address the PSC issue.","cats":{"new-dataset":0}}
{"text":"Specifically, we handle \"exposure_click_in-shop action\" and \"in-shop action_purchase\" separately in the light of characteristics of in-shop action.","cats":{"new-dataset":0}}
{"text":"The first path is still treated with conditional probability while the second one is treated with parameter constraint strategy.","cats":{"new-dataset":0}}
{"text":"Experiments on both offline and online environments in a large-scale recommendation system illustrate the superiority of our proposed methods over state-of-the-art models.","cats":{"new-dataset":0}}
{"text":"This paper proposes a new SOD dataset consisting of 39,070 images including 137,121 bird instances, which is called the Small Object Detection for Spotting Birds (SOD4SB) dataset","cats":{"new-dataset":1}}
{"text":"The dataset, the baseline code, and the website for evaluation on the public testset are publicly available.","cats":{"new-dataset":0}}
{"text":"Small Object Detection (SOD) is an important machine vision topic because (i) a variety of real-world applications require object detection for distant objects and (ii) SOD is a challenging task due to the noisy, blurred, and less-informative image appearances of small objects. .","cats":{"new-dataset":0}}
{"text":"The detail of the challenge with the SOD4SB dataset is introduced in this paper.","cats":{"new-dataset":0}}
{"text":"In total, 223 participants joined this challenge.","cats":{"new-dataset":0}}
{"text":"This paper briefly introduces the award-winning methods.","cats":{"new-dataset":0}}
{"text":"The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions.","cats":{"new-dataset":0}}
{"text":"The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities.","cats":{"new-dataset":0}}
{"text":"The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images.","cats":{"new-dataset":0}}
{"text":"To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.   ","cats":{"new-dataset":0}}
{"text":"To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences.","cats":{"new-dataset":0}}
{"text":"Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge.","cats":{"new-dataset":0}}
{"text":"The Text-guided Image Restoration auxiliary task is proposed with the aim of implicitly mapping abstract textual entities to specific image regions, facilitating alignment between textual and visual embeddings.","cats":{"new-dataset":0}}
{"text":"Additionally, we introduce a cross-modal triplet loss tailored for handling hard samples, enhancing the model's ability to distinguish minor differences.   ","cats":{"new-dataset":0}}
{"text":"To focus the model on the key components within sentences, we propose a novel text data augmentation technique.","cats":{"new-dataset":0}}
{"text":"Diffusion MRI tractography parcellation classifies streamlines into anatomical fiber tracts to enable quantification and visualization for clinical and scientific applications.","cats":{"new-dataset":0}}
{"text":"Current tractography parcellation methods rely heavily on registration, but registration inaccuracies can affect parcellation and the computational cost of registration is high for large-scale datasets.","cats":{"new-dataset":0}}
{"text":"Recently, deep-learning-based methods have been proposed for tractography parcellation using various types of representations for streamlines.","cats":{"new-dataset":0}}
{"text":"However, these methods only focus on the information from a single streamline, ignoring geometric relationships between the streamlines in the brain.","cats":{"new-dataset":0}}
{"text":"We propose TractCloud, a registration-free framework that performs whole-brain tractography parcellation directly in individual subject space.","cats":{"new-dataset":0}}
{"text":"We propose a novel, learnable, local-global streamline representation that leverages information from neighboring and whole-brain streamlines to describe the local anatomy and global pose of the brain.","cats":{"new-dataset":0}}
{"text":"We train our framework on a large-scale labeled tractography dataset, which we augment by applying synthetic transforms including rotation, scaling, and translations.","cats":{"new-dataset":0}}
{"text":"We test our framework on five independently acquired datasets across populations and health conditions.","cats":{"new-dataset":0}}
{"text":"TractCloud significantly outperforms several state-of-the-art methods on all testing datasets.","cats":{"new-dataset":0}}
{"text":"TractCloud achieves efficient and consistent whole-brain white matter parcellation across the lifespan (from neonates to elderly subjects, including brain tumor patients) without the need for registration.","cats":{"new-dataset":0}}
{"text":"The robustness and high inference speed of TractCloud make it suitable for large-scale tractography data analysis.","cats":{"new-dataset":0}}
{"text":"Our project page is available at https://tractcloud.github.io/.","cats":{"new-dataset":0}}
{"text":"The AgBioData Consortium (https://www.agbiodata.org/) currently represents 44 databases and resources covering model or crop plant and animal GGB data, ontologies, pathways, genetic variation and breeding platforms (referred to as 'databases' throughout).","cats":{"new-dataset":1}}
{"text":"Over the last several decades, there has been rapid growth in the number and scope of agricultural genetics, genomics and breeding (GGB) databases and resources.  ","cats":{"new-dataset":0}}
{"text":"One of the goals of the Consortium is to facilitate FAIR (Findable, Accessible, Interoperable, and Reusable) data management and the integration of datasets which requires data sharing, along with structured vocabularies and/or ontologies.","cats":{"new-dataset":0}}
{"text":"Two AgBioData working groups, focused on Data Sharing and Ontologies, conducted a survey to assess the status and future needs of the members in those areas.","cats":{"new-dataset":0}}
{"text":"A total of 33 researchers responded to the survey, representing 37 databases.","cats":{"new-dataset":0}}
{"text":"Results suggest that data sharing practices by AgBioData databases are in a healthy state, but it is not clear whether this is true for all metadata and data types across all databases; and that ontology use has not substantially changed since a similar survey was conducted in 2017.","cats":{"new-dataset":0}}
{"text":"We recommend 1) providing training for database personnel in specific data sharing techniques, as well as in ontology use; 2) further study on what metadata is shared, and how well it is shared among databases; 3) promoting an understanding of data sharing and ontologies in the stakeholder community; 4) improving data sharing and ontologies for specific phenotypic data types and formats; and 5) lowering specific barriers to data sharing and ontology use, by identifying sustainability solutions, and the identification, promotion, or development of data standards.","cats":{"new-dataset":0}}
{"text":"Combined, these improvements are likely to help AgBioData databases increase development efforts towards improved ontology use, and data sharing via programmatic means.","cats":{"new-dataset":0}}
{"text":"To feed the data-hungry models, we first elaborate interference generators and conduct comprehensive co-location experiments on a testbed to build Alioth-dataset which reflects the complexity and dynamicity in real-world scenarios","cats":{"new-dataset":1}}
{"text":"The dataset and code of Alioth have been released on GitHub.","cats":{"new-dataset":0}}
{"text":"Multi-tenancy in public clouds may lead to co-location interference on shared resources, which possibly results in performance degradation of cloud applications.","cats":{"new-dataset":0}}
{"text":"Cloud providers want to know when such events happen and how serious the degradation is, to perform interference-aware migrations and alleviate the problem.","cats":{"new-dataset":0}}
{"text":"However, virtual machines (VM) in Infrastructure-as-a-Service public clouds are black-boxes to providers, where application-level performance information cannot be acquired.","cats":{"new-dataset":0}}
{"text":"This makes performance monitoring intensely challenging as cloud providers can only rely on low-level metrics such as CPU usage and hardware counters.   ","cats":{"new-dataset":0}}
{"text":"We propose a novel machine learning framework, Alioth, to monitor the performance degradation of cloud applications. .","cats":{"new-dataset":0}}
{"text":"Then we construct Alioth by (1) augmenting features via recovering low-level metrics under no interference using denoising auto-encoders, (2) devising a transfer learning model based on domain adaptation neural network to make models generalize on test cases unseen in offline training, and (3) developing a SHAP explainer to automate feature selection and enhance model interpretability.","cats":{"new-dataset":0}}
{"text":"Experiments show that Alioth achieves an average mean absolute error of 5.29% offline and 10.8% when testing on applications unseen in the training stage, outperforming the baseline methods.","cats":{"new-dataset":0}}
{"text":"Alioth is also robust in signaling quality-of-service violation under dynamicity.","cats":{"new-dataset":0}}
{"text":"Finally, we demonstrate a possible application of Alioth's interpretability, providing insights to benefit the decision-making of cloud operators.","cats":{"new-dataset":0}}
{"text":"We introduce the problem of knot-based inverse perceptual art.","cats":{"new-dataset":0}}
{"text":"Given multiple target images and their corresponding viewing configurations, the objective is to find a 3D knot-based tubular structure whose appearance resembles the target images when viewed from the specified viewing configurations.","cats":{"new-dataset":0}}
{"text":"To solve this problem, we first design a differentiable rendering algorithm for rendering tubular knots embedded in 3D for arbitrary perspective camera configurations.","cats":{"new-dataset":0}}
{"text":"Utilizing this differentiable rendering algorithm, we search over the space of knot configurations to find the ideal knot embedding.","cats":{"new-dataset":0}}
{"text":"We represent the knot embeddings via homeomorphisms of the desired template knot, where the homeomorphisms are parametrized by the weights of an invertible neural network.","cats":{"new-dataset":0}}
{"text":"Our approach is fully differentiable, making it possible to find the ideal 3D tubular structure for the desired perceptual art using gradient-based optimization.","cats":{"new-dataset":0}}
{"text":"We propose several loss functions that impose additional physical constraints, ensuring that the tube is free of self-intersection, lies within a predefined region in space, satisfies the physical bending limits of the tube material and the material cost is within a specified budget.","cats":{"new-dataset":0}}
{"text":"We demonstrate through results that our knot representation is highly expressive and gives impressive results even for challenging target images in both single view as well as multiple view constraints.","cats":{"new-dataset":0}}
{"text":"Through extensive ablation study we show that each of the proposed loss function is effective in ensuring physical realizability.","cats":{"new-dataset":0}}
{"text":"To the best of our knowledge, we are the first to propose a fully differentiable optimization framework for knot-based inverse perceptual art.","cats":{"new-dataset":0}}
{"text":"Both the code and data will be made publicly available.","cats":{"new-dataset":0}}
{"text":"To do so, a comparable big dataset of metadata of apps has been collected for learning and evaluation in this work.","cats":{"new-dataset":1}}
{"text":"In the digitized world, smartphones and their apps play an important role.","cats":{"new-dataset":0}}
{"text":"To name just a few examples, some apps offer possibilities for entertainment, others for online banking, and others offer support for two-factor authentication.","cats":{"new-dataset":0}}
{"text":"Therefore, with smartphones also, sensitive information is shared; thus, they are a desirable target for malware.","cats":{"new-dataset":0}}
{"text":"The following technical report gives an overview of how machine learning, especially neural networks, can be employed to detect malicious Android apps based on their metadata.","cats":{"new-dataset":0}}
{"text":"Detection based on the metadata is necessary since not all of an app's information is readable from another app due to the security layout of Android.  ","cats":{"new-dataset":0}}
{"text":"The first section, after the introduction, presents the related work, followed by the description of the sources of the dataset and the selection of the features used for machine learning, in this case, only the app permissions.","cats":{"new-dataset":0}}
{"text":"Afterward, a free available dataset is used to find an efficient and effective neural network model for learning and evaluation.","cats":{"new-dataset":0}}
{"text":"Here, the fully connected network type consisting of dense layers is chosen.","cats":{"new-dataset":0}}
{"text":"It turns out that this model detects malware with an accuracy of 92.93% based on an app's permissions.","cats":{"new-dataset":0}}
{"text":"To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information.","cats":{"new-dataset":1}}
{"text":"Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training.","cats":{"new-dataset":1}}
{"text":"To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning","cats":{"new-dataset":1}}
{"text":"Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness.  ","cats":{"new-dataset":0}}
{"text":"Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounFurthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-shot and few-shot learning scenarios demonstrate the superiority of DialogStudio.","cats":{"new-dataset":0}}
{"text":"To improve transparency and support dataset and task-based research, as well as language model pre-train","cats":{"new-dataset":0}}
{"text":"Movement paths are used widely in intelligent transportation and smart city applications.","cats":{"new-dataset":0}}
{"text":"To serve such applications, path representation learning aims to provide compact representations of paths that enable efficient and accurate operations when used for different downstream tasks such as path ranking and travel cost estimation.","cats":{"new-dataset":0}}
{"text":"In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited environments and under green computing limitations, it is essential.","cats":{"new-dataset":0}}
{"text":"Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource consumption and scalability.   ","cats":{"new-dataset":0}}
{"text":"We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce resource consumption and achieve scalability without affecting accuracy, thus enabling broader applicability.","cats":{"new-dataset":0}}
{"text":"More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good scalability with respect to path length.","cats":{"new-dataset":0}}
{"text":"Next, we propose a relational reasoning framework to enable faster training of more robust sparse path encoders.","cats":{"new-dataset":0}}
{"text":"We also propose global-local knowledge distillation to further reduce the size and improve the performance of sparse path encoders.","cats":{"new-dataset":0}}
{"text":"Visual (re)localization is critical for various applications in computer vision and robotics.","cats":{"new-dataset":0}}
{"text":"Its goal is to estimate the 6 degrees of freedom (DoF) camera pose for each query image, based on a set of posed database images.","cats":{"new-dataset":0}}
{"text":"Currently, all leading solutions are structure-based that either explicitly construct 3D metric maps from the database with structure-from-motion, or implicitly encode the 3D information with scene coordinate regression models.","cats":{"new-dataset":0}}
{"text":"On the contrary, visual localization without reconstructing the scene in 3D offers clear benefits.","cats":{"new-dataset":0}}
{"text":"It makes deployment more convenient by reducing database pre-processing time, releasing storage requirements, and remaining unaffected by imperfect reconstruction, etc.","cats":{"new-dataset":0}}
{"text":"In this technical report, we demonstrate that it is possible to achieve high localization accuracy without reconstructing the scene from the database.","cats":{"new-dataset":0}}
{"text":"The key to achieving this owes to a tailored motion averaging over database-query pairs.","cats":{"new-dataset":0}}
{"text":"Experiments show that our visual localization proposal, LazyLoc, achieves comparable performance against state-of-the-art structure-based methods.","cats":{"new-dataset":0}}
{"text":"Furthermore, we showcase the versatility of LazyLoc, which can be easily extended to handle complex configurations such as multi-query co-localization and camera rigs.","cats":{"new-dataset":0}}
{"text":"This paper focuses on motion prediction for point cloud sequences in the challenging case of deformable 3D objects, such as human body motion.","cats":{"new-dataset":0}}
{"text":"First, we investigate the challenges caused by deformable shapes and complex motions present in this type of representation, with the ultimate goal of understanding the technical limitations of state-of-the-art models.","cats":{"new-dataset":0}}
{"text":"From this understanding, we propose an improved architecture for point cloud prediction of deformable 3D objects.","cats":{"new-dataset":0}}
{"text":"Specifically, to handle deformable shapes, we propose a graph-based approach that learns and exploits the spatial structure of point clouds to extract more representative features.","cats":{"new-dataset":0}}
{"text":"Then we propose a module able to combine the learned features in an adaptative manner according to the point cloud movements.","cats":{"new-dataset":0}}
{"text":"The proposed adaptative module controls the composition of local and global motions for each point, enabling the network to model complex motions in deformable 3D objects more effectively.","cats":{"new-dataset":0}}
{"text":"We tested the proposed method on the following datasets: MNIST moving digits, the Mixamo human bodies motions, JPEG and CWIPC-SXR real-world dynamic bodies.","cats":{"new-dataset":0}}
{"text":"Simulation results demonstrate that our method outperforms the current baseline methods given its improved ability to model complex movements as well as preserve point cloud shape.","cats":{"new-dataset":0}}
{"text":"Furthermore, we demonstrate the generalizability of the proposed framework for dynamic feature learning, by testing the framework for action recognition on the MSRAction3D dataset and achieving results on-par with state-of-the-art methods","cats":{"new-dataset":0}}
{"text":"Multimodal image registration is a challenging but essential step for numerous image-guided procedures.","cats":{"new-dataset":0}}
{"text":"Most registration algorithms rely on the computation of complex, frequently non-differentiable similarity metrics to deal with the appearance discrepancy of anatomical structures between imaging modalities.","cats":{"new-dataset":0}}
{"text":"Recent Machine Learning based approaches are limited to specific anatomy-modality combinations and do not generalize to new settings.","cats":{"new-dataset":0}}
{"text":"We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global registration.","cats":{"new-dataset":0}}
{"text":"We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neural network (CNN) which is inherently differentiable can be trained without registered data.","cats":{"new-dataset":0}}
{"text":"Our method is several orders of magnitude faster than local patch-based metrics and can be directly applied in clinical settings by replacing the similarity measure with the proposed one.","cats":{"new-dataset":0}}
{"text":"We make our training code and data publicly available.","cats":{"new-dataset":0}}
{"text":"Cross-lingual image captioning is confronted with both cross-lingual and cross-modal challenges for multimedia analysis.","cats":{"new-dataset":0}}
{"text":"The crucial issue in this task is to model the global and local matching between the image and different languages.","cats":{"new-dataset":0}}
{"text":"Existing cross-modal embedding methods based on Transformer architecture oversight the local matching between the image region and monolingual words, not to mention in the face of a variety of differentiated languages.","cats":{"new-dataset":0}}
{"text":"Due to the heterogeneous property of the cross-modal and cross-lingual task, we utilize the heterogeneous network to establish cross-domain relationships and the local correspondences between the image and different languages.","cats":{"new-dataset":0}}
{"text":"In this paper, we propose an Embedded Heterogeneous Attention Transformer (EHAT) to build reasoning paths bridging cross-domain for cross-lingual image captioning and integrate into transformer.","cats":{"new-dataset":0}}
{"text":"The proposed EHAT consists of a Masked Heterogeneous Cross-attention (MHCA), Heterogeneous Attention Reasoning Network (HARN) and Heterogeneous Co-attention (HCA).","cats":{"new-dataset":0}}
{"text":"HARN as the core network, models and infers cross-domain relationship anchored by vision bounding box representation features to connect two languages word features and learn the heterogeneous maps.","cats":{"new-dataset":0}}
{"text":"MHCA and HCA implement cross-domain integration in the encoder through the special heterogeneous attention and enable single model to generate two language captioning.","cats":{"new-dataset":0}}
{"text":"We test on MSCOCO dataset to generate English and Chinese, which are most widely used and have obvious difference between their language families.","cats":{"new-dataset":0}}
{"text":"Our experiments show that our method even achieve better than advanced monolingual methods.","cats":{"new-dataset":0}}
{"text":"In this paper, we study the Greek wiretappings scandal, which has been revealed in 2022 and attracted a lot of attention by press and citizens.","cats":{"new-dataset":0}}
{"text":"Specifically, we propose a methodology for collecting data and analyzing patterns of online public discussions on Twitter.","cats":{"new-dataset":0}}
{"text":"We apply our methodology to the Greek wiretappings use case, and present findings related to the evolution of the discussion over time, its polarization, and the role of the media.","cats":{"new-dataset":0}}
{"text":"The methodology can be of wider use and replicated to other topics.","cats":{"new-dataset":0}}
{"text":"We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music.","cats":{"new-dataset":1}}
{"text":"Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks.","cats":{"new-dataset":0}}
{"text":"At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles.","cats":{"new-dataset":0}}
{"text":"This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles.","cats":{"new-dataset":0}}
{"text":"To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to.  ","cats":{"new-dataset":0}}
{"text":"Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each target domain dataset.","cats":{"new-dataset":0}}
{"text":"Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best source dataset varies for each music culture.","cats":{"new-dataset":0}}
{"text":"The implementation and the trained models are both provided in a public repository.","cats":{"new-dataset":0}}
{"text":"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs).","cats":{"new-dataset":0}}
{"text":"However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency.","cats":{"new-dataset":0}}
{"text":"In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region.","cats":{"new-dataset":0}}
{"text":"This enables MLLMs to focus on the region of interest and achieve finer-grained interaction.","cats":{"new-dataset":0}}
{"text":"Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience.","cats":{"new-dataset":0}}
{"text":"We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating.","cats":{"new-dataset":0}}
{"text":"Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction.","cats":{"new-dataset":0}}
{"text":"Experimental results showcase ChatSpot's promising performance.","cats":{"new-dataset":0}}
{"text":"To evaluate the proposed framework, we build a dataset consisting of 1414 2-minute video segments annotated with 13 actions and 112 video segments annotated with two engagement levels.","cats":{"new-dataset":1}}
{"text":"In this paper, we propose a novel technique for measuring behavioral engagement through students' actions recognition.","cats":{"new-dataset":0}}
{"text":"The proposed approach recognizes student actions then predicts the student behavioral engagement level.","cats":{"new-dataset":0}}
{"text":"For student action recognition, we use human skeletons to model student postures and upper body movements.","cats":{"new-dataset":0}}
{"text":"To learn the dynamics of student upper body, a 3D-CNN model is used.","cats":{"new-dataset":0}}
{"text":"The trained 3D-CNN model is used to recognize actions within every 2minute video segment then these actions are used to build a histogram of actions which encodes the student actions and their frequencies.","cats":{"new-dataset":0}}
{"text":"This histogram is utilized as an input to SVM classifier to classify whether the student is engaged or disengaged.  ","cats":{"new-dataset":0}}
{"text":"Experimental results indicate that student actions can be recognized with top 1 accuracy 83.63% and the proposed framework can capture the average engagement of the class.","cats":{"new-dataset":0}}
{"text":"Point cloud registration is to estimate a transformation to align point clouds collected in different perspectives.","cats":{"new-dataset":0}}
{"text":"In learning-based point cloud registration, a robust descriptor is vital for high-accuracy registration.","cats":{"new-dataset":0}}
{"text":"However, most methods are susceptible to noise and have poor generalization ability on unseen datasets.","cats":{"new-dataset":0}}
{"text":"Motivated by this, we introduce SphereNet to learn a noise-robust and unseen-general descriptor for point cloud registration.","cats":{"new-dataset":0}}
{"text":"In our method, first, the spheroid generator builds a geometric domain based on spherical voxelization to encode initial features.","cats":{"new-dataset":0}}
{"text":"Then, the spherical interpolation of the sphere is introduced to realize robustness against noise.","cats":{"new-dataset":0}}
{"text":"Finally, a new spherical convolutional neural network with spherical integrity padding completes the extraction of descriptors, which reduces the loss of features and fully captures the geometric features.","cats":{"new-dataset":0}}
{"text":"To evaluate our methods, a new benchmark 3DMatch-noise with strong noise is introduced.","cats":{"new-dataset":0}}
{"text":"Extensive experiments are carried out on both indoor and outdoor datasets.","cats":{"new-dataset":0}}
{"text":"Under high-intensity noise, SphereNet increases the feature matching recall by more than 25 percentage points on 3DMatch-noise.","cats":{"new-dataset":0}}
{"text":"In addition, it sets a new state-of-the-art performance for the 3DMatch and 3DLoMatch benchmarks with 93.5\\% and 75.6\\% registration recall and also has the best generalization ability on unseen datasets.","cats":{"new-dataset":0}}
{"text":"Sequential decision-making under uncertainty is often associated with long feedback delays.","cats":{"new-dataset":0}}
{"text":"Such delays degrade the performance of the learning agent in identifying a subset of arms with the optimal collective reward in the long run.","cats":{"new-dataset":0}}
{"text":"This problem becomes significantly challenging in a non-stationary environment with structural dependencies amongst the reward distributions associated with the arms.","cats":{"new-dataset":0}}
{"text":"Therefore, besides adapting to delays and environmental changes, learning the causal relations alleviates the adverse effects of feedback delay on the decision-making process.","cats":{"new-dataset":0}}
{"text":"We formalize the described setting as a non-stationary and delayed combinatorial semi-bandit problem with causally related rewards.","cats":{"new-dataset":0}}
{"text":"We model the causal relations by a directed graph in a stationary structural equation model.","cats":{"new-dataset":0}}
{"text":"The agent maximizes the long-term average payoff, defined as a linear function of the base arms' rewards.","cats":{"new-dataset":0}}
{"text":"We develop a policy that learns the structural dependencies from delayed feedback and utilizes that to optimize the decision-making while adapting to drifts.","cats":{"new-dataset":0}}
{"text":"We prove a regret bound for the performance of the proposed algorithm.","cats":{"new-dataset":0}}
{"text":"Besides, we evaluate our method via numerical analysis using synthetic and real-world datasets to detect the regions that contribute the most to the spread of Covid-19 in Italy.","cats":{"new-dataset":0}}
{"text":"In this work, we curate the first aerial thermal near-shore dataset","cats":{"new-dataset":1}}
{"text":"Code and datasets used in this work will be available at: https://github.com/connorlee77/uav-thermal-water-segmentation.","cats":{"new-dataset":1}}
{"text":"We present a new method to adapt an RGB-trained water segmentation network to target-domain aerial thermal imagery using online self-supervision by leveraging texture and motion cues as supervisory signals.","cats":{"new-dataset":0}}
{"text":"This new thermal capability enables current autonomous aerial robots operating in near-shore environments to perform tasks such as visual navigation, bathymetry, and flow tracking at night.","cats":{"new-dataset":0}}
{"text":"Our method overcomes the problem of scarce and difficult-to-obtain near-shore thermal data that prevents the application of conventional supervised and unsupervised methods.","cats":{"new-dataset":0}}
{"text":", show that our approach outperforms fully-supervised segmentation models trained on limited target-domain thermal data, and demonstrate real-time capabilities onboard an Nvidia Jetson embedded computing platform.","cats":{"new-dataset":0}}
{"text":"Code and datasets used in this work will be available at: https://g","cats":{"new-dataset":0}}
{"text":"Facial expression recognition (FER) remains a challenging task due to the ambiguity of expressions.","cats":{"new-dataset":0}}
{"text":"The derived noisy labels significantly harm the performance in real-world scenarios.","cats":{"new-dataset":0}}
{"text":"To address this issue, we present a new FER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarks to mitigate the impact of label noise from two perspectives.","cats":{"new-dataset":0}}
{"text":"Firstly, LA-Net uses landmark information to suppress the uncertainty in expression space and constructs the label distribution of each sample by neighborhood aggregation, which in turn improves the quality of training supervision.","cats":{"new-dataset":0}}
{"text":"Secondly, the model incorporates landmark information into expression representations using the devised expression-landmark contrastive loss.","cats":{"new-dataset":0}}
{"text":"The enhanced expression feature extractor can be less susceptible to label noise.","cats":{"new-dataset":0}}
{"text":"Our method can be integrated with any deep neural network for better training supervision without introducing extra inference costs.","cats":{"new-dataset":0}}
{"text":"We conduct extensive experiments on both in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net achieves state-of-the-art performance.","cats":{"new-dataset":0}}
{"text":"The JAZZVAR dataset is a collection of 502 pairs of Variation and Original MIDI segments.","cats":{"new-dataset":1}}
{"text":"Jazz pianists often uniquely interpret jazz standards.","cats":{"new-dataset":0}}
{"text":"Passages from these interpretations can be viewed as sections of variation.","cats":{"new-dataset":0}}
{"text":"We manually extracted such variations from solo jazz piano performances.  ","cats":{"new-dataset":0}}
{"text":"Each Variation in the dataset is accompanied by a corresponding Original segment containing the melody and chords from the original jazz standard.","cats":{"new-dataset":0}}
{"text":"Our approach differs from many existing jazz datasets in the music information retrieval (MIR) community, which often focus on improvisation sections within jazz performances.","cats":{"new-dataset":0}}
{"text":"In this paper, we outline the curation process for obtaining and sorting the repertoire, the pipeline for creating the Original and Variation pairs, and our analysis of the dataset.","cats":{"new-dataset":0}}
{"text":"We also introduce a new generative music task, Music Overpainting, and present a baseline Transformer model trained on the JAZZVAR dataset for this task.","cats":{"new-dataset":0}}
{"text":"Other potential applications of our dataset include expressive performance analysis and performer identification.","cats":{"new-dataset":0}}
{"text":"The plant community composition is an essential indicator of environmental changes and is, for this reason, usually analyzed in ecological field studies in terms of the so-called plant cover.","cats":{"new-dataset":0}}
{"text":"The manual acquisition of this kind of data is time-consuming, laborious, and prone to human error.","cats":{"new-dataset":0}}
{"text":"Automated camera systems can collect high-resolution images of the surveyed vegetation plots at a high frequency.","cats":{"new-dataset":0}}
{"text":"In combination with subsequent algorithmic analysis, it is possible to objectively extract information on plant community composition quickly and with little human effort.","cats":{"new-dataset":0}}
{"text":"An automated camera system can easily collect the large amounts of image data necessary to train a Deep Learning system for automatic analysis.","cats":{"new-dataset":0}}
{"text":"However, due to the amount of work required to annotate vegetation images with plant cover data, only few labeled samples are available.","cats":{"new-dataset":0}}
{"text":"As automated camera systems can collect many pictures without labels, we introduce an approach to interpolate the sparse labels in the collected vegetation plot time series down to the intermediate dense and unlabeled images to artificially increase our training dataset to seven times its original size.","cats":{"new-dataset":0}}
{"text":"Moreover, we introduce a new method we call Monte-Carlo Cropping.","cats":{"new-dataset":0}}
{"text":"This approach trains on a collection of cropped parts of the training images to deal with high-resolution images efficiently, implicitly augment the training images, and speed up training.","cats":{"new-dataset":0}}
{"text":"We evaluate both approaches on a plant cover dataset containing images of herbaceous plant communities and find that our methods lead to improvements in the species, community, and segmentation metrics investigated.","cats":{"new-dataset":0}}
{"text":"Existing image editing tools, while powerful, typically disregard the underlying 3D geometry from which the image is projected.","cats":{"new-dataset":0}}
{"text":"As a result, edits made using these tools may become detached from the geometry and lighting conditions that are at the foundation of the image formation process.","cats":{"new-dataset":0}}
{"text":"In this work, we formulate the newt ask of language-guided 3D-aware editing, where objects in an image should be edited according to a language instruction in context of the underlying 3D scene.  ","cats":{"new-dataset":0}}
{"text":"Each example consists of an input image, editing instruction in language, and the edited image.","cats":{"new-dataset":0}}
{"text":"We also introduce 3DIT : single and multi-task models for four editing tasks.","cats":{"new-dataset":0}}
{"text":"Our models show impressive abilities to understand the 3D composition of entire scenes, factoring in surrounding objects, surfaces, lighting conditions, shadows, and physically-plausible object configurations.","cats":{"new-dataset":0}}
{"text":"Surprisingly, training on only synthetic scenes from OBJECT, editing capabilities of 3DIT generalize to real-world images.","cats":{"new-dataset":0}}
{"text":"A finely annotated segmentation dataset of approximately 10,000 consec-utive frames recorded during surgery is constructed for the first time for this field, addressing the problem of semantic segmentation.","cats":{"new-dataset":1}}
{"text":"Endoscopic surgery is currently an important treatment method in the field of spinal surgery and avoiding damage to the spinal nerves through video guidance is a key challenge.","cats":{"new-dataset":0}}
{"text":"This paper presents the first real-time segmentation method for spinal nerves in endoscopic surgery, which provides crucial navigational information for surgeons.  ","cats":{"new-dataset":0}}
{"text":"Based on this dataset, we propose FUnet (Frame-Unet), which achieves state-of-the-art performance by utilizing inter-frame information and self-attention mechanisms.","cats":{"new-dataset":0}}
{"text":"We also conduct extended exper-iments on a similar polyp endoscopy video dataset and show that the model has good generalization ability with advantageous performance.","cats":{"new-dataset":0}}
{"text":"The dataset and code of this work are presented at: https://github.com/zzzzzzpc/FUnet .","cats":{"new-dataset":0}}
{"text":"Recent advances in deep learning have significantly improved the performance of various computer vision applications.","cats":{"new-dataset":0}}
{"text":"However, discovering novel categories in an incremental learning scenario remains a challenging problem due to the lack of prior knowledge about the number and nature of new categories.","cats":{"new-dataset":0}}
{"text":"Existing methods for novel category discovery are limited by their reliance on labeled datasets and prior knowledge about the number of novel categories and the proportion of novel samples in the batch.","cats":{"new-dataset":0}}
{"text":"To address the limitations and more accurately reflect real-world scenarios, in this paper, we propose a novel unsupervised class incremental learning approach for discovering novel categories on unlabeled sets without prior knowledge.","cats":{"new-dataset":0}}
{"text":"The proposed method fine-tunes the feature extractor and proxy anchors on labeled sets, then splits samples into old and novel categories and clusters on the unlabeled dataset.","cats":{"new-dataset":0}}
{"text":"Furthermore, the proxy anchors-based exemplar generates representative category vectors to mitigate catastrophic forgetting.","cats":{"new-dataset":0}}
{"text":"Self-supervised learning (SSL) for clinical time series data has received significant attention in recent literature, since these data are highly rich and provide important information about a patient's physiological state.","cats":{"new-dataset":0}}
{"text":"However, most existing SSL methods for clinical time series are limited in that they are designed for unimodal time series, such as a sequence of structured features (e.g., lab values and vitals signs) or an individual high-dimensional physiological signal (e.g., an electrocardiogram).","cats":{"new-dataset":0}}
{"text":"These existing methods cannot be readily extended to model time series that exhibit multimodality, with structured features and high-dimensional data being recorded at each timestep in the sequence.","cats":{"new-dataset":0}}
{"text":"In this work, we address this gap and propose a new SSL method -- Sequential Multi-Dimensional SSL -- where a SSL loss is applied both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence in order to better capture information at both scales.","cats":{"new-dataset":0}}
{"text":"Our strategy is agnostic to the specific form of loss function used at each level -- it can be contrastive, as in SimCLR, or non-contrastive, as in VICReg.","cats":{"new-dataset":0}}
{"text":"Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and in several settings, can lead to improvements across different self-supervised loss functions.","cats":{"new-dataset":0}}
{"text":"Human motion generation aims to generate natural human pose sequences and shows immense potential for real-world applications.","cats":{"new-dataset":0}}
{"text":"Most research within this field focuses on generating human motions based on conditional signals, such as text, audio, and scene contexts.","cats":{"new-dataset":0}}
{"text":"While significant advancements have been made in recent years, the task continues to pose challenges due to the intricate nature of human motion and its implicit relationship with conditional signals.","cats":{"new-dataset":0}}
{"text":"In this survey, we present a comprehensive literature review of human motion generation, which, to the best of our knowledge, is the first of its kind in this field.","cats":{"new-dataset":0}}
{"text":"We begin by introducing the background of human motion and generative models, followed by an examination of representative methods for three mainstream sub-tasks: text-conditioned, audio-conditioned, and scene-conditioned human motion generation.","cats":{"new-dataset":0}}
{"text":"Lastly, we discuss open problems and outline potential future research directions.","cats":{"new-dataset":0}}
{"text":"We hope that this survey could provide the community with a comprehensive glimpse of this rapidly evolving field and inspire novel ideas that address the outstanding challenges.","cats":{"new-dataset":0}}
{"text":"Having efficient testing strategies is a core challenge that needs to be overcome for the release of automated driving.","cats":{"new-dataset":0}}
{"text":"This necessitates clear requirements as well as suitable methods for testing.","cats":{"new-dataset":0}}
{"text":"In this work, the requirements for perception modules are considered with respect to relevance.","cats":{"new-dataset":0}}
{"text":"The concept of relevance currently remains insufficiently defined and specified.","cats":{"new-dataset":0}}
{"text":"In this paper, we propose a novel methodology to overcome this challenge by exemplary application to collision safety in the highway domain.","cats":{"new-dataset":0}}
{"text":"Using this general system and use case specification, a corresponding concept for relevance is derived.","cats":{"new-dataset":0}}
{"text":"Irrelevant objects are thus defined as objects which do not limit the set of safe actions available to the ego vehicle under consideration of all uncertainties.","cats":{"new-dataset":0}}
{"text":"As an initial step, the use case is decomposed into functional scenarios with respect to collision relevance.","cats":{"new-dataset":0}}
{"text":"For each functional scenario, possible actions of both the ego vehicle and any other dynamic object are formalized as equations.","cats":{"new-dataset":0}}
{"text":"This set of possible actions is constrained by traffic rules, yielding relevance criteria.","cats":{"new-dataset":0}}
{"text":"As a result, we present a conservative estimation which dynamic objects are relevant for perception and need to be considered for a complete evaluation.","cats":{"new-dataset":0}}
{"text":"The estimation provides requirements which are applicable for offline testing and validation of perception components.","cats":{"new-dataset":0}}
{"text":"A visualization is presented for examples from the highD dataset, showing the plausibility of the results.","cats":{"new-dataset":0}}
{"text":"Finally, a possibility for a future validation of the presented relevance concept is outlined.","cats":{"new-dataset":0}}
{"text":"In a conventional Speech emotion recognition (SER) task, a classifier for a given language is trained on a pre-existing dataset for that same language.","cats":{"new-dataset":0}}
{"text":"However, where training data for a language does not exist, data from other languages can be used instead.","cats":{"new-dataset":0}}
{"text":"We experiment with cross-lingual and multilingual SER, working with Amharic, English, German and URDU.","cats":{"new-dataset":0}}
{"text":"We followed previous research in mapping labels for all datasets to just two classes, positive and negative.","cats":{"new-dataset":0}}
{"text":"Thus we can compare performance on different languages directly, and combine languages for training and testing.","cats":{"new-dataset":0}}
{"text":"In Experiment 1, monolingual SER trials were carried out using three classifiers, AlexNet, VGGE (a proposed variant of VGG), and ResNet50.","cats":{"new-dataset":0}}
{"text":"Results averaged for the three models were very similar for ASED and RAVDESS, suggesting that Amharic and English SER are equally difficult.","cats":{"new-dataset":0}}
{"text":"Similarly, German SER is more difficult, and Urdu SER is easier.","cats":{"new-dataset":0}}
{"text":"In Experiment 2, we trained on one language and tested on another, in both directions for each pair:","cats":{"new-dataset":0}}
{"text":"Amharic<->German, Amharic<->English, and Amharic<->Urdu.","cats":{"new-dataset":0}}
{"text":"Results with Amharic as target suggested that using English or German as source will give the best result.","cats":{"new-dataset":0}}
{"text":"In Experiment 3, we trained on several non-Amharic languages and then tested on Amharic.","cats":{"new-dataset":0}}
{"text":"The best accuracy obtained was several percent greater than the best accuracy in Experiment 2, suggesting that a better result can be obtained when using two or three non-Amharic languages for training than when using just one non-Amharic language.","cats":{"new-dataset":0}}
{"text":"Overall, the results suggest that cross-lingual and multilingual training can be an effective strategy for training a SER classifier when resources for a language are scarce.","cats":{"new-dataset":0}}
{"text":"In this paper, we first establish a large-scale audio-visual quality assessment dataset for omnidirectional videos, which includes 375 distorted omnidirectional audio-visual (A/V) sequences generated from 15 high-quality pristine omnidirectional A/V contents","cats":{"new-dataset":1}}
{"text":"Omnidirectional videos (ODVs) play an increasingly important role in the application fields of medical, education, advertising, tourism, etc.","cats":{"new-dataset":0}}
{"text":"Assessing the quality of ODVs is significant for service-providers to improve the user's Quality of Experience (QoE).","cats":{"new-dataset":0}}
{"text":"However, most existing quality assessment studies for ODVs only focus on the visual distortions of videos, while ignoring that the overall QoE also depends on the accompanying audio signals.","cats":{"new-dataset":0}}
{"text":", and the corresponding perceptual audio-visual quality scores.","cats":{"new-dataset":0}}
{"text":"Then, we design three baseline methods for full-reference omnidirectional audio-visual quality assessment (OAVQA), which combine existing state-of-the-art single-mode audio and video QA models via multimodal fusion strategies.","cats":{"new-dataset":0}}
{"text":"We validate the effectiveness of the A/V multimodal fusion method for OAVQA on our dataset, which provides a new benchmark for omnidirectional QoE evaluation.","cats":{"new-dataset":0}}
{"text":"With the increasing amount of spatial-temporal~(ST) ocean data, numerous spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, e.g., climate forecasting and disaster warning.","cats":{"new-dataset":0}}
{"text":"Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated with some unique characteristics, e.g., diverse regionality and high sparsity.","cats":{"new-dataset":0}}
{"text":"These characteristics make it difficult to design and train STDM models.","cats":{"new-dataset":0}}
{"text":"Unfortunately, an overview of these studies is still missing, hindering computer scientists to identify the research issues in ocean while discouraging researchers in ocean science from applying advanced STDM techniques.","cats":{"new-dataset":0}}
{"text":"To remedy this situation, we provide a comprehensive survey to summarize existing STDM studies in ocean.","cats":{"new-dataset":0}}
{"text":"Then, typical ST ocean data quality enhancement techniques are discussed.","cats":{"new-dataset":0}}
{"text":"Next, we classify existing STDM studies for ocean into four types of tasks, i.e., prediction, event detection, pattern mining, and anomaly detection, and elaborate the techniques for these tasks.","cats":{"new-dataset":0}}
{"text":"Finally, promising research opportunities are highlighted.","cats":{"new-dataset":0}}
{"text":"This survey will help scientists from the fields of both computer science and ocean science have a better understanding of the fundamental concepts, key techniques, and open challenges of STDM in ocean.","cats":{"new-dataset":0}}
{"text":"Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet under-explored topic.","cats":{"new-dataset":0}}
{"text":"Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with hard samples.","cats":{"new-dataset":0}}
{"text":"However, when both types of data are present, most existing methods treat them equally, which results in a decline in the overall performance of the model.","cats":{"new-dataset":0}}
{"text":"Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples.","cats":{"new-dataset":0}}
{"text":"These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples.","cats":{"new-dataset":0}}
{"text":"Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the hard samples.","cats":{"new-dataset":0}}
{"text":"We study various data partitioning methods in the presence of label noise and observe that filtering out noisy samples from hard samples with this proposed metric results in the best datasets as evidenced by the high test accuracy achieved after models are trained on the filtered datasets.","cats":{"new-dataset":0}}
{"text":"We demonstrate this for both our created synthetic datasets and for datasets with real-world label noise.","cats":{"new-dataset":0}}
{"text":"Furthermore, our proposed data partitioning method significantly outperforms other methods when employed within a semi-supervised learning framework.","cats":{"new-dataset":0}}
{"text":"SlowTV contains 1.7M images from a rich diversity of environments, such as worldwide seasonal hiking, scenic driving and scuba diving.","cats":{"new-dataset":0}}
{"text":"Self-supervised monocular depth estimation (SS-MDE) has the potential to scale to vast quantities of data.","cats":{"new-dataset":0}}
{"text":"Unfortunately, existing approaches limit themselves to the automotive domain, resulting in models incapable of generalizing to complex environments such as natural or indoor settings.    ","cats":{"new-dataset":0}}
{"text":"Using this dataset, wutperforms all existing SSL approaches and closes the gap on supervised SoTA, despite using a more efficient architecture.   ","cats":{"new-dataset":0}}
{"text":"We additionally introduce a collection of best-practices to further maximize performance and zero-shot generalization.","cats":{"new-dataset":0}}
{"text":"This includes 1) aspect ratio augmentation, 2) camera intrinsic estimation, 3) support frame randomization and 4) flexible motion estimation.","cats":{"new-dataset":0}}
{"text":"Code is available at https://github.com/jspenmar/slowtv_monodepth.","cats":{"new-dataset":0}}
{"text":"As an alternative, we present CZEch~NEws~Classification~dataset (CZE-NEC), one of the largest Czech classification datasets, composed of news articles from various sources spanning over twenty years, which allows a more rigorous evaluation of such models.","cats":{"new-dataset":1}}
{"text":"Pre-trained models for Czech Natural Language Processing are often evaluated on purely linguistic tasks (POS tagging, parsing, NER) and relatively simple classification tasks such as sentiment classification or article classification from a single news source.  ","cats":{"new-dataset":0}}
{"text":"We define four classification tasks: news source, news category, inferred author's gender, and day of the week.","cats":{"new-dataset":0}}
{"text":"To verify the task difficulty, we conducted a human evaluation, which revealed that human performance lags behind strong machine-learning baselines built upon pre-trained transformer models.","cats":{"new-dataset":0}}
{"text":"Furthermore, we show that language-specific pre-trained encoder analysis outperforms selected commercially available large-scale generative language models.","cats":{"new-dataset":0}}
{"text":"To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving","cats":{"new-dataset":1}}
{"text":"SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics.","cats":{"new-dataset":1}}
{"text":"Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks.","cats":{"new-dataset":0}}
{"text":"However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. .","cats":{"new-dataset":0}}
{"text":"SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set sfactory performance, with an overall score of merely 35.80%.","cats":{"new-dataset":0}}
{"text":"Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities.","cats":{"new-dataset":0}}
{"text":"Our analysis indicates that no single prompting strategy significantly outperforms others and some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills.","cats":{"new-dataset":0}}
{"text":"We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.","cats":{"new-dataset":0}}
{"text":"Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications.","cats":{"new-dataset":0}}
{"text":"Existing federated learning works mainly focus on model homogeneous settings.","cats":{"new-dataset":0}}
{"text":"However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients.","cats":{"new-dataset":0}}
{"text":"Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex.","cats":{"new-dataset":0}}
{"text":"Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential.","cats":{"new-dataset":0}}
{"text":"In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges.","cats":{"new-dataset":0}}
{"text":"In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons.","cats":{"new-dataset":0}}
{"text":"We classify existing methods from three different levels according to the HFL procedure: data-level, model-level, and server-level.","cats":{"new-dataset":0}}
{"text":"Finally, several critical and promising future research directions in HFL are discussed, which may facilitate further developments in this field.","cats":{"new-dataset":0}}
{"text":"A periodically updated collection on HFL is available at https://github.com/marswhu/HFL_Survey.","cats":{"new-dataset":0}}
{"text":"In this paper, we analyze the data that we have collected on the pendency of 24 high courts in the Republic of India as they were made available on High Court NJDG (HC-NJDG).","cats":{"new-dataset":1}}
{"text":"We collected data on 73 days beginning August 31, 2017 to December 26, 2018, including these days.","cats":{"new-dataset":0}}
{"text":"Thus, the data collected by us spans a period of almost sixteen months.","cats":{"new-dataset":0}}
{"text":"Indian Judiciary is suffering from burden of millions of cases that are lying pending in its courts at all the levels.  ","cats":{"new-dataset":0}}
{"text":"We imited to the number of judges in each high court, the number of cases pending in each high court, d and disposed, cases filed by women and senior citizens, etc.","cats":{"new-dataset":0}}
{"text":"Our results show that: 1) statistics as important as the number of judges in high courts have serious errors on NJDG (Fig. 1, 2, 10, 11, Table V).","cats":{"new-dataset":0}}
{"text":"2) pending cases in most of the high courts are increasing rather than decreasing (Fig. 3, 13).","cats":{"new-dataset":0}}
{"text":"3) regular update of HC-NJDG is required for it to be useful.","cats":{"new-dataset":0}}
{"text":"Data related to some high courts is not being updated regularly or is updated erroneously on the portal (Fig. 14).","cats":{"new-dataset":0}}
{"text":"4) there is a huge difference in terms of average load of cases on judges of different high courts (Fig. 6).","cats":{"new-dataset":0}}
{"text":"5) if all the high courts operate at their approved strength of judges, then for most of the high courts pendency can be nullified within 20 years from now (Fig. 21, 22).","cats":{"new-dataset":0}}
{"text":"6) the pending cases filed by women and senior citizens are disproportionately low, they together constitute less than 10% of the total pending cases (Fig. 23 - 27) 7) a better scheduling process for preparing causelists in courts can help reducing the number of pending cases in the High Courts (Fig. 29).","cats":{"new-dataset":0}}
{"text":"8) some statistics are not well defined (Fig. 31).","cats":{"new-dataset":0}}
{"text":"In this work, we describe the curation of a massive speech dataset of 8740 hours consisting of $\\sim9.8$K technical lectures in the English language along with their transcripts delivered by instructors representing various parts of Indian demography.","cats":{"new-dataset":1}}
{"text":"The dataset is sourced from the very popular NPTEL MOOC platform.","cats":{"new-dataset":0}}
{"text":"Automatic speech recognition (ASR) systems are designed to transcribe spoken language into written text and find utility in a variety of applications including voice assistants and transcription services.","cats":{"new-dataset":0}}
{"text":"However, it has been observed that state-of-the-art ASR systems which deliver impressive benchmark results, struggle with speakers of certain regions or demographics due to variation in their speech properties.  ","cats":{"new-dataset":0}}
{"text":"We use the curated dataset to measure the existing disparity in YouTube Automatic Captions and OpenAI Whisper model performance across the diverse demographic traits of speakers in Indi and speech rate of speakers, disparity based on caste is non-existent.","cats":{"new-dataset":0}}
{"text":"We also observe statistically significant disparity across the disciplines of the lectures.","cats":{"new-dataset":0}}
{"text":"These results indicate the need of more inclusive and robust ASR systems and more representational datasets for disparity evaluation in them.","cats":{"new-dataset":0}}
{"text":"In this work, we present DNA-Rendering, a large-scale, high-fidelity repository of human performance data for neural actor rendering","cats":{"new-dataset":1}}
{"text":"First, our dataset contains over 1500 human subjects, 5000 motion sequences, and 67.5M frames' data volume.","cats":{"new-dataset":0}}
{"text":"Second, we provide rich assets for each subject -- 2D/3D human body keypoints, foreground masks, SMPLX models, cloth/accessory materials, multi-view images, and videos","cats":{"new-dataset":1}}
{"text":"Third, we construct a professional multi-view system to capture data, which contains 60 synchronous cameras with max 4096 x 3000 resolution, 15 fps speed, and stern camera calibration steps, ensuring high-quality resources for task training and evaluation","cats":{"new-dataset":1}}
{"text":"The dataset, code, and benchmarks will be publicly available at https://dna-rendering.github.io/","cats":{"new-dataset":1}}
{"text":"Realistic human-centric rendering plays a key role in both computer vision and computer graphics.","cats":{"new-dataset":0}}
{"text":"Rapid progress has been made in the algorithm aspect over the years, yet existing human-centric rendering datasets and benchmarks are rather impoverished in terms of diversity, which are crucial for rendering effect.","cats":{"new-dataset":0}}
{"text":"Researchers are usually constrained to explore and evaluate a small set of rendering problems on current datasets, while real-world applications require methods to be robust across different scenarios. .","cats":{"new-dataset":0}}
{"text":"DNA-Rendering presents several alluring attributes.","cats":{"new-dataset":0}}
{"text":"Second, we provide rich rials, multi-view images, and videos.","cats":{"new-dataset":0}}
{"text":"These assets boost the current method's accuracy on downstream renderid stern camera calibration steps, ensuring high-quality resources for task training and evaluation.","cats":{"new-dataset":0}}
{"text":"Along with the dataset, we provide a large-scale and quantitative benchmark in full-scale, with multiple tasks to evaluate the existing progress of net, code, and benchmarks will be publicly available at https://dna-rendering.github.io/","cats":{"new-dataset":0}}
{"text":"We investigate the use of 2D black-and-white textures for the visualization of categorical data and contribute a summary of texture attributes, and the results of three experiments that elicited design strategies as well as aesthetic and effectiveness measures.","cats":{"new-dataset":0}}
{"text":"Black-and-white textures are useful, for instance, as a visual channel for categorical data on low-color displays, in 2D/3D print, to achieve the aesthetic of historic visualizations, or to retain the color hue channel for other visual mappings.","cats":{"new-dataset":0}}
{"text":"We specifically study how to use what we call geometric and iconic textures.","cats":{"new-dataset":0}}
{"text":"Geometric textures use patterns of repeated abstract geometric shapes, while iconic textures use repeated icons that may stand for data categories.","cats":{"new-dataset":0}}
{"text":"We parameterized both types of textures and developed a tool for designers to create textures on simple charts by adjusting texture parameters.","cats":{"new-dataset":0}}
{"text":"30 visualization experts used our tool and designed 66 textured bar charts, pie charts, and maps.","cats":{"new-dataset":0}}
{"text":"We then had 150 participants rate these designs for aesthetics.","cats":{"new-dataset":0}}
{"text":"Finally, with the top-rated geometric and iconic textures, our perceptual assessment experiment with 150 participants revealed that textured charts perform about equally well as non-textured charts, and that there are some differences depending on the type of chart.","cats":{"new-dataset":0}}
{"text":"As part of the data-driven paradigm and open science movement, the data paper is becoming a popular way for researchers to publish their research data, based on academic norms that cross knowledge domains.","cats":{"new-dataset":0}}
{"text":"Data journals have also been created to host this new academic genre.","cats":{"new-dataset":0}}
{"text":"The growing number of data papers and journals has made them an important large-scale data source for understanding how research data is published and reused in our research system.","cats":{"new-dataset":0}}
{"text":"One barrier to this research agenda is a lack of knowledge as to how data journals and their publications are indexed in the scholarly databases used for quantitative analysis.","cats":{"new-dataset":0}}
{"text":"To address this gap, this study examines how a list of 18 exclusively data journals (i.e., journals that primarily accept data papers) are indexed in four popular scholarly databases: the Web of Science, Scopus, Dimensions, and OpenAlex.","cats":{"new-dataset":0}}
{"text":"We investigate how comprehensively these databases cover the selected data journals and, in particular, how they present the document type information of data papers.","cats":{"new-dataset":0}}
{"text":"We find that the coverage of data papers, as well as their document type information, is highly inconsistent across databases, which creates major challenges for future efforts to study them quantitatively.","cats":{"new-dataset":0}}
{"text":"As a result, we argue that efforts should be made by data journals and databases to improve the quality of metadata for this emerging genre.","cats":{"new-dataset":0}}
{"text":"Situated visualization has become an increasingly popular research area in the visualization community, fueled by advancements in augmented reality (AR) technology and immersive analytics.","cats":{"new-dataset":0}}
{"text":"Visualizing data in spatial proximity to their physical referents affords new design opportunities and considerations not present in traditional visualization, which researchers are now beginning to explore.","cats":{"new-dataset":0}}
{"text":"However, the AR research community has an extensive history of designing graphics that are displayed in highly physical contexts.","cats":{"new-dataset":0}}
{"text":"In this work, we leverage the richness of AR research and apply it to situated visualization.","cats":{"new-dataset":0}}
{"text":"We derive design patterns which summarize common approaches of visualizing data in situ.","cats":{"new-dataset":0}}
{"text":"The design patterns are based on a survey of 293 papers published in the AR and visualization communities, as well as our own expertise.","cats":{"new-dataset":0}}
{"text":"We discuss design dimensions that help to describe both our patterns and previous work in the literature.","cats":{"new-dataset":0}}
{"text":"This discussion is accompanied by several guidelines which explain how to apply the patterns given the constraints imposed by the real world.","cats":{"new-dataset":0}}
{"text":"We conclude by discussing future research directions that will help establish a complete understanding of the design of situated visualization, including the role of interactivity, tasks, and workflows.","cats":{"new-dataset":0}}
{"text":"Automatic diagnosis (AD), a critical application of AI in healthcare, employs machine learning techniques to assist doctors in gathering patient symptom information for precise disease diagnosis.","cats":{"new-dataset":0}}
{"text":"The Transformer-based method utilizes an input symptom sequence, predicts itself through auto-regression, and employs the hidden state of the final symptom to determine the disease.","cats":{"new-dataset":0}}
{"text":"Despite its simplicity and superior performance demonstrated, a decline in disease diagnosis accuracy is observed caused by 1) a mismatch between symptoms observed during training and generation, and 2) the effect of different symptom orders on disease prediction.","cats":{"new-dataset":0}}
{"text":"To address the above obstacles, we introduce the CoAD, a novel disease and symptom collaborative generation framework, which incorporates several key innovations to improve AD: 1) aligning sentence-level disease labels with multiple possible symptom inquiry steps to bridge the gap between training and generation; 2) expanding symptom labels for each sub-sequence of symptoms to enhance annotation and eliminate the effect of symptom order; 3) developing a repeated symptom input schema to effectively and efficiently learn the expanded disease and symptom labels.","cats":{"new-dataset":0}}
{"text":"We evaluate the CoAD framework using four datasets, including three public and one private, and demonstrate that it achieves an average 2.3% improvement over previous state-of-the-art results in automatic disease diagnosis.","cats":{"new-dataset":0}}
{"text":"For reproducibility, we release the code and data at https://github.com/KwanWaiChung/coad.","cats":{"new-dataset":0}}
{"text":"To avoid potential risks posed by vulnerabilities in third-party libraries, security researchers maintain databases containing vulnerability reports, e.g., the National Vulnerability Database (NVD).","cats":{"new-dataset":0}}
{"text":"Application developers can identify vulnerable libraries by directly querying the databases with the name of each used library.","cats":{"new-dataset":0}}
{"text":"However, the querying results of vulnerable libraries are not reliable due to the incompleteness of vulnerability reports.","cats":{"new-dataset":0}}
{"text":"Thus, current approaches model the task of identifying vulnerable libraries as an extreme multi-label learning (XML) task.","cats":{"new-dataset":0}}
{"text":"These approaches suffer from highly inaccurate results and cannot identify zero-shot libraries (i.e., those not appearing during model training).","cats":{"new-dataset":0}}
{"text":"To address these limitations, in this paper, we propose the first entity-linking approach named VulLibMiner to identify vulnerable third-party libraries from textual descriptions of vulnerabilities and libraries, together with VulLib, a Java vulnerability dataset with vulnerability-affected libraries.","cats":{"new-dataset":0}}
{"text":"VulLibMiner consists of a coarse-grained TF-IDF matcher to efficiently screen out a small set of candidate libraries and a fine-grained BERT-FNN model to identify vulnerable libraries from these candidates effectively.","cats":{"new-dataset":0}}
{"text":"We evaluate VulLibMiner using two state-of-the-art/practice approaches of library identification (FastXML, LightXML) on both their dataset named VeraJava and our VulLib dataset.","cats":{"new-dataset":0}}
{"text":"Our evaluation results show that VulLibMiner can effectively identify vulnerable libraries with an average F1 score of 0.542 while the state-of-the-art/practice approaches achieve only 0.377.","cats":{"new-dataset":0}}
{"text":"We demonstrate VulLibMiner's high value of security practice by using VulLibMiner to identify 12,716 <vulnerability, library> pairs, and 7,936 of them do not appear in NVD.","cats":{"new-dataset":0}}
{"text":"We specify a file-oriented data format suitable for parallel, partition-independent disk I/O. Here, a partition refers to a disjoint and ordered distribution of the data elements between one or more processes.","cats":{"new-dataset":0}}
{"text":"The format is designed such that the file contents are invariant under linear (i. e., unpermuted), parallel repartition of the data prior to writing.","cats":{"new-dataset":0}}
{"text":"The file contents are indistinguishable from writing in serial.","cats":{"new-dataset":0}}
{"text":"In the same vein, the file can be read on any number of processes that agree on any partition of the number of elements stored.   ","cats":{"new-dataset":0}}
{"text":"In addition to the format specification we propose an optional convention to implement transparent per-element data compression.","cats":{"new-dataset":0}}
{"text":"The compressed data and metadata is layered inside ordinary format elements.","cats":{"new-dataset":0}}
{"text":"Overall, we pay special attention to both human and machine readability.","cats":{"new-dataset":0}}
{"text":"If pure ASCII data is written, or compressed data is reencoded to ASCII, the entire file including its header and sectioning metadata remains entirely in ASCII.","cats":{"new-dataset":0}}
{"text":"If binary data is written, the metadata stays easy on the human eye.   ","cats":{"new-dataset":0}}
{"text":"We refer to this format as scda.","cats":{"new-dataset":0}}
{"text":"Conceptually, it lies one layer below and is oblivious to the definition of variables, the binary representation of numbers, considerations of endianness, and self-describing headers, which may all be specified on top of scda.","cats":{"new-dataset":0}}
{"text":"The main purpose of the format is to abstract any parallelism and provide sufficient structure as a foundation for a generic and flexible archival and checkpoint/restart.","cats":{"new-dataset":0}}
{"text":"A documented reference implementation is available as part of the general-purpose libsc free software library.","cats":{"new-dataset":0}}
{"text":"This paper explores the integration of Large Language Models (LLMs) into Automatic Speech Recognition (ASR) systems to improve transcription accuracy.","cats":{"new-dataset":0}}
{"text":"The increasing sophistication of LLMs, with their in-context learning capabilities and instruction-following behavior, has drawn significant attention in the field of Natural Language Processing (NLP).","cats":{"new-dataset":0}}
{"text":"Our primary focus is to investigate the potential of using an LLM's in-context learning capabilities to enhance the performance of ASR systems, which currently face challenges such as ambient noise, speaker accents, and complex linguistic contexts.","cats":{"new-dataset":0}}
{"text":"We designed a study using the Aishell-1 and LibriSpeech datasets, with ChatGPT and GPT-4 serving as benchmarks for LLM capabilities.","cats":{"new-dataset":0}}
{"text":"Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM's in-context learning for ASR applications.","cats":{"new-dataset":0}}
{"text":"Despite further exploration with varied settings and models, the corrected sentences from the LLMs frequently resulted in higher Word Error Rates (WER), demonstrating the limitations of LLMs in speech applications.","cats":{"new-dataset":0}}
{"text":"This paper provides a detailed overview of these experiments, their results, and implications, establishing that using LLMs' in-context learning capabilities to correct potential errors in speech recognition transcriptions is still a challenging task at the current stage.","cats":{"new-dataset":0}}
{"text":"The application offers functionalities for data loading, management, summarization, basic graphs, advanced analysis, and contact.","cats":{"new-dataset":0}}
{"text":"Additionally, the application offers statistical tools such as time series analysis using ARIMA and SARIMA models, forecasting, and Ljung-Box statistic.","cats":{"new-dataset":0}}
{"text":"Its user-friendly interface empowers individuals from various domains, including beginners in statistics, to make informed decisions.","cats":{"new-dataset":0}}
{"text":"Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.","cats":{"new-dataset":1}}
{"text":"Captions are crucial for understanding scientific visualizations and documents.","cats":{"new-dataset":0}}
{"text":"Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences.","cats":{"new-dataset":0}}
{"text":"To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences.","cats":{"new-dataset":0}}
{"text":"Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences.","cats":{"new-dataset":0}}
{"text":"We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models.","cats":{"new-dataset":0}}
{"text":"In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively.","cats":{"new-dataset":0}}
{"text":"In this paper, we introduce RetouchingFFHQ, a large-scale and fine-grained face retouching dataset that contains over half a million conditionally-retouched images.","cats":{"new-dataset":1}}
{"text":"RetouchingFFHQ stands out from previous datasets due to its large scale, high quality, fine-grainedness, and customization","cats":{"new-dataset":1}}
{"text":"With the proposed new dataset, we believe there is great potential for future work to tackle the challenging problem of real-world fine-grained face retouching detection.","cats":{"new-dataset":0}}
{"text":"The widespread use of face retouching filters on short-video platforms has raised concerns about the authenticity of digital appearances and the impact of deceptive advertising.","cats":{"new-dataset":0}}
{"text":"To address these issues, there is a pressing need to develop advanced face retouching techniques.","cats":{"new-dataset":0}}
{"text":"However, the lack of large-scale and fine-grained face retouching datasets has been a major obstacle to progress in this field.  ","cats":{"new-dataset":0}}
{"text":"RetouchingFFHQ stands out from previous datasets due to its large scale, high quality, fine-grainedness, and customization.","cats":{"new-dataset":0}}
{"text":"By including four typical types of face  multi-retouching type, and multi-retouching level estimation problem.","cats":{"new-dataset":0}}
{"text":"Additionally, we propose a Multi-granularity Attention Module (MAM) as a plugin for CNN backbones for enhanced cross-scale representation learning.","cats":{"new-dataset":0}}
{"text":"Extensive experiments using different baselines as well as our proposed method on RetouchingFFHQ show decent performance on face retouching detection.","cats":{"new-dataset":0}}
{"text":"In this study, we introduce an event-based dataset on fine-grained manipulation actions and perform an experimental study on the use of transformers for action prediction with events","cats":{"new-dataset":1}}
{"text":"Finally, we release the new event dataset, which is the first in the literature for manipulation action recognition.","cats":{"new-dataset":0}}
{"text":"Neuromorphic visual sensors are artificial retinas that output sequences of asynchronous events when brightness changes occur in the scene.","cats":{"new-dataset":0}}
{"text":"These sensors offer many advantages including very high temporal resolution, no motion blur and smart data compression ideal for real-time processing. .","cats":{"new-dataset":0}}
{"text":"There is enormous interest in the fields of cognitive robotics and human-robot interaction on understanding and predicting human actions as early as possible.","cats":{"new-dataset":0}}
{"text":"Early prediction allows anticipating complex stages for planning, enabling effective and real-time interaction.","cats":{"new-dataset":0}}
{"text":"Our Transformer network uses events to predict manipulation actions as they occur, using online inference.","cats":{"new-dataset":0}}
{"text":"The model succeeds at predicting actions early on, building up confidence over time and achieving state-of-the-art classification.","cats":{"new-dataset":0}}
{"text":"Moreover, the attention-based transformer architecture allows us to study the role of the spatio-temporal patterns selected by the model.","cats":{"new-dataset":0}}
{"text":"Our experiments show that the Transformer network captures action dynamic features outperforming video-based approaches and succeeding with scenarios where the differences between actions lie in very subtle cues.","cats":{"new-dataset":0}}
{"text":"Code will be available at https://github.com/DaniDeniz/EventVisio","cats":{"new-dataset":0}}
{"text":"This dataset encompasses three image collections in which rodent neuronal cells' nuclei and cytoplasm are stained with diverse markers to highlight their anatomical or functional characteristics","cats":{"new-dataset":1}}
{"text":"Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting","cats":{"new-dataset":1}}
{"text":"First, given the variety of annotations and their accessible formats, we envision our work facilitating methodological advancements in computer vision approaches for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning, and related areas","cats":{"new-dataset":1}}
{"text":"The data are available at: https://amsacta.unibo.it/id/eprint/7347","cats":{"new-dataset":1}}
{"text":"Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy images and the corresponding ground-truth annotations, designed to foster innovative research in the domains of Life Sciences and Deep Learning. .","cats":{"new-dataset":0}}
{"text":"Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting.","cats":{"new-dataset":0,"data-quality":0}}
{"text":"The contribution is two-fold.","cats":{"new-dataset":0}}
{"text":"First, given for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning, and related areas.","cats":{"new-dataset":0}}
{"text":"Second, by enabling extensive exploration and benchma","cats":{"new-dataset":0}}
{"text":"Traditional Chinese Painting (TCP) is an invaluable cultural heritage resource and a unique visual art style.","cats":{"new-dataset":0}}
{"text":"In recent years, increasing interest has been placed on digitalizing TCPs to preserve and revive the culture.","cats":{"new-dataset":0}}
{"text":"The resulting digital copies have enabled the advancement of computational methods for structured and systematic understanding of TCPs.","cats":{"new-dataset":0}}
{"text":"To explore this topic, we conducted an in-depth analysis of 92 pieces of literature.","cats":{"new-dataset":0}}
{"text":"We examined the current use of computer technologies on TCPs from three perspectives, based on numerous conversations with specialists.","cats":{"new-dataset":0}}
{"text":"First, in light of the \"Six Principles of Painting\" theory, we categorized the articles according to their research focus on artistic elements.","cats":{"new-dataset":0}}
{"text":"Second, we created a four-stage framework to illustrate the purposes of TCP applications.","cats":{"new-dataset":0}}
{"text":"Third, we summarized the popular computational techniques applied to TCPs.","cats":{"new-dataset":0}}
{"text":"The framework also provides insights into potential applications and future prospects, with professional opinion.","cats":{"new-dataset":0}}
{"text":"The list of surveyed publications and related information is available online at https://ca4tcp.com.","cats":{"new-dataset":0}}
{"text":"Using the datasets built from in-the-wild images, our method gradually presents a controllable appearance editing function","cats":{"new-dataset":1}}
{"text":"We will release the dataset and code on https://lty2226262.github.io/car-studio/ to facilitate further research in the field.","cats":{"new-dataset":1}}
{"text":"Compositional neural scene graph studies have shown that radiance fields can be an efficient tool in an editable autonomous driving simulator.","cats":{"new-dataset":0}}
{"text":"However, previous studies learned within a sequence of autonomous driving datasets, resulting in unsatisfactory blurring when rotating the car in the simulator.","cats":{"new-dataset":0}}
{"text":"In this letter, we propose a pipeline for learning unconstrained images and building a dataset from processed images.","cats":{"new-dataset":0}}
{"text":"To meet the requirements of the simulator, which demands that the vehicle maintain clarity when the perspective changes and that the contour remains sharp from the background to avoid artifacts when editing, we design a radiation field of the vehicle, a crucial part of the urban scene foreground.","cats":{"new-dataset":0}}
{"text":"Through experiments, we demonstrate that our model achieves competitive performance compared to baselines. .","cats":{"new-dataset":0}}
{"text":"We will release the dataset and code on https://lty2226262.github.io/car-studio/ to facilitate further research in the fie","cats":{"new-dataset":0}}
{"text":"To solve this task, we build a heterogeneous multi-agent tidying-up benchmark dataset in a large number of houses with multiple rooms based on ProcTHOR-10K.","cats":{"new-dataset":1}}
{"text":"Multi-agent embodied tasks have recently been studied in complex indoor visual environments.","cats":{"new-dataset":0}}
{"text":"Collaboration among multiple agents can improve work efficiency and has significant practical value.","cats":{"new-dataset":0}}
{"text":"However, most of the existing research focuses on homogeneous multi-agent tasks.","cats":{"new-dataset":0}}
{"text":"Compared with homogeneous agents, heterogeneous agents can leverage their different capabilities to allocate corresponding sub-tasks and cooperate to complete complex tasks.","cats":{"new-dataset":0}}
{"text":"Heterogeneous multi-agent tasks are common in real-world scenarios, and the collaboration strategy among heterogeneous agents is a challenging and important problem to be solved.","cats":{"new-dataset":0}}
{"text":"To study collaboration among heterogeneous agents, we propose the heterogeneous multi-agent tidying-up task, in which multiple heterogeneous agents with different capabilities collaborate with each other to detect misplaced objects and place them in reasonable locations.","cats":{"new-dataset":0}}
{"text":"This is a demanding task since it requires agents to make the best use of their different capabilities to conduct reasonable task planning and complete the whole task.  ","cats":{"new-dataset":0}}
{"text":"We propose the hierarchical decision model based on misplaced object detection, reasonable receptacle prediction, as well as the handshake-based group communication mechanism.","cats":{"new-dataset":0}}
{"text":"Extensive experiments are conducted to demonstrate the effectiveness of the proposed model.","cats":{"new-dataset":0}}
{"text":"The project's website and videos of experiments can be found at https://hetercol.github.io/.","cats":{"new-dataset":0}}
{"text":"The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking.","cats":{"new-dataset":0}}
{"text":"While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets.","cats":{"new-dataset":0}}
{"text":"To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets.","cats":{"new-dataset":0}}
{"text":"At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data.","cats":{"new-dataset":0}}
{"text":"As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights.","cats":{"new-dataset":0}}
{"text":"trajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata","cats":{"new-dataset":0}}
{"text":"The core recipe of GrammarGPT is to leverage the hybrid dataset of ChatGPT-generated and human-annotated.","cats":{"new-dataset":1}}
{"text":"Grammatical error correction aims to correct ungrammatical sentences automatically.","cats":{"new-dataset":0}}
{"text":"Recently, some work has demonstrated the excellent capabilities of closed-source Large Language Models (LLMs, e.g., ChatGPT) in grammatical error correction.","cats":{"new-dataset":0}}
{"text":"However, the potential of open-source LLMs remains unexplored.","cats":{"new-dataset":0}}
{"text":"In this paper, we introduced GrammarGPT, an open-source LLM, to preliminary explore its potential for native Chinese grammatical error correction.  ","cats":{"new-dataset":0}}
{"text":"For grammatical errors with clues, we proposed a heuristic method to guide ChatGPT to generate ungrammatical sentences by providing those clues.","cats":{"new-dataset":0}}
{"text":"For grammatical errors without clues, we collected ungrammatical sentences from publicly available websites and manually corrected them.","cats":{"new-dataset":0}}
{"text":"In addition, we employed an error-invariant augmentation method to enhance the ability of the model to correct native Chinese grammatical errors.","cats":{"new-dataset":0}}
{"text":"We ultimately constructed about 1k parallel data and utilized these data to fine-tune open-source LLMs (e.g., Phoenix, released by The Chinese University of Hong Kong, Shenzhen) with instruction tuning.","cats":{"new-dataset":0}}
{"text":"The experimental results show that GrammarGPT outperforms the existing SOTA system significantly.","cats":{"new-dataset":0}}
{"text":"Although model parameters are 20x larger than the SOTA baseline, the required amount of data for instruction tuning is 1200x smaller, illustrating the potential of open-source LLMs on native CGEC.","cats":{"new-dataset":0}}
{"text":"Our GrammarGPT ranks $3^{rd}$ on NLPCC2023 SharedTask1, demonstrating our approach's effectiveness.","cats":{"new-dataset":0}}
{"text":"The code and data are available at \\url{https://github.com/FreedomIntelligence/GrammarGPT}.","cats":{"new-dataset":0}}
{"text":"Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks.","cats":{"new-dataset":0}}
{"text":"However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions.","cats":{"new-dataset":0}}
{"text":"Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs.","cats":{"new-dataset":0}}
{"text":"In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations.","cats":{"new-dataset":0}}
{"text":"Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models.","cats":{"new-dataset":0}}
{"text":"To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations.","cats":{"new-dataset":0}}
{"text":"To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery.","cats":{"new-dataset":0}}
{"text":"Empirical evaluations on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines.","cats":{"new-dataset":0}}
{"text":"We present YOLOBench, a benchmark comprised of 550+ YOLO-based object detection models on 4 different datasets and 4 different embedded hardware platforms (x86 CPU, ARM CPU, Nvidia GPU, NPU).","cats":{"new-dataset":0}}
{"text":"We collect accuracy and latency numbers for a variety of YOLO-based one-stage detectors at different model scales by performing a fair, controlled comparison of these detectors with a fixed training environment (code and training hyperparameters).","cats":{"new-dataset":0}}
{"text":"Pareto-optimality analysis of the collected data reveals that, if modern detection heads and training techniques are incorporated into the learning process, multiple architectures of the YOLO series achieve a good accuracy-latency trade-off, including older models like YOLOv3 and YOLOv4.","cats":{"new-dataset":0}}
{"text":"We also evaluate training-free accuracy estimators used in neural architecture search on YOLOBench and demonstrate that, while most state-of-the-art zero-cost accuracy estimators are outperformed by a simple baseline like MAC count, some of them can be effectively used to predict Pareto-optimal detection models.","cats":{"new-dataset":0}}
{"text":"We showcase that by using a zero-cost proxy to identify a YOLO architecture competitive against a state-of-the-art YOLOv8 model on a Raspberry Pi 4 CPU.","cats":{"new-dataset":0}}
{"text":"The code and data are available at https://github.com/Deeplite/deeplite-torch-zoo","cats":{"new-dataset":0}}
{"text":"Advancements in large pre-trained generative models have expanded their potential as effective data generators in visual recognition.","cats":{"new-dataset":0}}
{"text":"This work delves into the impact of generative images, primarily comparing paradigms that harness external data (\\ie generative \\vs retrieval \\vs original).   ","cats":{"new-dataset":0}}
{"text":"Our key contributions are: \\textbf{1) GenBench Construction:}","cats":{"new-dataset":0}}
{"text":"We devise \\textbf{GenBench}, a broad benchmark comprising 22 datasets with 2548 categories, to appraise generative data across various visual recognition tasks.","cats":{"new-dataset":0}}
{"text":"\\textbf{2) CLER Score:} To address the insufficient correlation of existing metrics (\\eg, FID, CLIP score) with downstream recognition performance, we propose \\textbf{CLER}, a training-free metric indicating generative data's efficiency for recognition tasks prior to training.","cats":{"new-dataset":0}}
{"text":"\\textbf{3) New Baselines:} Comparisons of generative data with retrieved data from the same external pool help to elucidate the unique traits of generative data.","cats":{"new-dataset":0}}
{"text":"\\textbf{4)","cats":{"new-dataset":0}}
{"text":"Our exhaustive benchmark and analysis spotlight generative data's promise in visual recognition, while identifying key challenges for future investigation.","cats":{"new-dataset":0}}
{"text":"We introduce this large-scale synthesised dataset of 250K photorealistic images and corresponding 3DMM parameters.","cats":{"new-dataset":1}}
{"text":"Accurate 3D face shape estimation is an enabling technology with applications in healthcare, security, and creative industries, yet current state-of-the-art methods either rely on self-supervised training with 2D image data or supervised training with very limited 3D data.","cats":{"new-dataset":0}}
{"text":"To bridge this gap, we present a novel approach which uses a conditioned stable diffusion model for face image generation, leveraging the abundance of 2D facial information to inform 3D space.","cats":{"new-dataset":0}}
{"text":"By conditioning stable diffusion on depth maps sampled from a 3D Morphable Model (3DMM) of the human face, we generate diverse and shape-consistent images, forming the basis of SynthFace.  ","cats":{"new-dataset":0}}
{"text":"We further propose ControlFace, a deep neural network, trained on SynthFace, which achieves competitive performance on the NoW benchmark, without requiring 3D supervision or manual 3D asset creation.","cats":{"new-dataset":0}}
{"text":"To this aim we focused only on images generated by the famous DALL-E 2 and collected images (both original and generated) of five renowned artists.","cats":{"new-dataset":1}}
{"text":"Dataset and code are available at: https://github.com/ictlab-unict/not-with-my-name .","cats":{"new-dataset":0}}
{"text":"Diffusion Models (DM) are highly effective at generating realistic, high-quality images.","cats":{"new-dataset":0}}
{"text":"However, these models lack creativity and merely compose outputs based on their training data, guided by a textual input provided at creation time.","cats":{"new-dataset":0}}
{"text":"Is it acceptable to generate images reminiscent of an artist, employing his name as input?","cats":{"new-dataset":0}}
{"text":"This imply that if the DM is able to replicate an artist's work then it was trained on some or all of his artworks thus violating copyright.","cats":{"new-dataset":0}}
{"text":"In this paper, a preliminary study to infer the probability of use of an artist's name in the input string of a generated image is presented.  ","cats":{"new-dataset":0}}
{"text":"Finally, a dedicated Siamese Neural Network was employed to have a first kind of probability.","cats":{"new-dataset":0}}
{"text":"Experimental results demonstrate that our approach is an optimal starting point and can be employed as a prior for predicting a complete input string of an investigated image.","cats":{"new-dataset":0}}
{"text":"The consumption of podcast media has been increasing rapidly.","cats":{"new-dataset":0}}
{"text":"Due to the lengthy nature of podcast episodes, users often carefully select which ones to listen to.","cats":{"new-dataset":0}}
{"text":"Although episode descriptions aid users by providing a summary of the entire podcast, they do not provide a topic-by-topic breakdown.","cats":{"new-dataset":0}}
{"text":"This study explores the combined application of topic segmentation and text summarisation methods to investigate how podcast episode comprehension can be improved.","cats":{"new-dataset":0}}
{"text":"We have sampled 10 episodes from Spotify's English-Language Podcast Dataset and employed TextTiling and TextSplit to segment them.","cats":{"new-dataset":0}}
{"text":"Moreover, three text summarisation models, namely T5, BART, and Pegasus, were applied to provide a very short title for each segment.","cats":{"new-dataset":0}}
{"text":"The segmentation part was evaluated using our annotated sample with the $P_k$ and WindowDiff ($WD$) metrics.","cats":{"new-dataset":0}}
{"text":"A survey was also rolled out ($N=25$) to assess the quality of the generated summaries.","cats":{"new-dataset":0}}
{"text":"The TextSplit algorithm achieved the lowest mean for both evaluation metrics ($\\bar{P_k}=0.41$ and $\\bar{WD}=0.41$), while the T5 model produced the best summaries, achieving a relevancy score only $8\\%$ less to the one achieved by the human-written titles.","cats":{"new-dataset":0}}
{"text":"Our team gathered a dataset of 19,779 accounts that meet standardized criteria to enable future research on bots in open-source projects.","cats":{"new-dataset":1}}
{"text":"Social coding platforms have revolutionized collaboration in software development, leading to using software bots for streamlining operations.","cats":{"new-dataset":0}}
{"text":"However, The presence of open-source software (OSS) bots gives rise to problems including impersonation, spamming, bias, and security risks.","cats":{"new-dataset":0}}
{"text":"Identifying bot accounts and behavior is a challenging task in the OSS project.","cats":{"new-dataset":0}}
{"text":"This research aims to investigate bots' behavior in open-source software projects and identify bot accounts with maximum possible accuracy.  ","cats":{"new-dataset":0}}
{"text":"We follow a rigorous workflow to ensure that the data we collect is accurate, generalizable, scalable, and up-to-date.","cats":{"new-dataset":0}}
{"text":"We've identified four types of bot accounts in open-source software projects by analyzing their behavior across 17 features in 5 dimensions.","cats":{"new-dataset":0}}
{"text":"Our team created BotHawk, a highly effective model for detecting bots in open-source software projects.","cats":{"new-dataset":0}}
{"text":"It outperforms other models, achieving an AUC of 0.947 and an F1-score of 0.89.","cats":{"new-dataset":0}}
{"text":"BotHawk can detect a wider variety of bots, including CI/CD and scanning bots.","cats":{"new-dataset":0}}
{"text":"Furthermore, we find that the number of followers, number of repositories, and tags contain the most relevant features to identify the account type.","cats":{"new-dataset":0}}
{"text":"We curate a large-scale dataset to enable pre-training of 3D medical radiology images (MRI and CT).","cats":{"new-dataset":1}}
{"text":"Harnessing the power of pre-training on large-scale datasets like ImageNet forms a fundamental building block for the progress of representation learning-driven solutions in computer vision.","cats":{"new-dataset":0}}
{"text":"Medical images are inherently different from natural images as they are acquired in the form of many modalities (CT, MR, PET, Ultrasound etc.) and contain granulated information like tissue, lesion, organs etc.","cats":{"new-dataset":0}}
{"text":"These characteristics of medical images require special attention towards learning features representative of local context.","cats":{"new-dataset":0}}
{"text":"In this work, we focus on designing an effective pre-training framework for 3D radiology images.","cats":{"new-dataset":0}}
{"text":"First, we propose a new masking strategy called local masking where the masking is performed across channel embeddings instead of tokens to improve the learning of local feature representations.","cats":{"new-dataset":0}}
{"text":"We combine this with classical low-level perturbations like adding noise and downsampling to further enable low-level representation learning.","cats":{"new-dataset":0}}
{"text":"To this end, we introduce Disruptive Autoencoders, a pre-training framework that attempts to reconstruct the original image from disruptions created by a combination of local masking and low-level perturbations.","cats":{"new-dataset":0}}
{"text":"Additionally, we also devise a cross-modal contrastive loss (CMCL) to accommodate the pre-training of multiple modalities in a single framework.  ","cats":{"new-dataset":0}}
{"text":"The proposed pre-training framework is tested across multiple downstream tasks and achieves state-of-the-art performance.","cats":{"new-dataset":0}}
{"text":"Notably, our proposed method tops the public test leaderboard of BTCV multi-organ segmentation challenge.","cats":{"new-dataset":0}}
{"text":"In this paper, we introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset)","cats":{"new-dataset":1}}
{"text":"Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop the English subset of MIRACL, a publicly available information retrieval dataset.","cats":{"new-dataset":1}}
{"text":"The rise of large language models (LLMs) had a transformative impact on search, ushering in a new era of search engines that are capable of generating search results in natural language text, imbued with citations for supporting sources.","cats":{"new-dataset":0}}
{"text":"Building generative information-seeking models demands openly accessible datasets, which currently remain lacking.  for building end-to-end generative information-seeking models that are capable of retrieving candidate quotes and generating attributed explanations.","cats":{"new-dataset":0}}
{"text":"Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop the English subontext citation style using an LLM, i.e. GPT-3.5.","cats":{"new-dataset":0}}
{"text":"Next, we ask human annotators to evaluate the LLM explanations based on two criteria: informativeness and attributability.","cats":{"new-dataset":0}}
{"text":"HAGRID serves as a catalyst for the development of information-seeking models with better attribution capabilities.","cats":{"new-dataset":0}}
{"text":"Retriever-augmented instruction-following models are attractive alternatives to fine-tuned approaches for information-seeking tasks such as question answering (QA).","cats":{"new-dataset":0}}
{"text":"By simply prepending retrieved documents in its input along with an instruction, these models can be adapted to various information domains and tasks without additional fine-tuning.","cats":{"new-dataset":0}}
{"text":"While the model responses tend to be natural and fluent, the additional verbosity makes traditional QA evaluation metrics such as exact match (EM) and F1 unreliable for accurately quantifying model performance.   ","cats":{"new-dataset":0}}
{"text":"In this work, we investigate the performance of instruction-following models across three information-seeking QA tasks.","cats":{"new-dataset":0}}
{"text":"We use both automatic and human evaluation to evaluate these models along two dimensions: 1) how well they satisfy the user's information need (correctness), and 2) whether they produce a response based on the provided knowledge (faithfulness).","cats":{"new-dataset":0}}
{"text":"Guided by human evaluation and analysis, we highlight the shortcomings of traditional metrics for both correctness and faithfulness.","cats":{"new-dataset":0}}
{"text":"We then propose simple token-overlap based and model-based metrics that reflect the true performance of these models.","cats":{"new-dataset":0}}
{"text":"Our analysis reveals that instruction-following models are competitive, and sometimes even outperform fine-tuned models for correctness.","cats":{"new-dataset":0}}
{"text":"However, these models struggle to stick to the provided knowledge and often hallucinate in their responses.","cats":{"new-dataset":0}}
{"text":"We hope our work encourages a more holistic evaluation of instruction-following models for QA.","cats":{"new-dataset":0}}
{"text":"Our code and data is available at https://github.com/McGill-NLP/instruct-qa","cats":{"new-dataset":0}}
{"text":"Our dataset comprises traffic at a real environment for more than 1 year.","cats":{"new-dataset":1}}
{"text":"Indicators of Compromise (IOCs), such as IP addresses, file hashes, and domain names associated with known malware or attacks, are cornerstones of cybersecurity, serving to identify malicious activity on a network.","cats":{"new-dataset":0}}
{"text":"In this work, we leverage real data to compare different parameterizations of IOC aging models.  ","cats":{"new-dataset":0}}
{"text":"Among our trace-driven findings, we determine thresholds for the ratio between miss over monitoring costs such that the system benefits from storing IOCs for a finite time-to-live (TTL) before eviction.","cats":{"new-dataset":0}}
{"text":"To the best of our knowledge, this is the first real world evaluation of thresholds related to IOC aging, paving the way towards realistic IOC decaying models.","cats":{"new-dataset":0}}
{"text":"As people's daily life becomes increasingly inseparable from various mobile electronic devices, relevant service application platforms and network operators can collect numerous individual information easily.","cats":{"new-dataset":0}}
{"text":"When releasing these data for scientific research or commercial purposes, users' privacy will be in danger, especially in the publication of spatiotemporal trajectory datasets.","cats":{"new-dataset":0}}
{"text":"Therefore, to avoid the leakage of users' privacy, it is necessary to anonymize the data before they are released.","cats":{"new-dataset":0}}
{"text":"However, more than simply removing the unique identifiers of individuals is needed to protect the trajectory privacy, because some attackers may infer the identity of users by the connection with other databases.","cats":{"new-dataset":0}}
{"text":"Much work has been devoted to merging multiple trajectories to avoid re-identification, but these solutions always require sacrificing data quality to achieve the anonymity requirement.","cats":{"new-dataset":0}}
{"text":"In order to provide sufficient privacy protection for users' trajectory datasets, this paper develops a study on trajectory privacy against re-identification attacks, proposing a trajectory K-anonymity model based on Point Density and Partition (KPDP).","cats":{"new-dataset":0}}
{"text":"Our approach improves the existing trajectory generalization anonymization techniques regarding trajectory set partition preprocessing and trajectory clustering algorithms.","cats":{"new-dataset":0}}
{"text":"It successfully resists re-identification attacks and reduces the data utility loss of the k-anonymized dataset.","cats":{"new-dataset":0}}
{"text":"A series of experiments on a real-world dataset show that the proposed model has significant advantages in terms of higher data utility and shorter algorithm execution time than other existing techniques.","cats":{"new-dataset":0}}
{"text":"We first present ToolBench, an instruction-tuning dataset for tool use, which is created automatically using ChatGPT.","cats":{"new-dataset":1}}
{"text":"Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub","cats":{"new-dataset":1}}
{"text":"Despite the advancements of open-source large language models (LLMs) and their variants, e.g., LLaMA and Vicuna, they remain significantly limited in performing higher-level tasks, such as following human instructions to use external tools (APIs).","cats":{"new-dataset":0}}
{"text":"This is because current instruction tuning largely focuses on basic language tasks instead of the tool-use domain.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"This is in contrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have demonstrated excellent tool-use capabilities but are unfortunately closed source.","cats":{"new-dataset":0}}
{"text":"To facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework of data construction, model training and evaluation.  ","cats":{"new-dataset":0}}
{"text":"Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub, then prompt ChatGPTi-tool scenarios.","cats":{"new-dataset":0}}
{"text":"Finally, we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction.","cats":{"new-dataset":0}}
{"text":"To make the searching process more efficient, we develop a novel depth-first search-based decision tree (DFSDT), enabling LLMs to evaluate multiple reasoning traces and expand the search space.","cats":{"new-dataset":0}}
{"text":"We show that DFSDT significantly enhances the planning and reasoning capabilities of LLMs.","cats":{"new-dataset":0}}
{"text":"For efficient tool-use assessment, we develop an automatic evaluator: ToolEval.","cats":{"new-dataset":0}}
{"text":"We fine-tune LLaMA on ToolBench and obtain ToolLLaMA.","cats":{"new-dataset":0}}
{"text":"Our ToolEval reveals that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT.","cats":{"new-dataset":0}}
{"text":"To make the pipeline more practical, we devise a neural API retriever to recommend appropriate APIs for each instruction, negating the need for manual API selection.","cats":{"new-dataset":0}}
{"text":"This paper accompanies a new dataset of non-linear real arithmetic problems for the SMT-LIB benchmark collection.","cats":{"new-dataset":0}}
{"text":"The problems come from an automated proof procedure of Gerhold--Kauers, which is well suited for solution by SMT.","cats":{"new-dataset":0}}
{"text":"The problems of this type have not been tackled by SMT-solvers before.","cats":{"new-dataset":0}}
{"text":"We describe the proof technique and give one new such proof to illustrate it.","cats":{"new-dataset":0}}
{"text":"The benchmarks on the new dataset are quite different to the existing ones.","cats":{"new-dataset":0}}
{"text":"The benchmarking also brings forward some interesting debate on the use/inclusion of rational functions and algebraic numbers in the SMT-LIB.","cats":{"new-dataset":0}}
{"text":"Moreover, we release a challenging dataset named ChaMS, comprising both real-world and synthetic sets with significant parallax, providing a new option for comprehensive evaluation.","cats":{"new-dataset":1}}
{"text":"Multi-spectral image stitching leverages the complementarity between infrared and visible images to generate a robust and reliable wide field-of-view (FOV) scene.","cats":{"new-dataset":0}}
{"text":"The primary challenge of this task is to explore the relations between multi-spectral images for aligning and integrating multi-view scenes.","cats":{"new-dataset":0}}
{"text":"Capitalizing on the strengths of Graph Convolutional Networks (GCNs) in modeling feature relationships, we propose a spatial graph reasoning based multi-spectral image stitching method that effectively distills the deformation and integration of multi-spectral images across different viewpoints.","cats":{"new-dataset":0}}
{"text":"To accomplish this, we embed multi-scale complementary features from the same view position into a set of nodes.","cats":{"new-dataset":0}}
{"text":"The correspondence across different views is learned through powerful dense feature embeddings, where both inter- and intra-correlations are developed to exploit cross-view matching and enhance inner feature disparity.","cats":{"new-dataset":0}}
{"text":"By introducing long-range coherence along spatial and channel dimensions, the complementarity of pixel relations and channel interdependencies aids in the reconstruction of aligned multi-view features, generating informative and reliable wide FOV scenes.  ","cats":{"new-dataset":0}}
{"text":"Extensive experiments demonstrate that our method surpasses the state-of-the-arts.","cats":{"new-dataset":0}}
{"text":"Here, we collect and publicly release Repair-QA, the first large dataset of TPRs in a conversational question answering (QA) setting.","cats":{"new-dataset":1}}
{"text":"The data is comprised of the TPR turns, corresponding dialogue contexts, and candidate repairs of the original turn for execution of TPRs.","cats":{"new-dataset":1}}
{"text":"The ability to handle miscommunication is crucial to robust and faithful conversational AI.","cats":{"new-dataset":0}}
{"text":"People usually deal with miscommunication immediately as they detect it, using highly systematic interactional mechanisms called repair.","cats":{"new-dataset":0}}
{"text":"One important type of repair is Third Position Repair (TPR) whereby a speaker is initially misunderstood but then corrects the misunderstanding as it becomes apparent after the addressee's erroneous response.  ","cats":{"new-dataset":0}}
{"text":"The data is comprised of the TPR turns, corresponding dialogue contexts, and candidate repairs of the original turn for execution of execution, we perform both automatic and human evaluations on a fine-tuned T5 model, as well as OpenAI's GPT-3 LLMs.","cats":{"new-dataset":0}}
{"text":"Additionally, we extrinsically evaluate the LLMs' TPR processing capabilities in the downstream conversational QA task.","cats":{"new-dataset":0}}
{"text":"The results indicate poor out-of-the-box performance on TPR's by the GPT-3 models, which then significantly improves when exposed to Repair-QA.","cats":{"new-dataset":0}}
{"text":"The datasets, competition toolkit, workshop recordings, and source code from the winning teams are publicly available on the challenge website.","cats":{"new-dataset":1}}
{"text":"Accurate depth estimation under out-of-distribution (OoD) scenarios, such as adverse weather conditions, sensor failure, and noise contamination, is desirable for safety-critical applications.","cats":{"new-dataset":0}}
{"text":"Existing depth estimation systems, however, suffer inevitably from real-world corruptions and perturbations and are struggled to provide reliable depth predictions under such cases.","cats":{"new-dataset":0}}
{"text":"In this paper, we summarize the winning solutions from the RoboDepth Challenge -- an academic competition designed to facilitate and advance robust OoD depth estimation.","cats":{"new-dataset":0}}
{"text":"This challenge was developed based on the newly established KITTI-C and NYUDepth2-C benchmarks.","cats":{"new-dataset":0}}
{"text":"We hosted two stand-alone tracks, with an emphasis on robust self-supervised and robust fully-supervised depth estimation, respectively.","cats":{"new-dataset":0}}
{"text":"Out of more than two hundred participants, nine unique and top-performing solutions have appeared, with novel designs ranging from the following aspects: spatial- and frequency-domain augmentations, masked image modeling, image restoration and super-resolution, adversarial training, diffusion-based noise suppression, vision-language pre-training, learned model ensembling, and hierarchical feature enhancement.","cats":{"new-dataset":0}}
{"text":"Extensive experimental analyses along with insightful observations are drawn to better understand the rationale behind each design.","cats":{"new-dataset":0}}
{"text":"We hope this challenge could lay a solid foundation for future research on robust and reliable depth estimation and beyond.","cats":{"new-dataset":0}}
{"text":"Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is widely recognized that realistic sensor simulation will play a critical role in solving remaining corner cases by simulating them.","cats":{"new-dataset":0}}
{"text":"To this end, we propose an autonomous driving simulator based upon neural radiance fields (NeRFs).","cats":{"new-dataset":0}}
{"text":"Compared with existing works, ours has three notable features: (1) Instance-aware.","cats":{"new-dataset":0}}
{"text":"Our simulator models the foreground instances and background environments separately with independent networks so that the static (e.g., size and appearance) and dynamic (e.g., trajectory) properties of instances can be controlled separately.","cats":{"new-dataset":0}}
{"text":"(2) Modular.","cats":{"new-dataset":0}}
{"text":"Our simulator allows flexible switching between different modern NeRF-related backbones, sampling strategies, input modalities, etc.","cats":{"new-dataset":0}}
{"text":"We expect this modular design to boost academic progress and industrial deployment of NeRF-based autonomous driving simulation.","cats":{"new-dataset":0}}
{"text":"(3) Realistic.","cats":{"new-dataset":0}}
{"text":"Our simulator set new state-of-the-art photo-realism results given the best module selection.","cats":{"new-dataset":0}}
{"text":"Our simulator will be open-sourced while most of our counterparts are not.","cats":{"new-dataset":0}}
{"text":"Project page: https://open-air-sun.github.io/mars/.","cats":{"new-dataset":0}}
{"text":"Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning, with applications spanning a wide range of domains.","cats":{"new-dataset":0}}
{"text":"In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text.","cats":{"new-dataset":0}}
{"text":"Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and recurrent neural networks for temporal modeling, with connectionist temporal classification used for text generation.","cats":{"new-dataset":0}}
{"text":"However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks.","cats":{"new-dataset":0}}
{"text":"Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in the post-processing stage to boost accuracy.","cats":{"new-dataset":0}}
{"text":"To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard sequence-to-sequence Transformer, and compare their performance in terms of accuracy and speed.","cats":{"new-dataset":0}}
{"text":"Our approach can model language dependencies and relies only on the attention mechanism, thereby making it more parallelizable and less complex.","cats":{"new-dataset":0}}
{"text":"We employ pre-trained Transformers for both image understanding and language modeling.","cats":{"new-dataset":0}}
{"text":"Our evaluation on the Arabic KHATT dataset demonstrates that our proposed method outperforms the current state-of-the-art approaches for recognizing offline Arabic handwritten text.","cats":{"new-dataset":0}}
{"text":"For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.","cats":{"new-dataset":1}}
{"text":"This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling.","cats":{"new-dataset":0}}
{"text":"Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy.","cats":{"new-dataset":0}}
{"text":"Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec.","cats":{"new-dataset":0}}
{"text":"A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings.","cats":{"new-dataset":0}}
{"text":"In ground-view object change detection, the recently emerging map-less navigation has great potential as a means of navigating a robot to distantly detected objects and identifying their changing states (appear/disappear/no-change) with high resolution imagery.","cats":{"new-dataset":0}}
{"text":"However, the brute-force naive action strategy of navigating to every distant object requires huge sense/plan/action costs proportional to the number of objects.","cats":{"new-dataset":0}}
{"text":"In this work, we study this new problem of ``Which distant objects should be prioritized for map-less navigation?\" and in order to speed up the R{\\&}D cycle, propose a highly-simplified approach that is easy to implement and easy to extend.","cats":{"new-dataset":0}}
{"text":"In our approach, a new layer called map-based navigation is added on top of the map-less navigation, which constitutes a hierarchical planner.","cats":{"new-dataset":0}}
{"text":"First, a dataset consisting of $N$ view sequences is acquired by a real robot via map-less navigation.","cats":{"new-dataset":0}}
{"text":"Then, an environment simulator was built to simulate a simple action planning problem: ``Which view sequence should the robot select next?\".","cats":{"new-dataset":0}}
{"text":"Then, a solver was built inspired by the analogy to the multi-armed bandit problem: ``Which arm should the player select next?\".","cats":{"new-dataset":0}}
{"text":"Finally, the effectiveness of the proposed framework was verified using the semantically non-trivial scenario ``sofa as bookshelf\".","cats":{"new-dataset":0}}
{"text":"Despite the presence of the classification task in many different benchmark datasets for perception in the automotive domain, few efforts have been undertaken to define consistent classification requirements.","cats":{"new-dataset":0}}
{"text":"This work addresses the topic by proposing a structured method to generate a classification structure.","cats":{"new-dataset":0}}
{"text":"First, legal categories are identified based on behavioral requirements for the vehicle.","cats":{"new-dataset":0}}
{"text":"This structure is further substantiated by considering the two aspects of collision safety for objects as well as perceptual categories.","cats":{"new-dataset":0}}
{"text":"A classification hierarchy is obtained by applying the method to an exemplary legal text.","cats":{"new-dataset":0}}
{"text":"A comparison of the results with benchmark dataset categories shows limited agreement.","cats":{"new-dataset":0}}
{"text":"This indicates the necessity for explicit consideration of legal requirements regarding perception.","cats":{"new-dataset":0}}
{"text":"The main challenges of 3D pose transfer are: 1) Lack of paired training data with different characters performing the same pose; 2) Disentangling pose and shape information from the target mesh; 3) Difficulty in applying to meshes with different topologies.","cats":{"new-dataset":0}}
{"text":"We thus propose a novel weakly-supervised keypoint-based framework to overcome these difficulties.","cats":{"new-dataset":0}}
{"text":"Specifically, we use a topology-agnostic keypoint detector with inverse kinematics to compute transformations between the source and target meshes.","cats":{"new-dataset":0}}
{"text":"Our method only requires supervision on the keypoints, can be applied to meshes with different topologies and is shape-invariant for the target which allows extraction of pose-only information from the target meshes without transferring shape information.","cats":{"new-dataset":0}}
{"text":"We further design a cycle reconstruction to perform self-supervised pose transfer without the need for ground truth deformed mesh with the same pose and shape as the target and source, respectively.","cats":{"new-dataset":0}}
{"text":"We evaluate our approach on benchmark human and animal datasets, where we achieve superior performance compared to the state-of-the-art unsupervised approaches and even comparable performance with the fully supervised approaches.","cats":{"new-dataset":0}}
{"text":"We test on the more challenging Mixamo dataset to verify our approach's ability in handling meshes with different topologies and complex clothes.","cats":{"new-dataset":0}}
{"text":"We curate and release a dataset we call COVERAGEEVAL by executing tests and code from the HumanEval dataset and collecting code coverage information.","cats":{"new-dataset":1}}
{"text":"Code coverage is a widely used metric for quantifying the extent to which program elements, such as statements or branches, are executed during testing.","cats":{"new-dataset":0}}
{"text":"Calculating code coverage is resource-intensive, requiring code building and execution with additional overhead for the instrumentation.","cats":{"new-dataset":0}}
{"text":"Furthermore, computing coverage of any snippet of code requires the whole program context.","cats":{"new-dataset":0}}
{"text":"Using Machine Learning to amortize this expensive process could lower the cost of code coverage by requiring only the source code context, and the task of code coverage prediction can be a novel benchmark for judging the ability of models to understand code.","cats":{"new-dataset":0}}
{"text":"We propose a novel benchmark task called Code Coverage Prediction for Large Language Models (LLMs).","cats":{"new-dataset":0}}
{"text":"We formalize this task to evaluate the capability of LLMs in understanding code execution by determining which lines of a method are executed by a given test case and inputs.  ","cats":{"new-dataset":0}}
{"text":"We report the performance of four state-of-the-art LLMs used for code-related tasks, including OpenAI's GPT-4 and GPT-3.5-Turbo, Google's BARD, and Anthropic's Claude, on the Code Coverage Prediction task.","cats":{"new-dataset":0}}
{"text":"Finally, we argue that code coverage as a metric and pre-training data source are valuable for overall LLM performance on software engineering tasks.","cats":{"new-dataset":0}}
{"text":"Task-oriented grasping (TOG) refers to the problem of predicting grasps on an object that enable subsequent manipulation tasks.","cats":{"new-dataset":0}}
{"text":"To model the complex relationships between objects, tasks, and grasps, existing methods incorporate semantic knowledge as priors into TOG pipelines.","cats":{"new-dataset":0}}
{"text":"However, the existing semantic knowledge is typically constructed based on closed-world concept sets, restraining the generalization to novel concepts out of the pre-defined sets.","cats":{"new-dataset":0}}
{"text":"To address this issue, we propose GraspGPT, a large language model (LLM) based TOG framework that leverages the open-end semantic knowledge from an LLM to achieve zero-shot generalization to novel concepts.","cats":{"new-dataset":0}}
{"text":"We conduct experiments on Language Augmented TaskGrasp (LA-TaskGrasp) dataset and demonstrate that GraspGPT outperforms existing TOG methods on different held-out settings when generalizing to novel concepts out of the training set.","cats":{"new-dataset":0}}
{"text":"The effectiveness of GraspGPT is further validated in real-robot experiments.","cats":{"new-dataset":0}}
{"text":"Our code, data, appendix, and video are publicly available at https://sites.google.com/view/graspgpt/.","cats":{"new-dataset":0}}
{"text":"With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged.","cats":{"new-dataset":0}}
{"text":"However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation.","cats":{"new-dataset":0}}
{"text":"In this paper, we build an environment for agent command and control that is highly realistic and reproducible.","cats":{"new-dataset":0}}
{"text":"Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.","cats":{"new-dataset":0}}
{"text":"Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving.","cats":{"new-dataset":0}}
{"text":"Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions.","cats":{"new-dataset":0}}
{"text":"The tasks in our benchmark are diverse, long-horizon, and are designed to emulate tasks that humans routinely perform on the internet.","cats":{"new-dataset":0}}
{"text":"We design and implement several autonomous agents, integrating recent techniques such as reasoning before acting.","cats":{"new-dataset":0}}
{"text":"The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 10.59%.","cats":{"new-dataset":0}}
{"text":"These results highlight the need for further development of robust agents, that current state-of-the-art LMs are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.","cats":{"new-dataset":0}}
{"text":"Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.dev/.","cats":{"new-dataset":0}}
{"text":"Crafting effective deep learning models for medical image analysis is a complex task, particularly in cases where the medical image dataset lacks significant inter-class variation.","cats":{"new-dataset":0}}
{"text":"This challenge is further aggravated when employing such datasets to generate synthetic images using generative adversarial networks (GANs), as the output of GANs heavily relies on the input data.","cats":{"new-dataset":0}}
{"text":"In this research, we propose a novel filtering algorithm called Cosine Similarity-based Image Filtering (CosSIF).","cats":{"new-dataset":0}}
{"text":"We leverage CosSIF to develop two distinct filtering methods: Filtering Before GAN Training (FBGT) and Filtering After GAN Training (FAGT).","cats":{"new-dataset":0}}
{"text":"FBGT involves the removal of real images that exhibit similarities to images of other classes before utilizing them as the training dataset for a GAN.","cats":{"new-dataset":0}}
{"text":"On the other hand, FAGT focuses on eliminating synthetic images with less discriminative features compared to real images used for training the GAN.","cats":{"new-dataset":0}}
{"text":"Experimental results reveal that employing either the FAGT or FBGT method with modern transformer and convolutional-based networks leads to substantial performance gains in various evaluation metrics.","cats":{"new-dataset":0}}
{"text":"FAGT implementation on the ISIC-2016 dataset surpasses the baseline method in terms of sensitivity by 1.59\\% and AUC by 1.88\\%.","cats":{"new-dataset":0}}
{"text":"Furthermore, for the HAM10000 dataset, applying FABT outperforms the baseline approach in terms of recall by 13.75\\%, and with the sole implementation of FAGT, achieves a maximum accuracy of 94.44\\%.","cats":{"new-dataset":0}}
{"text":"Recently, there has been growing interest in extending the context length of instruction-following models in order to effectively process single-turn long input (e.g. summarizing a paper) and conversations with more extensive histories.","cats":{"new-dataset":0}}
{"text":"While proprietary models such as GPT-4 and Claude have demonstrated considerable advancements in handling tens of thousands of tokens of context, open-sourced models are still in the early stages of experimentation.","cats":{"new-dataset":0}}
{"text":"It also remains unclear whether developing these long context models can offer substantial gains on practical downstream tasks over retrieval-based methods or models simply trained on chunked contexts.","cats":{"new-dataset":0}}
{"text":"To address this challenge, we propose to institute standardized evaluation for long context language models.","cats":{"new-dataset":0}}
{"text":"Concretely, we develop L-Eval which contains 411 long documents and over 2,000 query-response pairs manually annotated and checked by the authors encompassing areas such as law, finance, school lectures, lengthy conversations, news, long-form novels, and meetings.","cats":{"new-dataset":0}}
{"text":"L-Eval also adopts diverse evaluation methods and instruction styles, enabling a more reliable assessment of Long Context Language Models (LCLMs).","cats":{"new-dataset":0}}
{"text":"Our findings indicate that while open-source models typically lag behind their commercial counterparts, they still exhibit impressive performance.","cats":{"new-dataset":0}}
{"text":"LLaMA2 achieves the best results (win 45\\% vs turbo-16k) on open-ended tasks with only 4k context length and ChatGLM2 achieves the best results on closed-ended tasks with 8k input tokens.","cats":{"new-dataset":0}}
{"text":"We release our new evaluation suite, code, and all generation results including predictions from all open-sourced LCLMs, GPT4-32k, Cluade-100k at {\\url{https://github.com/OpenLMLab/LEval}}.","cats":{"new-dataset":0}}
{"text":"Object localization in general environments is a fundamental part of vision systems.","cats":{"new-dataset":0}}
{"text":"While dominating on the COCO benchmark, recent Transformer-based detection methods are not competitive in diverse domains.","cats":{"new-dataset":0}}
{"text":"Moreover, these methods still struggle to very accurately estimate the object bounding boxes in complex environments.   ","cats":{"new-dataset":0}}
{"text":"We introduce Cascade-DETR for high-quality universal object detection.","cats":{"new-dataset":0}}
{"text":"We jointly tackle the generalization to diverse domains and localization accuracy by proposing the Cascade Attention layer, which explicitly integrates object-centric information into the detection decoder by limiting the attention to the previous box prediction.","cats":{"new-dataset":0}}
{"text":"To further enhance accuracy, we also revisit the scoring of queries.","cats":{"new-dataset":0}}
{"text":"Instead of relying on classification scores, we predict the expected IoU of the query, leading to substantially more well-calibrated confidences.","cats":{"new-dataset":0}}
{"text":"Lastly, we introduce a universal object detection benchmark, UDB10, that contains 10 datasets from diverse domains.","cats":{"new-dataset":0}}
{"text":"While also advancing the state-of-the-art on COCO, Cascade-DETR substantially improves DETR-based detectors on all datasets in UDB10, even by over 10 mAP in some cases.","cats":{"new-dataset":0}}
{"text":"The improvements under stringent quality requirements are even more pronounced.","cats":{"new-dataset":0}}
{"text":"Our code and models will be released at https://github.com/SysCV/cascade-detr.","cats":{"new-dataset":0}}
{"text":"Neural Radiance Field (NeRF) is a promising approach for synthesizing novel views, given a set of images and the corresponding camera poses of a scene.","cats":{"new-dataset":0}}
{"text":"However, images photographed from a low-light scene can hardly be used to train a NeRF model to produce high-quality results, due to their low pixel intensities, heavy noise, and color distortion.","cats":{"new-dataset":0}}
{"text":"Combining existing low-light image enhancement methods with NeRF methods also does not work well due to the view inconsistency caused by the individual 2D enhancement process.","cats":{"new-dataset":0}}
{"text":"In this paper, we propose a novel approach, called Low-Light NeRF (or LLNeRF), to enhance the scene representation and synthesize normal-light novel views directly from sRGB low-light images in an unsupervised manner.","cats":{"new-dataset":0}}
{"text":"The core of our approach is a decomposition of radiance field learning, which allows us to enhance the illumination, reduce noise and correct the distorted colors jointly with the NeRF optimization process.","cats":{"new-dataset":0}}
{"text":"Our method is able to produce novel view images with proper lighting and vivid colors and details, given a collection of camera-finished low dynamic range (8-bits/channel) images from a low-light scene.","cats":{"new-dataset":0}}
{"text":"Experiments demonstrate that our method outperforms existing low-light enhancement methods and NeRF methods.","cats":{"new-dataset":0}}
{"text":"Using a scalable data engine that incorporates human feedback and efficient models in the loop, we create a new dataset (AS-1B) with over 1 billion regions annotated with semantic tags, question-answering pairs, and detailed captions.","cats":{"new-dataset":1}}
{"text":"Leveraging this new dataset, we develop the All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding","cats":{"new-dataset":1}}
{"text":"Models and the dataset shall be released at https://github.com/OpenGVLab/All-Seeing, and demo can be seen at https://huggingface.co/spaces/OpenGVLab/all-seeing.","cats":{"new-dataset":0}}
{"text":"We present the All-Seeing (AS) project: a large-scale data and model for recognizing and understanding everything in the open world.  ","cats":{"new-dataset":0}}
{"text":"It covers a wide range of 3.5 million common and rare concepts in the real world, and has 132.2 billion tokens that describe the concepts and their attributes.","cats":{"new-dataset":0}}
{"text":"Leveraging this new dataset, we develop the All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding.","cats":{"new-dataset":0}}
{"text":"The model is trained with open-ended language prompts and locations, which allows it to generalioning, and question-answering.","cats":{"new-dataset":0}}
{"text":"We hope that this project can serve as a foundation for vision-language artificial general intelligence research.","cats":{"new-dataset":0}}
{"text":"Conversational agents are consistently growing in popularity and many people interact with them every day.","cats":{"new-dataset":0}}
{"text":"While many conversational agents act as personal assistants, they can have many different goals.","cats":{"new-dataset":0}}
{"text":"Some are task-oriented, such as providing customer support for a bank or making a reservation.","cats":{"new-dataset":0}}
{"text":"Others are designed to be empathetic and to form emotional connections with the user.","cats":{"new-dataset":0}}
{"text":"The dataset has been scraped from Codeforces, a major competitive programming website.","cats":{"new-dataset":1}}
{"text":"In the past decade, the amount of research being done in the fields of machine learning and deep learning, predominantly in the area of natural language processing (NLP), has risen dramatically.","cats":{"new-dataset":0}}
{"text":"A well-liked method for developing programming abilities like logic building and problem solving is competitive programming.","cats":{"new-dataset":0}}
{"text":"It can be tough for novices and even veteran programmers to traverse the wide collection of questions due to the massive number of accessible questions and the variety of themes, levels of difficulty, and questions offered.","cats":{"new-dataset":0}}
{"text":"In order to help programmers find questions that are appropriate for their knowledge and interests, there is a need for an automated method.","cats":{"new-dataset":0}}
{"text":"This can be done using automated tagging of the questions using Text Classification.","cats":{"new-dataset":0}}
{"text":"Text classification is one of the important tasks widely researched in the field of Natural Language Processing.","cats":{"new-dataset":0}}
{"text":"In this paper, we present a way to use text classification techniques to determine the domain of a competitive programming problem.","cats":{"new-dataset":0}}
{"text":"A variety of models, including are implemented LSTM, GRU, and MLP.  ","cats":{"new-dataset":0}}
{"text":"A total of 2400 problems were scraped and preprocessed, which we used as a dataset for our training and testing of models.","cats":{"new-dataset":0}}
{"text":"The maximum accuracy reached using our model is 78.0% by MLP(Multi Layer Perceptron).","cats":{"new-dataset":0}}
{"text":"Open-source EDA shows promising potential in unleashing EDA innovation and lowering the cost of chip design.","cats":{"new-dataset":0}}
{"text":"This paper presents an open-source EDA project, iEDA, aiming for building a basic infrastructure for EDA technology evolution and closing the industrial-academic gap in the EDA area.","cats":{"new-dataset":0}}
{"text":"iEDA now covers the whole flow of physical design (including Floorplan, Placement, CTS, Routing, Timing Optimization etc.), and part of the analysis tools (Static Timing Analysis and Power Analysis).","cats":{"new-dataset":0}}
{"text":"To demonstrate the effectiveness of iEDA, we implement and tape out three chips of different scales (from 700k to 1.5M gates) on different process nodes (110nm and 28nm) with iEDA.","cats":{"new-dataset":0}}
{"text":"This paper addresses such issues with several contributions: (1) we introduce models for interaction signature estimation (ISP) encompassing contact detection, segmentation, and 3d contact signature prediction; (2) we show how such components can be leveraged to ensure contact consistency during 3d reconstruction; (3) we construct several large datasets for learning and evaluating 3d contact prediction and reconstruction methods; specifically, we introduce CHI3D, a lab-based accurate 3d motion capture dataset with 631 sequences containing $2,525$ contact events, $728,664$ ground truth 3d poses, as well as FlickrCI3D, a dataset of $11,216$ images, with $14,081$ processed pairs of people, and $81,233$ facet-level surface correspondences.","cats":{"new-dataset":1}}
{"text":"Understanding 3d human interactions is fundamental for fine-grained scene analysis and behavioural modeling.","cats":{"new-dataset":0}}
{"text":"However, most of the existing models predict incorrect, lifeless 3d estimates, that miss the subtle human contact aspects--the essence of the event--and are of little use for detailed behavioral understanding.  ","cats":{"new-dataset":0}}
{"text":"Finally, (4) we propose methodology for recovering the ground-truth pose and shape of interacting people in a controlled setup and (5) annotate all 3d interaction motions in CHI3D with textual descriptions.","cats":{"new-dataset":0}}
{"text":"Motion data in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) is made available for research purposes at \\url{https://ci3d.imar.ro}, together with an evaluation server and a public benchmark.","cats":{"new-dataset":0}}
{"text":"Our dataset, consisting of input and output images, was generated by implementing boundary conditions and extracting stress-strain field maps.","cats":{"new-dataset":1}}
{"text":"This study investigated the potential of end-to-end deep learning tools as a more effective substitute for FEM in predicting stress-strain fields within 2D cross sections of arterial wall.","cats":{"new-dataset":0}}
{"text":"We first proposed a U-Net based fully convolutional neural network (CNN) to predict the von Mises stress and strain distribution based on the spatial arrangement of calcification within arterial wall cross-sections.","cats":{"new-dataset":0}}
{"text":"Further, we developed a conditional generative adversarial network (cGAN) to enhance, particularly from the perceptual perspective, the prediction accuracy of stress and strain field maps for arterial walls with various calcification quantities and spatial configurations.","cats":{"new-dataset":0}}
{"text":"On top of U-Net and cGAN, we also proposed their ensemble approaches, respectively, to further improve the prediction accuracy of field maps.  ","cats":{"new-dataset":0}}
{"text":"The trained U-Net models can accurately predict von Mises stress and strain fields, with structural similarity index scores (SSIM) of 0.854 and 0.830 and mean squared errors of 0.017 and 0.018 for stress and strain, respectively, on a reserved test set.","cats":{"new-dataset":0}}
{"text":"Meanwhile, the cGAN models in a combination of ensemble and transfer learning techniques demonstrate high accuracy in predicting von Mises stress and strain fields, as evidenced by SSIM scores of 0.890 for stress and 0.803 for strain.","cats":{"new-dataset":0}}
{"text":"Additionally, mean squared errors of 0.008 for stress and 0.017 for strain further support the model's performance on a designated test set.","cats":{"new-dataset":0}}
{"text":"Overall, this study developed a surrogate model for finite element analysis, which can accurately and efficiently predict stress-strain fields of arterial walls regardless of complex geometries and boundary conditions.","cats":{"new-dataset":0}}
{"text":"The authors collected a dataset of over 150k uncurated surgical pathology reports, containing gross descriptions, final diagnoses, and condition codes.","cats":{"new-dataset":1}}
{"text":"This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex, domain-specific tasks.","cats":{"new-dataset":0}}
{"text":"Specifically, the authors demonstrate their approach by extracting structured condition codes from pathology reports.","cats":{"new-dataset":0}}
{"text":"The proposed approach utilizes local LLMs, which can be fine-tuned to respond to specific generative instructions and provide structured outputs.  ","cats":{"new-dataset":0}}
{"text":"They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance.","cats":{"new-dataset":0}}
{"text":"The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics, even with extremely reduced precision.","cats":{"new-dataset":0}}
{"text":"The LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-label tasks.","cats":{"new-dataset":0}}
{"text":"Overall, this work presents an effective approach for utilizing LLMs to perform domain-specific tasks using accessible hardware, with potential applications in the medical domain, where complex data extraction and classification are required.","cats":{"new-dataset":0}}
{"text":"Patch-DM produces high-quality image synthesis results on our newly collected dataset of nature images (1024$\\times$512), as well as on standard benchmarks of smaller sizes (256$\\times$256), including LSUN-Bedroom, LSUN-Church, and FFHQ.","cats":{"new-dataset":1}}
{"text":"We propose an effective denoising diffusion model for generating high-resolution images (e.g., 1024$\\times$512), trained on small-size image patches (e.g., 64$\\times$64).","cats":{"new-dataset":0}}
{"text":"We name our algorithm Patch-DM, in which a new feature collage strategy is designed to avoid the boundary artifact when synthesizing large-size images.","cats":{"new-dataset":0}}
{"text":"Feature collage systematically crops and combines partial features of the neighboring patches to predict the features of a shifted image patch, allowing the seamless generation of the entire image due to the overlap in the patch feature space.  ","cats":{"new-dataset":0}}
{"text":"We compare our method with previous patch-based generation methods and achieve state-of-the-art FID scores on all four datasets.","cats":{"new-dataset":0}}
{"text":"Further, Patch-DM also reduces memory complexity compared to the classic diffusion models.","cats":{"new-dataset":0}}
{"text":"Next basket recommendation (NBR) is the task of predicting the next set of items based on a sequence of already purchased baskets.","cats":{"new-dataset":0}}
{"text":"It is a recommendation task that has been widely studied, especially in the context of grocery shopping.","cats":{"new-dataset":0}}
{"text":"In next basket recommendation (NBR), it is useful to distinguish between repeat items, i.e., items that a user has consumed before, and explore items, i.e., items that a user has not consumed before.","cats":{"new-dataset":0}}
{"text":"Most NBR work either ignores this distinction or focuses on repeat items.","cats":{"new-dataset":0}}
{"text":"We formulate the next novel basket recommendation (NNBR) task, i.e., the task of recommending a basket that only consists of novel items, which is valuable for both real-world application and NBR evaluation.","cats":{"new-dataset":0}}
{"text":"We evaluate how existing NBR methods perform on the NNBR task and find that, so far, limited progress has been made w.r.t.","cats":{"new-dataset":0}}
{"text":"the NNBR task.","cats":{"new-dataset":0}}
{"text":"To address the NNBR task, we propose a simple bi-directional transformer basket recommendation model (BTBR), which is focused on directly modeling item-to-item correlations within and across baskets instead of learning complex basket representations.","cats":{"new-dataset":0}}
{"text":"To properly train BTBR, we propose and investigate several masking strategies and training objectives: (i) item-level random masking, (ii) item-level select masking, (iii) basket-level all masking, (iv) basket-level explore masking, and (v) joint masking.","cats":{"new-dataset":0}}
{"text":"In addition, an item-basket swapping strategy is proposed to enrich the item interactions within the same baskets.","cats":{"new-dataset":0}}
{"text":"We conduct extensive experiments on three open datasets with various characteristics.","cats":{"new-dataset":0}}
{"text":"The results demonstrate the effectiveness of BTBR and our masking and swapping strategies for the NNBR task.","cats":{"new-dataset":0}}
{"text":"BTBR with a properly selected masking and swapping strategy can substantially improve NNBR performance.","cats":{"new-dataset":0}}
{"text":"Enumeration kernelization was first proposed by Creignou et al.","cats":{"new-dataset":0}}
{"text":"[TOCS 2017] and was later refined by Golovach et al.","cats":{"new-dataset":0}}
{"text":"[JCSS 2022] into two different variants: fully-polynomial enumeration kernelization and polynomial-delay enumeration kernelization.","cats":{"new-dataset":0}}
{"text":"In this paper, we consider the d-CUT problem from the perspective of (polynomial-delay) enumeration kenrelization.","cats":{"new-dataset":0}}
{"text":"Given an undirected graph G = (V, E), a cut F = E(A, B) is a d-cut of G if every u in A has at most d neighbors in B and every v in B has at most d neighbors in A. Checking the existence of a d-cut in a graph is a well-known NP-hard problem and is well-studied in parameterized complexity","cats":{"new-dataset":0}}
{"text":"[Algorithmica 2021, IWOCA 2021].","cats":{"new-dataset":0}}
{"text":"This problem also generalizes a well-studied problem MATCHING CUT (set d = 1) that has been a central problem in the literature of polynomial-delay enumeration kernelization.","cats":{"new-dataset":0}}
{"text":"In this paper, we study three different enumeration variants of this problem, ENUM d-CUT, ENUM MIN-d-CUT and ENUM MAX-d-CUT that intends to enumerate all the d-cuts, all the minimal d-cuts and all the maximal d-cuts respectively.","cats":{"new-dataset":0}}
{"text":"We consider various structural parameters of the input and provide polynomial-delay enumeration kernels for ENUM d-CUT and ENUM MAX-d-CUT and fully-polynomial enumeration kernels of polynomial size for ENUM MIN-d-CUT.","cats":{"new-dataset":0}}
{"text":"Large language models (LLMs) such as ChatGPT are increasingly being used for various use cases, including text content generation at scale.","cats":{"new-dataset":0}}
{"text":"Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector on such AI-generated text, inspired by works that use ChatGPT as a data labeler or annotator.","cats":{"new-dataset":0}}
{"text":"We evaluate the zero-shot performance of ChatGPT in the task of human-written vs. AI-generated text detection, and perform experiments on publicly available datasets.","cats":{"new-dataset":0}}
{"text":"We empirically investigate if ChatGPT is symmetrically effective in detecting AI-generated or human-written text.","cats":{"new-dataset":0}}
{"text":"Our findings provide insight on how ChatGPT and similar LLMs may be leveraged in automated detection pipelines by simply focusing on solving a specific aspect of the problem and deriving the rest from that solution.","cats":{"new-dataset":0}}
{"text":"All code and data is available at \\url{https://github.com/AmritaBh/ChatGPT-as-Detector}.","cats":{"new-dataset":0}}
{"text":"As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar angle into account in a NeRF-based framework for rendering a scene from a novel viewpoint using satellite images for training.","cats":{"new-dataset":0}}
{"text":"Our work extends those contributions and shows how one can make the renderings season-specific.","cats":{"new-dataset":0}}
{"text":"Our main challenge was creating a Neural Radiance Field (NeRF) that could render seasonal features independently of viewing angle and solar angle while still being able to render shadows.","cats":{"new-dataset":0}}
{"text":"We teach our network to render seasonal features by introducing one more input variable -- time of the year.","cats":{"new-dataset":0}}
{"text":"However, the small training datasets typical of satellite imagery can introduce ambiguities in cases where shadows are present in the same location for every image of a particular season.","cats":{"new-dataset":0}}
{"text":"We add additional terms to the loss function to discourage the network from using seasonal features for accounting for shadows.","cats":{"new-dataset":0}}
{"text":"We show the performance of our network on eight Areas of Interest containing images captured by the Maxar WorldView-3 satellite.","cats":{"new-dataset":0}}
{"text":"This evaluation includes tests measuring the ability of our framework to accurately render novel views, generate height maps, predict shadows, and specify seasonal features independently from shadows.","cats":{"new-dataset":0}}
{"text":"Our ablation studies justify the choices made for network design parameters.","cats":{"new-dataset":0}}
{"text":"To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models.","cats":{"new-dataset":1}}
{"text":"Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry.","cats":{"new-dataset":0}}
{"text":"However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries.","cats":{"new-dataset":0}}
{"text":"Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage.  ","cats":{"new-dataset":0}}
{"text":"Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques.","cats":{"new-dataset":0}}
{"text":"It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances.","cats":{"new-dataset":0}}
{"text":"Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud environment.","cats":{"new-dataset":0}}
{"text":"In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, using crowdsourced images.","cats":{"new-dataset":0}}
{"text":"These models are available for viewing, interaction, and download on the Tirtha website.","cats":{"new-dataset":0}}
{"text":"Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage co","cats":{"new-dataset":0}}
{"text":"We also present the musdb-XL-train dataset, consisting of 300k segments created by applying a commercial limiter plug-in for training real-world friendly de-limiter networks.","cats":{"new-dataset":1}}
{"text":"The loudness war, an ongoing phenomenon in the music industry characterized by the increasing final loudness of music while reducing its dynamic range, has been a controversial topic for decades.","cats":{"new-dataset":0}}
{"text":"Music mastering engineers have used limiters to heavily compress and make music louder, which can induce ear fatigue and hearing loss in listeners.","cats":{"new-dataset":0}}
{"text":"In this paper, we introduce music de-limiter networks that estimate uncompressed music from heavily compressed signals.","cats":{"new-dataset":0}}
{"text":"Inspired by the principle of a limiter, which performs sample-wise gain reduction of a given signal, we propose the framework of sample-wise gain inversion (SGI).  ","cats":{"new-dataset":0}}
{"text":"Our proposed de-limiter network achieves excellent performance with a scale-invariant source-to-distortion ratio (SI-SDR) of 23.8 dB in reconstructing musdb-HQ from musdb- XL data, a limiter-applied version of musdb-HQ.","cats":{"new-dataset":0}}
{"text":"The training data, codes, and model weights are available in our repository (https://github.com/jeonchangbin49/De-limiter).","cats":{"new-dataset":0}}
{"text":"Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage.","cats":{"new-dataset":0}}
{"text":"Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen.","cats":{"new-dataset":0}}
{"text":"Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide.","cats":{"new-dataset":0}}
{"text":"As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable.","cats":{"new-dataset":0}}
{"text":"Our work provides an alternative to demonstrations: tool documentation.","cats":{"new-dataset":0}}
{"text":"We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations.","cats":{"new-dataset":0}}
{"text":"We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities.","cats":{"new-dataset":0}}
{"text":"First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts.","cats":{"new-dataset":0}}
{"text":"Second, on a newly collected realistic tool-use dataset with hundreds of available tool APIs, we show that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation.","cats":{"new-dataset":0}}
{"text":"Third, we highlight the benefits of tool documentations by tackling image generation and video tracking using just-released unseen state-of-the-art models as tools.","cats":{"new-dataset":0}}
{"text":"Finally, we highlight the possibility of using tool documentation to automatically enable new applications: by using nothing more than the documentation of GroundingDino, Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the just-released Grounded-SAM and Track Anything models.","cats":{"new-dataset":0}}
{"text":"In this paper, we construct an ancient Chinese character image dataset that contains both radical-level and character-level annotations to satisfy the requirements of the above-mentioned methods, namely, ACCID, where radical-level annotations include radical categories, radical locations, and structural relations.","cats":{"new-dataset":1}}
{"text":"Optical character recognition (OCR) methods have been applied to diverse tasks, e.g., street view text recognition and document analysis.","cats":{"new-dataset":0}}
{"text":"Recently, zero-shot OCR has piqued the interest of the research community because it considers a practical OCR scenario with unbalanced data distribution.","cats":{"new-dataset":0}}
{"text":"However, there is a lack of benchmarks for evaluating such zero-shot methods that apply a divide-and-conquer recognition strategy by decomposing characters into radicals.","cats":{"new-dataset":0}}
{"text":"Meanwhile, radical recognition, as another important OCR task, also lacks radical-level annotation for model training.  ","cats":{"new-dataset":0}}
{"text":"To increase the adaptability of ACCID, we propose a splicing-based synthetic character algorithm to augment the training samples and apply an image denoising method to improve the image quality.","cats":{"new-dataset":0}}
{"text":"By introducing character decomposition and recombination, we propose a baseline method for zero-shot OCR.","cats":{"new-dataset":0}}
{"text":"The experimental results demonstrate the validity of ACCID and the baseline model quantitatively and qualitatively.","cats":{"new-dataset":0}}
{"text":"Advanced driving assistance systems are available on many late-model vehicles, and automated driving systems are testing on public roads.","cats":{"new-dataset":0}}
{"text":"Regulators and developers continue to assess the safety of these vehicles by comparing automated vehicle crash rates to baseline, human-driven crash rates.","cats":{"new-dataset":0}}
{"text":"While there are several widely-cited automated vehicle and conventional vehicle crash databases, these databases have different underlying assumptions and inclusion criteria.","cats":{"new-dataset":0}}
{"text":"Crash rates among databases may be directly comparable only with significant filtering and normalization, if at all.","cats":{"new-dataset":0}}
{"text":"This paper reviews current automated vehicle and baseline human-driven crash databases and evaluates their comparability.","cats":{"new-dataset":0}}
{"text":"Recommendations are presented to improve their comparability, both in terms of normalization and contextualization, as well as additional data fields that can be incorporated into existing databases.","cats":{"new-dataset":0}}
{"text":"These findings may assist researchers, regulators, and automated vehicle developers attempting to evaluate the safety of driving automation systems.","cats":{"new-dataset":0}}